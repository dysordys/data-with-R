[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to data analysis and visualization with R",
    "section": "",
    "text": "Lecture notes for NBIC58 (Analysis of Biological Data) at Linköping University, organized into the beginnings of a book. This work is preliminary, so any feedback is much appreciated!"
  },
  {
    "objectID": "Intro_R_RStudio.html",
    "href": "Intro_R_RStudio.html",
    "title": "1  Introduction to R and RStudio",
    "section": "",
    "text": "This chapter introduces R and RStudio. R is a free and open-source programming language for statistics, graphing, and modeling, originally developed by statisticians. In recent years, R has become extremely popular among biologists, and you will almost certainly encounter it as part of real-world research projects. In this course, we will be learning some of the ways in which R can be used for efficient data analysis and visualization.\nRStudio is an “integrated development environment” (IDE) for R, which means it is a software application that lets you write, run, and interact graphically with programs. RStudio integrates a text editor, the R console (where you run R commands), package management, plotting, help, and more."
  },
  {
    "objectID": "Intro_R_RStudio.html#installing-r-and-rstudio",
    "href": "Intro_R_RStudio.html#installing-r-and-rstudio",
    "title": "1  Introduction to R and RStudio",
    "section": "1.2 Installing R and RStudio",
    "text": "1.2 Installing R and RStudio\nYou can download the most up-to-date R distribution for free here:\nhttp://www.r-project.org\nRun the installer as directed and you should be set to go. We will not interact with the installed R application directly, but the R software components you install will be used by RStudio under the hood.\nTo install RStudio on your computer, download it from here:\nhttps://www.rstudio.com/products/rstudio/download/\nOn a Mac, open the downloaded disk image and drag the RStudio application into your Applications folder. On Windows, run the installer you downloaded."
  },
  {
    "objectID": "Intro_R_RStudio.html#getting-around-rstudio",
    "href": "Intro_R_RStudio.html#getting-around-rstudio",
    "title": "1  Introduction to R and RStudio",
    "section": "1.3 Getting Around RStudio",
    "text": "1.3 Getting Around RStudio\nRStudio should be available in the usual places: the Applications folder (on a Mac) or the Start menu (on Windows). When you start it up, you will see four sections of the screen. In short:\n\nUpper-left: an area for viewing and editing text files\nLower-left: Console, where you send commands to R\nUpper-right:\n\nEnvironment: for loading, saving, and examining data\nHistory: a list of commands you have typed\nConnections: for establishing remote connections with data sources\nTutorial: resources for learning R\n\nLower-right:\n\nFiles: a list of files in the “working directory” (more on this later)\nPlots: where the plots (graphs) you draw show up\nPackages: an area for managing installed R packages\nHelp: access to all the official documentation for R\nViewer: you can view certain objects here, e.g., formatted tables of data\n\n\n\n\n\nRStudio starting screen\n\n\n\n1.3.1 A Simple Calculation\nEven if you don’t know R, you can start by typing some simple calculations into the console. The > symbol indicates that R is waiting for you to type something. Click on the console, type 2 + 2, hit Enter (or Return on a Mac), and you should see R produce the right answer:\n\n> 2 + 2\n[1] 4\n>\n\nNow, press the Up arrow on the keyboard. You will notice that the 2 + 2 you typed before shows up. You can use the Up and Down arrows to go back and forth through past things you have typed, which can save a lot of repetitive typing when you are trying things out. You can change the text in these historical commands: change the 2 to a 3 and press Enter (Return, on a Mac) again:\n\n> 2 + 3\n[1] 5\n>\n\n(The [1] at the beginning of the result just means that the following number is at position 1 of a vector. In this case, the vector only has one element, but when R needs to print out a long vector, it splits it into into multiple lines tells you at the beginning of each line what position you are at.)\nBefore going deeper into R programming, we need to discuss a few things to enable you to get around R and RStudio more easily.\n\n\n1.3.2 Writing R scripts\nThe upper-right part of RStudio is a simple text editor where you can write R code. But, instead of having to enter it one line at a time as we did in the console above, you can string long sequences of R instructions together that build on one another. You can save such a text file (Ctrl-S on Windows; Cmd-S on a Mac), giving it an appropriate name. It is then known as an R script, a text file containing R code that can be run from within R.\nAs an example, enter the following code. Do not worry about how or why it works just yet. It is a simple simulation and visualization of exponential population growth:\n\ntime <- 1:15\ngrowthRate <- 1.2\npopSize <- rep(100, times = length(time))\nfor (i in 1:(length(time) - 1)) {\n  popSize[i + 1] <- popSize[i] * growthRate\n}\nplot(time, popSize, ylab = \"population size\", type = \"b\")\n\nAfter having typed this, highlight all lines (either with a mouse, or by pressing Ctrl-A on Windows / Cmd-A on a Mac, or by using the arrow keys while holding the Shift key down). Then, to send these instructions to R for processing, press Ctrl-Enter (Windows) or Cmd-Return (Mac). If all went well, the lower right screen section should have jumped to the Plots panel, showing the following graph:\n\n\n\n\n\n\n\n1.3.3 Setting the Working Directory\nWhen you ask R to run a program or load data from a file, it needs to know where to find the file. Unless you specify the complete path to the file on your machine, it assumes that file names are relative to what is called the “working directory”.\nThe first thing you should do when starting a project is to create a directory (folder) on your computer to store all the files related to the project, and then tell R to set the working directory to that location.\nThe R function setwd(\"/path/to/directory\") is used to set the working directory, where you substitute in the actual path and directory name in place of path/to/directory. In turn, getwd() tells you what the current working directory is. The path can be found using File Explorer on a Windows PC, but you will need to change backslashes to forward slashes (\\ to /). On a Mac, you can find the path by selecting a folder and choosing File > Get Info (the path is under “Where:”).\nThere is also a convenient graphical way to set the working directory in RStudio. In the Files panel, you can navigate around the computer’s file space. You can do this either in the panel itself, or using the ellipsis (…) to bring up the system-standard file browser. Looking around there does not immediately set the working directory, but you can set it by clicking the “More” button and choosing “Set as Working Directory”.\n\n\n\n\n\n\nNote\n\n\n\nIt is worth repeating: finding the appropriate directory in the Files panel is not enough. It will not set the working directory automatically. You need to actually choose “Set as Working Directory” by clicking on it.\n\n\nYou may have also noticed that you don’t actually need to type getwd(): the RStudio Console panel shows the current working directory below the word “Console”.\n\n\n1.3.4 Packages\nOne of the primary reasons ecologists use R is the availability of hundreds of free, user-contributed pieces of software, called packages. Packages are generally created by people who wanted to solve a particular problem for their own research and then realized that other people might find their code useful. Take a moment to browse the packages available on the main R site:\nhttp://cran.r-project.org/web/packages/\nTo install a package, you take its name, put it in quotes, and put it in between the parentheses of install.packages(). For example, to install the package tidyverse (which we will be relying on later), you type:\n\ninstall.packages(\"tidyverse\")\n\nand press Enter (Return, on a Mac). Note that some packages can take quite a while to install. If you are installing tidyverse for instance, it could take anywhere between five minutes to an hour (!) depending on your computer setup. This is normal, and the good news is that you only need to do this once on a computer. Once the package is installed, it will stick around.\nHowever, to actually use a previously installed package in an R session, you need to load it from your disk directly into the computer’s memory. That can be done like this:\n\nlibrary(tidyverse)\n\nNote the lack of quotation marks when loading a package.\nRStudio also makes package management a bit easier. In the Packages panel (top line of lower right portion of the screen) you can see a list of all installed packages. You can also load and unload packages simply by checking a checkbox, and you can install new packages using a graphical interface (although you will still need to know the name of the package you want to install)."
  },
  {
    "objectID": "Intro_R_RStudio.html#additional-reading",
    "href": "Intro_R_RStudio.html#additional-reading",
    "title": "1  Introduction to R and RStudio",
    "section": "1.4 Additional Reading",
    "text": "1.4 Additional Reading\nR:\n\nR website\nCRAN package index\n\nRStudio:\n\nRStudio documentation"
  },
  {
    "objectID": "Intro_R_RStudio.html#exercises",
    "href": "Intro_R_RStudio.html#exercises",
    "title": "1  Introduction to R and RStudio",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\n\nCreate a folder named analysis_of_biological_data on your computer and set it as the working directory in R. Make certain that the working directory has indeed been set properly.\nUse the RStudio file browser to set the working directory somewhere else on your hard drive, and then set it back to the analysis_of_biological_data folder you created earlier. Make sure it is being set properly at each step.\nInstall an R package called vegan, using install.packages as discussed in Section 1.3.4. (The vegan package contains various utilities for community ecology.)\nLoad the vegan package invoking library(vegan). Afterwards, try unloading and then loading the vegan package again, using the Packages panel in RStudio this time."
  },
  {
    "objectID": "R_programming_basics.html#using-r-as-a-calculator",
    "href": "R_programming_basics.html#using-r-as-a-calculator",
    "title": "2  R programming basics",
    "section": "2.1 Using R as a calculator",
    "text": "2.1 Using R as a calculator\nAs we have seen before, R can be used as a glorified pocket calculator. Elementary operations work as expected: + and - are symbols for addition and subtraction, while * and / are multiplication and division. Thus, we can enter things such as\n\n3 * 4 - 6 / 2 + 1\n\nin the console, and press Enter (Return, on a Mac) to get the result, 10. One even has exponentiation, denoted by the symbol ^. To raise 2 to the 5th power), we enter\n\n2^5\n\nand obtain the expected 32. Furthermore, one is not restricted to integers. It is possible to calculate with fractional numbers:\n\n1.62 * 34.56\n\nwhose result is 55.9872. Note: in line with Anglo-Saxon tradition, R uses decimal points instead of decimal commas. Entering 1,62 * 34,56 will throw an error.\nR also has many basic mathematical functions built into it. For example, sqrt() is the square root function; cos() is the cosine function, log() is the (natural) logarithm, exp() is the exponential function, and so on. The following tables summarize the symbols for various arithmetic operations and basic mathematical functions built into R:\n\n\n\nSymbol\nMeaning\nExample\nForm in R\n\n\n\n\n+\naddition\n2 + 3\n2 + 3\n\n\n-\nsubtraction\n5 - 1\n5 - 1\n\n\n*\nmultiplication\n2 · 6\n2 * 6\n\n\n/\ndivision\n9 / 5\n9 / 5\n\n\n^\nraise to power\n32\n3 ^ 2\n\n\n\n\n\n\nFunction\nMeaning\nExample\nForm in R\n\n\n\n\nlog()\nnatural log\nlog(4)\nlog(4)\n\n\nexp()\nexponential\ne4\nexp(4)\n\n\nsqrt()\nsquare root\n√4\nsqrt(4)\n\n\nlog2()\nbase-2 log\nlog2(4)\nlog2(4)\n\n\nlog10()\nbase-10 log\nlog10(4)\nlog10(4)\n\n\nsin()\nsine (radians!)\nsin(4)\nsin(4)\n\n\nabs()\nabsolute value\n|-4|\nabs(-4)\n\n\n\nExpressions built from these basic blocks can be freely combined. Try to calculate 3log(4) - sin(e2) for instance. To do so, we simply type\n\n3^log(4) - sin(exp(2))\n\nand press Enter to get the result, 3.692108. Now obtain e1.3(4 - sin(π/3)). Notice the parentheses enclosing 4 - sin(π/3). This means, as usual, that this expression is evaluated first, before any of the other computations. It can be implemented in R the same way, by using parentheses:\n\nexp(1.3) * (4 - sin(3.14159 / 3))\n\nNote also that you do need to indicate the symbol for multiplication between closing and opening parentheses: omitting this results in an error. Try it: entering exp(1.3)(4 - sin(3.14159/3)) instead of exp(1.3)*(4 - sin(3.14159/3)) throws an error message. Also, be mindful that exp(1.3)*(4 - sin(3.14159/3)) is not the same as exp(1.3)*4 - sin(3.14159/3). This is because multiplication takes precedence over addition and subtraction, meaning that multiplications and divisions are performed first, and additions/subtractions get executed only afterwards—unless, of course, we override this behaviour with parentheses. In general, whenever you are uncertain about the order of execution of operations, it can be useful to explicitly use parentheses, even if it turns out they aren’t really necessary. For instance, you might be uncertain whether 3 * 6 + 2 first multiplies 3 by 6 and then adds 2 to the result, or if it first adds 2 to 6 and then multiplies that by 3. In that case, if you want to be absolutely sure that you perform the multiplication first, just write (3 * 6) + 2, explicitly indicating with the parentheses that the multiplication should be performed first—even though doing so would not be strictly necessary in this case.\nIncidentally, you do not need to type out 3.14159 to approximate π in the mathematical expressions above. R has a built-in constant, pi, that you can use instead. Therefore, exp(1.3)*(4 - sin(pi/3)) produces the same result as our earlier exp(1.3)*(4 - sin(3.14159/3)).\nAnother thing to note is that the number of spaces between various operations is irrelevant. 4*(9-6) is the same as 4*(9 - 6), or 4 * (9 - 6), or, for that matter, 4   * (9-    6). To the machine, they are all the same—it is only us, the human users, who might get confused by that last form…\nIt is possible to get help on any function from the system itself. Type either help(asin) or the shorter ?asin in the console to get information on the function asin, for instance. Whenever you are not sure how to use a certain function, just ask the computer."
  },
  {
    "objectID": "R_programming_basics.html#variables-and-types",
    "href": "R_programming_basics.html#variables-and-types",
    "title": "2  R programming basics",
    "section": "2.2 Variables and types",
    "text": "2.2 Variables and types\n\n2.2.1 Numerical variables and variable names\nYou can assign a value to a named variable, and then whenever you call on that variable, the assigned value will be substituted. For instance, to obtain the square root of 9, you can simply type sqrt(9); or you can assign the value 9 to a variable first:\n\nx <- 9\nsqrt(x)\n\nThis will calculate the square root of x, and since x was defined as 9, we get sqrt(9), or 3.\nThe name for a variable can be almost anything, but a few restrictions apply. First, the name must consist only of letters, numbers, the period (.), and the underscore (_) character. Second, the variable’s name cannot start with a number or an underscore. So one_result or one.result are fine variable names, but 1_result or _one_result are not. Similarly, the name crowns to $ is not valid because of the spaces and the dollar ($) symbol, neither of which are numbers, letters, period, or the underscore.\nAdditionally, there are a few reserved words which have a special meaning in R, and therefore cannot be used as variable names. Examples are: if, NA, TRUE, FALSE, NULL, and function. You can see the complete list by typing ?Reserved.\nHowever, one can override all these rules and give absolutely any name to a variable by enclosing it in backward tick marks (` `). So while crowns to $ and function are not valid variable names, `crowns to $` and `function` are! For instance, you could type\n\n`crowns to $` <- 0.09 # Approximate SEK-to-USD exchange rate\nmy_money <- 123 # Assumed to be given in Swedish crowns\nmy_money_in_USD <- my_money * `crowns to $`\nprint(my_money_in_USD)\n\n[1] 11.07\n\n\nto get our money’s worth in US dollars. Note that the freedom of naming our variables whatever we wish comes at the price of having to always include them between back ticks to refer to them. It is entirely up to you whether you would like to use this feature or avoid it; however, be sure to recognize what it means when looking at R code written by others.\nNotice also that the above chunk of code includes comments, prefaced by the hash (#) symbol. Anything that comes after the hash symbol on a line is ignored by R; it is only there for other humans to read.\n\n\n\n\n\n\nWarning\n\n\n\nThe variable my_money_in_USD above was defined in terms of the two variables my_money and `crowns to $`. You might be wondering: if we change my_money to a different value by executing my_money <- 1000 (say), does my_money_in_USD also get automatically updated? The answer is no: the value of my_money_in_USD will remain unchanged. In other words, variables are not automatically recalculated the way Excel formula cells are. To recompute my_money_in_USD, you will need to execute my_money_in_USD <- my_money * `crowns to $` again. This leads to a recurring theme in programming: while assigning variables is convenient, it also carries some dangers, in case we forget to appropriately update them. In this course, we will be emphasizing a style of programming which avoids relying on (re-)assigning variables as much as possible.\n\n\n\n\n2.2.2 Strings\nSo far we have worked with numerical data. R can also work with textual information. In computer science, these are called character strings, or just strings for short. To assign a string to a variable, one has to enclose the text in quotes. For instance,\n\ns <- \"Hello World!\"\n\nassigns the literal text Hello World! to the variable s. You can print it to screen either by just typing s at the console and pressing Enter, or typing print(s) and pressing Enter.\nOne useful function that works on strings is paste(), which makes a single string out of several ones (in computer lingo, this is known as string concatenation). For example, try\n\ns1 <- \"Hello\"\ns2 <- \"World!\"\nmessage <- paste(s1, s2)\nprint(message)\n\n[1] \"Hello World!\"\n\n\nThe component strings are separated by a space, but this can be changed with the optional sep argument to the paste() function:\n\nmessage <- paste(s1, s2, sep = \"\")\nprint(message)\n\n[1] \"HelloWorld!\"\n\n\nThis results in message becoming HelloWorld!, without the space in between. Between the quotes, you can put any character (including nothing, like above), which will be used as a separator when merging the strings s1 and s2. So specifying sep = \"-\" would have set message equal to Hello-World! (try it out and see how it works).\nIt is important to remember that quotes distinguish information to be treated as text from information to be treated as numbers. Consider the following two variable assignments:\n\na <- 6.7\nb <- \"6.7\"\n\nAlthough they look superficially similar, a is the number 6.7 while b is the string “6.7”, and the two are not equal! For instance, executing 2 * a results in 13.4, but 2 * b throws an error, because it does not make sense to multiply a bunch of text by 2.\n\n\n2.2.3 Logical values\nLet us type the following into the console, and press Enter:\n\n2 > 1\n\n[1] TRUE\n\n\nWe are asking the computer whether 2 is larger than 1. And it returns the answer: TRUE. By contrast, if we ask whether two is less than one, we get FALSE:\n\n2 < 1\n\n[1] FALSE\n\n\nSimilar to “greater than” and “less than”, there are other logical operations as well, such as “greater than or equal to”, “equal to”, “not equal to”, and others. The table below lists the most common options.\n\n\n\nSymbol\nMeaning\nExample in R\nResult\n\n\n\n\n<\nless than\n1 < 2\nTRUE\n\n\n>\ngreater than\n1 > 2\nFALSE\n\n\n<=\nless than or equal\n2 <= 5.3\nTRUE\n\n\n>=\ngreater than or equal\n4.2 >= 3.6\nTRUE\n\n\n==\nequal to\n5 == 6\nFALSE\n\n\n!=\nnot equal to\n5 != 6\nTRUE\n\n\n!\nnot\n!FALSE\nTRUE\n\n\n%in%\nis element of set\n2 %in% c(1, 2, 3)\nTRUE\n\n\n\nThe == and != operators can also be used with strings: \"Hello World\" == \"Hello World!\" returns FALSE, because the two strings are not exactly identical, differing in the final exclamation mark. Similarly, \"Hello World\" != \"Hello World!\" returns TRUE, because it is indeed true that the two strings are unequal.\nLogical values can either be TRUE or FALSE, with no other options.1 This is in contrast with numbers and character strings, which can take on a myriad different values. Note that TRUE and FALSE must be capitalized: true, False, or anything other than the fully capitalized forms will result in an error. Just like in the case of strings and numbers, logical values can be assigned to variables:\n\nlgl <- 3 > 4 # Since 3 > 4 is FALSE, lgl will be assigned FALSE\nprint(!lgl) # lgl is FALSE, so !lgl (\"not lgl\") will be TRUE\n\n[1] TRUE\n\n\nThe function ifelse takes advantage of logical values, doing different things depending on whether some condition is TRUE or FALSE (“if the condition is true then do something, else do some other thing”). It takes three arguments: the first is a condition, the second is the expression that gets executed only if the condition is true, and the third is the expression that executes only if the condition is false. To illustrate its use, we can apply it in a program that simulates a coin toss. R will generate n random numbers between 0 and 1 by invoking runif(n). Here runif is a shorthand for “random-uniform”, randomly generated numbers from a uniform distribution between 0 and 1. The function call runif(1) therefore produces a single random number, and we can interpret values less than 0.5 as having tossed heads, and other values as having tossed tails. The following lines implement this:\n\n\n\n\ntoss <- runif(1)\ncoin <- ifelse(toss < 0.5, \"heads\", \"tails\")\nprint(coin)\n\n[1] \"heads\"\n\n\nThis time we happened to have tossed heads, but try re-running the above three lines over and over again, to see that the results keep coming up at random."
  },
  {
    "objectID": "R_programming_basics.html#vectors",
    "href": "R_programming_basics.html#vectors",
    "title": "2  R programming basics",
    "section": "2.3 Vectors",
    "text": "2.3 Vectors\nA vector is simply a sequence of variables of the same type. That is, the sequence may consist of numbers or strings or logical values, but one cannot intermix them. The c() function will create a vector in the following way:\n\nx <- c(2, 5, 1, 6, 4, 4, 3, 3, 2, 5)\n\nThis is a vector of numbers. If, after entering this line, you type x or print(x) and press Enter, all the values in the vector will appear on screen:\n\nx\n\n [1] 2 5 1 6 4 4 3 3 2 5\n\n\nWhat can you do if you want to display only the third entry? The way to do this is by applying brackets:\n\nx[3]\n\n[1] 1\n\n\nNever forget that vectors and its elements are simply variables! To show this, calculate the value of x[1] * (x[2] + x[3]), but before pressing Enter, guess what the result will be. Then check if you were correct. You can also try typing x * 2:\n\nx * 2\n\n [1]  4 10  2 12  8  8  6  6  4 10\n\n\nWhat happened? Now you performed an operation on the vector as a whole, i.e., you multiplied each element of the vector by two. Remember: you can perform all the elementary operations on vectors as well, and then the result will be obtained by applying the operation on each element separately.\nCertain functions are specific to vectors. Try mean(x) and var(x) for instance (if you are not sure what these do, just ask by typing ?mean or ?var). Some others to try: max, min, length, and sum.\nOne can quickly generate vectors of sequences of values, using one of two ways. First, the notation 1:10 generates a vector of integers ranging from 1 to 10, in steps of 1. (Similarly, 2:7 generates the same vector as c(2, 3, 4, 5, 6, 7), and so on). Second, the function seq() generates sequences, starting with the first argument, ending with the last, in steps defined by an optional by argument. So calling\n\nseq(0, 10, by = 0.1)\n\n  [1]  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.0  1.1  1.2  1.3  1.4\n [16]  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9\n [31]  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9  4.0  4.1  4.2  4.3  4.4\n [46]  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4  5.5  5.6  5.7  5.8  5.9\n [61]  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9  7.0  7.1  7.2  7.3  7.4\n [76]  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4  8.5  8.6  8.7  8.8  8.9\n [91]  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9 10.0\n\n\ncreates a vector of numbers ranging from 0 to 10, in steps of 0.1.\nJust as one can create a vector of numerical values, it is also possible to create a vector of character strings of logical values. For example:\n\nstringVec <- c(\"I am the first string\", \"I am the second\", \"And I am the 3rd\")\n\nNow stringVec[1] is simply equal to the string \"I am the first string\", stringVec[2] is equal to \"I am the second\", and so on. Similarly, defining\n\nlogicVec <- c(TRUE, FALSE, TRUE, TRUE)\n\ngives a vector whose second entry, logicVec[2], is equal to FALSE, and its other three entries are TRUE."
  },
  {
    "objectID": "R_programming_basics.html#functions",
    "href": "R_programming_basics.html#functions",
    "title": "2  R programming basics",
    "section": "2.4 Functions",
    "text": "2.4 Functions\nA function in R can be thought of as a black box which receives inputs and, depending on those inputs, produces some output. Vending machines provide a good working model of what a “function” is in computer science: depending on the inputs they receive (in the form of coins of various denomination, plus the buttons you press for a particular item) they give you some output (Mars bars, Coke, and the like). It’s just that computer scientists like to refer to the inputs as “function arguments” or simply “arguments” instead of coins, and to the output as the “return value” instead of Red Bull. Arguments are often also referred to as “parameters” to the function.\nWe have already seen some functions at work in R: sqrt and log are functions, but so are setwd (which, as you may recall, will set your working directory) and library (which loads R packages). The general workings of a function are illustrated below:\n                 --------------\nargument 1 ----> |            |\nargument 2 ----> |            |\nargument 3 ----> |  FUNCTION  | ----> return value\n...              |            |\nargument n ----> |            |\n                 --------------\nWhen you ask a function to do something, you’re calling the function. The arguments of functions are always enclosed in parentheses. For example, executing sqrt(9), calls the built-in square root function. Its argument (or input, or parameter) is 9, and its return value is the square root of 9, which is 3.\n\n2.4.1 User-defined functions\nThus far, we have been using many built-in functions in R, such as exp(), log(), sqrt(), setwd(), and others. However, it is also possible to define our own functions, which can then be used just like any built-in function. The way to do this is to use the function keyword, followed by the function’s arguments in parentheses, and then the R code comprising the function’s body enclosed in curly braces {}. For example, here is a function which calculates the area of a circle with radius r:\n\ncircleArea <- function(r) {\n  area <- r^2 * pi\n  return(area)\n}\n\nThe function implements the formula that the area of a circle is equal to π times its radius squared. The return keyword determines what result the function will output when it finishes executing. In this case, the function returns the value of area that is created within the function. After running the above lines, the computer now “knows” the function. Calling circleArea(3) will, for example, calculate the area of a circle with radius 3, which is approximately 28.27433.\nOne can define functions with more than one argument. For instance, here is a function that calculates the volume of a cylinder with radius r and height h:\n\ncylinderVol <- function(r, h) {\n  baseArea <- circleArea(r)\n  volume <- baseArea * h\n  return(volume)\n}\n\nHere we used the fact that the volume of a cylinder is the area of its base circle, times its height. Notice also that we made use of our earlier circleArea function within the body of cylinderVol. While this was not a necessity and we could have simply written volume <- r^2 * pi * h above, this is generally speaking good practice: by constructing functions to solve smaller problems, you can write slightly more complicated functions which make use of those simpler ones. Then, you will be able to write even more complex functions using the slightly more complex ones in turn—and so on. We will discuss this principle in more detail below, in Section 2.4.3.\nOne very important property of functions is that any variables defined within them (such as volume above) are local to that function. This means that they are not visible from outside: even after calling the function, the variable volume will not be accessible to the rest of the program, despite the fact that it was declared in the function. This helps us create programs with modular structure, where functions operate as black boxes: we can use them without looking inside.\nWhen calling a function, it is optional but possible to name the arguments explicitly. This means that calling circleArea(3) is the same as calling circleArea(r = 3), and calling cylinderVol(2, 3) is the same as calling cylinderVol(r = 2, h = 3). Even more is true: since naming the arguments removes any ambiguity about which argument is which, one may even call cylinderVol(h = 3, r = 2), with the arguments in reverse order, and this will still be equivalent to cylinderVol(2, 3). As mentioned, naming arguments this way is optional, but it can be useful to do so, because it can increase the clarity of our programs. To give an example from a built-in function in R, take rep(5, 3). Does this function create a vector with 5 entries, each equal to 3, or does it make a vector with 3 entries, each equal to 5? While reading the documentation (or simply executing these two function calls and comparing the outputs) reveals that it is the latter, one can clarify this easily, because the second argument of rep() is called times, as seen from reading the help after typing ?rep. We can then write rep(5, times = 3), which is now easy to interpret: it is a vector with the number 5 repeated 3 times.\n\nrep(5, times = 3)\n\n[1] 5 5 5\n\n\nOne may even define default values for one or more of the arguments to any function. If defaults are given, the user does not even have to specify the value for that argument. It will then automatically be set to the default value instead. For example, one could rewrite the cylinderVol() function to specify default values for r and h. Making these defaults be 1 means we can write:\n\ncylinderVol <- function(r = 1, h = 1) {\n  baseArea <- circleArea(r)\n  volume <- baseArea * h\n  return(volume)\n}\n\nIf we now call cylinderVol() without specifying arguments, the defaults will be substituted for r and h. Since both are equal to 1, the cylinder volume will simply be π (about 3.14159), which is the result we will get back. Alternatively, if we call cylinderVol(r = 2), then the function returns 4π (approximately 12.56637), because the default value of 1 is substituted in place of the unspecified height argument h. Importantly, if we don’t define default values and yet omit to specify one or more of those parameters, we get back an error message. For example, our earlier circleArea function had no default value for its argument r, so leaving it unspecified throws an error:\n\ncircleArea()\n\nError in circleArea(): argument \"r\" is missing, with no default\n\n\n\n\n2.4.2 Naming rules for functions and the concept of syntactic sugar\nThe rules for naming functions is the same as for naming variables. A valid function name is a combination of letters, numbers, and underscores (_), as long as the first character is not a number or underscore. Additionally, a function’s name cannot be one of the reserved words (see ?Reserved). Just like in the case of variables, one can override this and give any name whatsoever to functions if one encloses the name between back ticks. So while crowns to $ is not a valid function name, `crowns to $` is.\nOne thing to know about R is that even elementary operations are treated as function calls internally. When we write down even something as innocuous as 2 + 5, what really happens is that R calls the function called +, with arguments 2 and 5. In fact, we can write it that way too: 2 + 5 is completely equivalent to writing `+`(2, 5). Note the back ticks around `+`: these are required because + is not a letter, number, or underscore. Whenever we write down 2 + 5, the system internally converts it into `+`(2, 5) first, and then proceeds with the execution. Thus, the fact that we can add two numbers by writing 2 + 5 is just a convenience, a way of entering addition in a way that we tend to be more used to. Such constructions have a name in computer science: they are called syntactic sugar. Writing 2 + 5 is just syntactic sugar for the actual internal form `+`(2, 5), because the latter would be stranger to write. Of course, the same holds for all other elementary operations: `-`, `*`, `/`, and `^` are also functions in R. This means that, e.g., writing `-`(`^`(2, 3), `*`(4, 2)) is equivalent to 2^3 - 4 * 2.\nAnother example for the fact that internally R treats operations as functions is the subsetting of vectors or matrices. As we have learned, given the vector x, typing x[3] will extract the third entry of the vector. In fact, this is again syntactic sugar for easier use. Internally, an expression such as x[3] is actually interpreted as `[`(x, 3). The function `[` (note the back ticks, which are necessary due to the fact that the symbol [ is not a letter, number, or underscore) takes two arguments: a vector, and the index (or indices) which we request from that vector.\nWhile generally speaking, one would never actually want to type `[`(x, 3) instead of x[3] (the reason we have the syntactic sugar is to make our lives easier!), there are situations where being aware of these details of the internal workings of R can be helpful. We will see an example later in this chapter.\n\n\n2.4.3 Function composition\nA function is like a vending machine: we give it some input(s), and it produces some output. The output itself may then be fed as input to another function—which in turn produces an output, which can be fed to yet another function, and so on. Chaining functions together in this manner is called the composition of functions. For example, we might need to take the square root of a number, then calculate the logarithm of the output, and finally, obtain the cosine of the result. This is as simple as writing cos(log(sqrt(9))), if the number we start with is 9. More generally, one might even define a new function (let us call it cls(), after the starting letters of cos, log, and sqrt) like this:\n\ncls <- function(x) {\n  return(cos(log(sqrt(x))))\n}\n\nA remarkable property of composition is that the composed function (in this case, cls) is in many ways just like its constituents: it is also a black box which takes a single number as input and produces another number as its output. Putting it differently, if one did not know that the function cls() was defined by me manually as the composition of three more “elementary” functions, and instead claimed it was just another elementary built-in function in R, there would be no way to tell the difference just based on the behaviour of the function itself. The composition of functions thus has the important property of self-similarity: if we manage to solve a problem through the composition of functions, then that solution itself will behave like an “elementary” function, and so can be used to solve even more complex problems via composition—and so on.\nIf we conceive of a program written in R as a large lego building, then one can think of functions as the lego blocks out of which the whole construction is made. Lego pieces are designed to fit well together, one can always combine them in various ways. Furthermore, any combination of lego pieces itself behaves like a more elementary lego piece: it can be fitted together with other pieces in much the same way. Thus, the composition of functions is analogous to building larger lego blocks out of simpler ones. Remarkably, just as the size of a lego block does not hamper our ability to stick them together, the composability of functions is retained regardless of how many more elementary pieces each of them consist of. Thus, the composition of functions is an excellent way (some claim it is the way) to handle the complexity of large software systems.\n\n\n2.4.4 Function piping\nOne problem with composing many functions together is that the order of application must be read backwards. An expression such as sqrt(sin(cos(log(1)))) means: “take the square root of the sine of the cosine of the natural logarithm of 1”. But it is more convenient for the human brain to think of it the other way round: we first take the log of 1, then the cosine of the result, then the sine of what we got, and finally the square root. The problem of interpreting composed functions gets more difficult when the functions have more than one argument. Even something as relatively simple as\n\nexp(mean(log(seq(-3, 11, by = 2)), na.rm = TRUE))\n\n[1] 4.671655\n\n\nmay cause one to stop and have to think about what this expression actually does—and it only involves the composition of four simple functions. One can imagine the difficulties of having to parse the composition of dozens of functions in this style.\nThe above piece of R code generates the numeric sequence -3, -1, 1, …, 11 (jumping in steps of 2), and computes their geometric mean. To do so, it takes the logarithms of each value, takes their mean, and finally, exponentiates the result back. The problem is that the logarithm of negative numbers does not exist (more precisely, they are not real numbers), and therefore, log(-3) and log(-1) both produce undefined results. Thus, when taking the mean of the logarithms, we must remove any such undefined values. This can be accomplished via an extra argument to mean, called na.rm (“NA-remove”). By default, this is set to FALSE, but by changing it to TRUE, undefined values are simply ignored when computing the mean. For example mean(c(1, 2, 3, NA)) returns NA, because of the undefined entry in the vector; but mean(c(1, 2, 3, NA), na.rm = TRUE) returns 2, the result one gets after discarding the NA entry.\nAll the above is difficult to see when looking at the expression\n\nexp(mean(log(seq(-3, 11, by = 2)), na.rm = TRUE))\n\nPart of the reason is the awkward “backwards order” of function applications, and that it is hard to see which function the argument na.rm = TRUE belongs to. Fortunately, there is a simple operator in R called a pipe (written %>%), which allows one to write the same code in a more streamlined way. The pipe was originally provided by the magrittr package,2 but invoking tidyverse will also load it automatically:\n\nlibrary(tidyverse)\n\nThe pipe allows one to write function application in reverse order (first the argument and then the function), making the code more transparent. Formally, x %>% f() is equivalent to f(x) for any function f. For example, sqrt(9) can also be written 9 %>% sqrt(). Thus, sqrt(sin(cos(log(1)))) can be written as 1 %>% log() %>% cos %>% sin() %>% sqrt(), which reads straightforwardly as “start with the number 1; then take its log; then take the cosine of the result; then take the sine of that result; and then, finally, take the square root to obtain the final output”. In general, it helps to pronounce %>% as “then”.\nThe pipe also works for functions with multiple arguments. In that case, x %>% f(y, ...) is equivalent to f(x, y, ...). That is, the pipe refers to the function’s first argument (though it is possible to override this). Instead of mean(log(seq(-3, 11, by = 2)), na.rm = TRUE), we can therefore write:\n\nseq(-3, 11, by = 2) %>%\n  log() %>%\n  mean(na.rm = TRUE) %>%\n  exp()\n\n[1] 4.671655\n\n\nThis is fully equivalent to the traditional form, but is much more readable, because the functions are written in the order in which they actually get applied. Moreover, even though the program is built only from the composition of functions, it reads straightforwardly as if it was a sequence of imperative instructions: we start from the vector of integers c(-3, -1, 1, 3, 5, 7, 9, 11); then we take the logarithm of each; then we take their average, discarding any invalid entries (produced in this case by taking the logarithm of negative numbers); and then, finally, we exponentiate back the result to obtain the geometric mean."
  },
  {
    "objectID": "R_programming_basics.html#exercises",
    "href": "R_programming_basics.html#exercises",
    "title": "2  R programming basics",
    "section": "2.5 Exercises",
    "text": "2.5 Exercises\n\nWhich of the variable names below are valid, and why?\n\nfirst.result.of_computation\n2nd.result.of_computation\ndsaqwerty\ndsaq werty\n`dsaq werty`\nbreak\nis this valid?...\n`is this valid?...`\nis_this_valid?...\n\nCreate a vector called z, with entries 1.2, 5, 3, 13.7, 6.66, and 4.2 (in that order). Then, by applying functions to this vector, obtain:\n\nIts smallest entry.\nIts largest entry.\nThe sum of all its entries.\nThe number of entries in the vector.\nThe vector’s entries sorted in increasing order (Hint: look up the help for the built-in function sort).\nThe vector’s entries sorted in decreasing order.\nThe product of the fourth entry with the difference of the third and sixth entries. Then take the absolute value of the result.\n\nDefine a vector of strings, called s, with the three entries \"the fat cat\", \"sat on\", and \"the mat\".\n\nCombine these three strings into a single string, and print it on the screen. (Hint: look up the help for the paste function, in particular its collapse argument.)\nReverse the entries of s, so they come in the order \"the mat\", \"sat on\", and \"the fat cat\". (Hint: check out the rev function.) Then merge the three strings again into a single one, and print it on the screen.\n\nAssume you have a population of some organism in which one given allele of some gene is the only one available in the gene pool. If a new mutant organism with a different, selectively advantageous allele appears, it would be reasonable to conclude that the new allele will fix in the population and eliminate the original one over time. This, however, is not necessarily true, because a very rare allele might succumb to being eliminated by chance, regardless of how advantageous it is. According to Motoo Kimura’s famous formula, the probability of such a new allele eventually fixing in the population is given as:\n\\[ P = \\frac{1 - \\text{e}^{-s}}{1 - \\text{e}^{-2Ns}} \\]\n(Gillespie 2004). Here P is the probability of eventual fixation, s is the selection differential (the degree to which the new allele is advantageous over the original one), and N is the (effective) population size.\n\nWrite a function that implements this formula. It should take the selection differential s and the population size N as parameters, and return the fixation probability as its result.\nA selection differential of 0.5 is very strong (though not unheard of). What is the likelihood that an allele with that level of advantage will fix in a population of 1000 individuals? Interpret the result.\n\nA text is palindromic if it reads backwards the same as it reads forwards. For example, “racecar”, “deified”, and “rotator” are all palindromic words. Assume that you are given a word in all lowercase, broken up by characters. For instance, you could be given the vector c(\"r\", \"a\", \"c\", \"e\", \"c\", \"a\", \"r\") (a palindrome) or c(\"h\", \"e\", \"l\", \"l\", \"o\") (not a palindrome).\n\nWrite a function which checks whether the vector encodes a palindromic text. The function should return TRUE if the text is a palindrome, and FALSE otherwise. (Hint: reverse the text, collapse both the original and the reversed vectors into single strings, and then compare them using logical equality.)\nModify the function to allow for both upper- and lowercase text, treating case as irrelevant (i.e., \"A\" is treated to be equal to \"a\" when evaluating whether the text is palindromic). One simple way to do this is to convert each character of the text into uppercase (or lowercase; it doesn’t matter which), and use this standardized text for reversing and comparing with. Look up the functions toupper and tolower, and implement this improvement in your palindrome checker function.\nIf you haven’t done so already: try to rewrite your function to rely as much on function composition as possible.\n\n\n\n\n\n\nGillespie, John H. 2004. Population Genetics: A Concise Guide. Baltimore, MD, USA: Johns Hopkins University Press."
  },
  {
    "objectID": "Data_reading.html",
    "href": "Data_reading.html",
    "title": "3  Reading tabular data from disk",
    "section": "",
    "text": "A suite of R packages, sharing the same design philosophy, are collected under the name tidyverse. In case this is not yet installed on your computer, type\n\ninstall.packages(\"tidyverse\")\n\nat the R console and press Enter. After making sure that the package is installed, you must load it. This is done via the function call\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.0 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nAs you see, eight packages are now loaded, called ggplot2, tibble, and so on. We will get to know these in more detail throughout the course.\nThere are actually even more packages that are part of the tidyverse. Typing and executing tidyverse_packages() will show all such packages. Of all these options, only eight are loaded by default when invoking library(tidyverse). The others must be loaded separately. For example, readxl is a tidyverse package for loading Excel files in R. To use it, execute library(readxl).\nIn general, it is a good idea to load all necessary packages at the top of your R script. There are two reasons for this. First, once you close RStudio, it forgets the packages, which do not get automatically reloaded after reopening RStudio. Second, often other users will run the scripts you write on their own computers, and they will not be able to do so unless the proper packages are loaded first. It is then helpful to others if the necessary packages are all listed right at the top, showing what is needed to run your program."
  },
  {
    "objectID": "Data_reading.html#reading-tabular-data",
    "href": "Data_reading.html#reading-tabular-data",
    "title": "3  Reading tabular data from disk",
    "section": "3.2 Reading tabular data",
    "text": "3.2 Reading tabular data\nOne of the packages loaded by default with tidyverse is called readr. This package contains tools for loading data files, and writing them to disk. To see how it works, download the files Goldberg2010_data.csv, Goldberg2010_data.xlsx, and Smith2003_data.txt from Lisam. Then set the working directory in RStudio to the folder where you have saved them (as a reminder, you can do this by executing setwd(/path/to/files), where you should substitute in your own path in place of /path/to/files).\n\n3.2.1 The CSV file format\nGoldberg et al. (2010) collected data on self-incompatibility in the family Solanaceae (nightshades). It contains a list of 356 species, along with a flag determining self-incompatibility status (0: self-incompatible; 1: self-compatible; 2-5: more complicated selfing scenarios). The data are in the file Goldberg2010_data.csv. This is a so-called comma-separated value (CSV) file, meaning that the different columns of the data are separated by commas. One can see this by viewing the file in any simple text editor. For example, this can be done in RStudio itself, by clicking on the file in the Files panel in the lower right part of the RStudio window, and then choosing the option “View file” (ignore the other option called “Import dataset…”). Having done this, a new tab opens in your editor panel (upper left region) where you should see something like the following:\nSpecies,Status\nAcnistus_arborescens,1\nAnisodus_tanguticus,1\nAtropa_belladonna,1\nBrachistus_stramonifolius,1\nBrugmansia_aurea,0\nBrugmansia_sanguinea,0\nCapsicum_annuum,1\nCapsicum_baccatum,1\nCapsicum_cardenasii,2\nCapsicum_chacoense,1\nAnd so on. As you can see, the first line (Species,Status) is actually an indicator of what the corresponding columns of data will contain: the first column has the species name, and the second one the numerical flag indicating self-compatibility status. The subsequent rows hold the actual data. Notice that the boundary between the columns is always indicated by a comma. This is what gave rise to the name “comma-separated value” (CSV) file.\nThe above raw format is not yet amenable to processing within R. To make it so, we first need to import the data. For comma-separated value files, there is a convenient function, read_csv, that makes this especially simple:1\n\nread_csv(\"Goldberg2010_data.csv\")\n\n# A tibble: 356 × 2\n   Species                   Status\n   <chr>                      <dbl>\n 1 Acnistus_arborescens           1\n 2 Anisodus_tanguticus            1\n 3 Atropa_belladonna              1\n 4 Brachistus_stramonifolius      1\n 5 Brugmansia_aurea               0\n 6 Brugmansia_sanguinea           0\n 7 Capsicum_annuum                1\n 8 Capsicum_baccatum              1\n 9 Capsicum_cardenasii            2\n10 Capsicum_chacoense             1\n# … with 346 more rows\n\n\n(We will interpret the output in the next subsection.) The above line loads the data, but does not save it into a variable. That is perfectly fine in case we immediately start performing operations on it via function composition (we will see many, many examples later on). However, in case we do want to assign the result to a variable, we can do so without problems. For instance, to put the table into the variable dat, we simply write:\n\ndat <- read_csv(\"Goldberg2010_data.csv\")\n\n\n\n3.2.2 The tibble data structure\nLook at the output produced by read_csv(\"Goldberg2010_data.csv\") above. You can mostly ignore the top part of that output—it simply provides information on how it interpreted the data it just read in. Instead, the interesting part starts with A tibble: 356 x 2. A tibble (or data frame2) is the R-equivalent of an Excel-style spreadsheets. In this case, it has 356 rows and 2 columns (hence the 356 x 2). The simplest way to conceive of a tibble is as a collection of vectors, glued together side-by-side to form a table of data. Importantly, although each vector must consist of entries of the same type, as usual (e.g., they can be vectors of numbers, vectors of strings, or vectors of logical values), the different columns need not share types. For example, in the above table, the first column consists of character strings, but the second one consists of numerical values. This can be seen right below the header information. Below Species, you can see <chr>, which stands for “character string”. Below Status, we have <dbl> which, confusing as it may look at first sight, refers simply to ordinary numbers.3 In turn, columns comprising of logical values would have the tag <lgl> underneath them (in this case though, we don’t have such a column). The point is that by looking at the type information below the header, you can see how R has interpreted each of the columns at a glance.\nThe fact that the individual columns are simply vectors can be made explicit, by relying on the $-notation. To access a given column of the table as a vector, we write the name of the table, followed by the $ symbol, followed by the name of the column in question. For example, we can access the Status column from the dat table as a vector of numbers like this:\n\ndat$Status\n\n  [1] 1 1 1 1 0 0 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 0 0 1 1 1 1 0 4 0 0 0 1 1 1 0\n [38] 0 0 0 0 1 1 1 1 1 1 0 0 0 0 4 0 3 0 0 4 0 4 4 0 4 1 0 0 0 4 4 4 0 1 1 0 1\n [75] 1 1 1 0 0 1 1 1 1 1 1 1 0 1 2 1 1 1 1 1 1 2 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1\n[112] 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 2 0 0 2 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0\n[149] 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 4 1 1 0 0 1 0 0 0 1 1 1 1 1 0 4 0 4 0 1 2 0\n[186] 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 4 4 1 1 1 4 0 0 0 0 1 1 0 1 1 0 0 1 2 1 1 0\n[223] 1 2 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 4 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0\n[260] 1 0 1 1 2 1 1 1 1 0 0 0 1 2 1 0 0 1 1 4 1 0 2 0 4 1 1 0 0 0 1 2 4 1 1 1 1\n[297] 1 1 1 1 1 1 1 0 2 0 1 1 0 2 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 2 4\n[334] 0 4 0 1 1 1 1 1 0 1 1 5 4 4 1 4 1 0 0 0 0 0 2\n\n\nHere dat$Status is really just a vector, and can be treated as such. For example, to get the 9th entry of this vector, we can use the usual bracket notation:\n\ndat$Status[9]\n\n[1] 2\n\n\nThe result is an ordinary numerical value.\nFinally, let us take one more look at the output again:\n\nprint(dat)\n\n# A tibble: 356 × 2\n   Species                   Status\n   <chr>                      <dbl>\n 1 Acnistus_arborescens           1\n 2 Anisodus_tanguticus            1\n 3 Atropa_belladonna              1\n 4 Brachistus_stramonifolius      1\n 5 Brugmansia_aurea               0\n 6 Brugmansia_sanguinea           0\n 7 Capsicum_annuum                1\n 8 Capsicum_baccatum              1\n 9 Capsicum_cardenasii            2\n10 Capsicum_chacoense             1\n# … with 346 more rows\n\n\nWhen displaying large tibbles, R will not dump all the data at you. Instead, it will display the first 10 rows, with a message indicating how many more rows remain (in our case, we have ...with 346 more rows written at the end of the printout). The system is still aware of the other rows; it just does not show them. To get a full view of a tibble in a more digestible, spreadsheet-like style, one can use the view function. Try running view(dat) and see what happens!\n\n\n3.2.3 The TSV file format\nAnother type of file is one where the columns are separated by tabulators instead of commas. These are called tab-separated value (TSV) files. An example is provided by the file associated with data from Smith et al. (2003). The authors compiled a database of the body mass of mammals of the late Quaternary period. The data file is Smith2003_data.txt. Its rows are the different mammal species; its columns are: the species’ native continent; whether the species is still alive or extinct; the order, family, genus, and species names; the base-10 log body mass; the actual body mass (in grams); and numbered references representing research papers which served as the source of the data.\nViewing the TSV file Smith2003_data.txt in its raw form begins something like the following:\nAF  extant  Artiodactyla    Bovidae Addax   nasomaculatus   4.85    70000.3 60\nAF  extant  Artiodactyla    Bovidae Aepyceros   melampus    4.72    52500.1 \"63, 70\"\nAF  extant  Artiodactyla    Bovidae Alcelaphus  buselaphus  5.23    171001.5    \"63, 70\"\nAF  extant  Artiodactyla    Bovidae Ammodorcas  clarkei 4.45    28049.8 60\nAF  extant  Artiodactyla    Bovidae Ammotragus  lervia  4.68    48000   75\nAF  extant  Artiodactyla    Bovidae Antidorcas  marsupialis 4.59    39049.9 60\nAF  extinct Artiodactyla    Bovidae Antidorcas  bondi   4.53    34000   1\nAF  extinct Artiodactyla    Bovidae Antidorcas  australis   4.6 40000   2\nAF  extant  Artiodactyla    Bovidae Bos taurus  5.95    900000  -999\nAF  extant  Artiodactyla    Bovidae Capra   walie   5   100000  -999\nSince the file is tab- and not comma-separated, trying to load it using read_csv will not work correctly:\n\nread_csv(\"Smith2003_data.txt\")\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nRows: 5730 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): AF extant  Artiodactyla    Bovidae Addax   nasomaculatus   4.85    70000.3 60\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 5,730 × 1\n   `AF\\textant\\tArtiodactyla\\tBovidae\\tAddax\\tnasomaculatus\\t4.85\\t70000.3\\t60` \n   <chr>                                                                        \n 1 \"AF\\textant\\tArtiodactyla\\tBovidae\\tAepyceros\\tmelampus\\t4.72\\t52500.1\\t\\\"63…\n 2 \"AF\\textant\\tArtiodactyla\\tBovidae\\tAlcelaphus\\tbuselaphus\\t5.23\\t171001.5\\t…\n 3 \"AF\\textant\\tArtiodactyla\\tBovidae\\tAmmodorcas\\tclarkei\\t4.45\\t28049.8\\t60\"  \n 4 \"AF\\textant\\tArtiodactyla\\tBovidae\\tAmmotragus\\tlervia\\t4.68\\t48000\\t75\"     \n 5 \"AF\\textant\\tArtiodactyla\\tBovidae\\tAntidorcas\\tmarsupialis\\t4.59\\t39049.9\\t…\n 6 \"AF\\textinct\\tArtiodactyla\\tBovidae\\tAntidorcas\\tbondi\\t4.53\\t34000\\t1\"      \n 7 \"AF\\textinct\\tArtiodactyla\\tBovidae\\tAntidorcas\\taustralis\\t4.6\\t40000\\t2\"   \n 8 \"AF\\textant\\tArtiodactyla\\tBovidae\\tBos\\ttaurus\\t5.95\\t900000\\t-999\"         \n 9 \"AF\\textant\\tArtiodactyla\\tBovidae\\tCapra\\twalie\\t5\\t100000\\t-999\"           \n10 \"AF\\textant\\tArtiodactyla\\tBovidae\\tCapra\\tibex\\t5\\t100999.7\\t65\"            \n# … with 5,720 more rows\n\n\nAs you can see, there is even a warning at the top about “One or more parsing issues”, meaning that read_csv had a hard time reading in the file. Below the message, you can also see that the attempted read is a mess, with all data in the rows treated as being part of a single column.\nInstead, to correctly read TSV files, one should use read_tsv:\n\nread_tsv(\"Smith2003_data.txt\")\n\n# A tibble: 5,730 × 9\n   AF    extant  Artiodactyla Bovidae Addax nasomaculatus `4.85` `70000.3` `60` \n   <chr> <chr>   <chr>        <chr>   <chr> <chr>          <dbl>     <dbl> <chr>\n 1 AF    extant  Artiodactyla Bovidae Aepy… melampus        4.72    52500. 63, …\n 2 AF    extant  Artiodactyla Bovidae Alce… buselaphus      5.23   171002. 63, …\n 3 AF    extant  Artiodactyla Bovidae Ammo… clarkei         4.45    28050. 60   \n 4 AF    extant  Artiodactyla Bovidae Ammo… lervia          4.68    48000  75   \n 5 AF    extant  Artiodactyla Bovidae Anti… marsupialis     4.59    39050. 60   \n 6 AF    extinct Artiodactyla Bovidae Anti… bondi           4.53    34000  1    \n 7 AF    extinct Artiodactyla Bovidae Anti… australis       4.6     40000  2    \n 8 AF    extant  Artiodactyla Bovidae Bos   taurus          5.95   900000  -999 \n 9 AF    extant  Artiodactyla Bovidae Capra walie           5      100000  -999 \n10 AF    extant  Artiodactyla Bovidae Capra ibex            5      101000. 65   \n# … with 5,720 more rows\n\n\nThis is now a neatly formatted table, with 9 columns as needed.\nThere is a problem though. This file does not contain a header—a row, which is the first in a data file, specifying the names of the various columns. (Recall that the first row of Goldberg2010_data.csv contained not data, but the names of the columns.) Instead, the first row is itself part of the data. To override the default behavior of treating the first row as one of column names, one can use the col_names = FALSE option:\n\nread_tsv(\"Smith2003_data.txt\", col_names = FALSE)\n\n# A tibble: 5,731 × 9\n   X1    X2      X3           X4      X5         X6              X7     X8 X9   \n   <chr> <chr>   <chr>        <chr>   <chr>      <chr>        <dbl>  <dbl> <chr>\n 1 AF    extant  Artiodactyla Bovidae Addax      nasomaculat…  4.85 7.00e4 60   \n 2 AF    extant  Artiodactyla Bovidae Aepyceros  melampus      4.72 5.25e4 63, …\n 3 AF    extant  Artiodactyla Bovidae Alcelaphus buselaphus    5.23 1.71e5 63, …\n 4 AF    extant  Artiodactyla Bovidae Ammodorcas clarkei       4.45 2.80e4 60   \n 5 AF    extant  Artiodactyla Bovidae Ammotragus lervia        4.68 4.8 e4 75   \n 6 AF    extant  Artiodactyla Bovidae Antidorcas marsupialis   4.59 3.90e4 60   \n 7 AF    extinct Artiodactyla Bovidae Antidorcas bondi         4.53 3.4 e4 1    \n 8 AF    extinct Artiodactyla Bovidae Antidorcas australis     4.6  4   e4 2    \n 9 AF    extant  Artiodactyla Bovidae Bos        taurus        5.95 9   e5 -999 \n10 AF    extant  Artiodactyla Bovidae Capra      walie         5    1   e5 -999 \n# … with 5,721 more rows\n\n\nThe col_names argument is set by default to TRUE; in case we wish to override this, we must explicitly change is, just like above.\n\n\n3.2.4 Renaming columns\nWhile the above works, the column names now default to the moderately informative labels X1, X2, and so on. Fortunately, columns can be renamed using the rename function from the tidyverse. This function takes a tibble as its first argument, and a renaming instruction as its second, of the form new_name = old_name. For example, to rename the first column (which, as you may remember, refers to the continent of the corresponding mammal):\n\nsmithData <- read_tsv(\"Smith2003_data.txt\", col_names = FALSE)\nrename(smithData, Continent = X1) # Rename column \"X1\" to \"Continent\"\n\n# A tibble: 5,731 × 9\n   Continent X2      X3           X4      X5         X6          X7     X8 X9   \n   <chr>     <chr>   <chr>        <chr>   <chr>      <chr>    <dbl>  <dbl> <chr>\n 1 AF        extant  Artiodactyla Bovidae Addax      nasomac…  4.85 7.00e4 60   \n 2 AF        extant  Artiodactyla Bovidae Aepyceros  melampus  4.72 5.25e4 63, …\n 3 AF        extant  Artiodactyla Bovidae Alcelaphus buselap…  5.23 1.71e5 63, …\n 4 AF        extant  Artiodactyla Bovidae Ammodorcas clarkei   4.45 2.80e4 60   \n 5 AF        extant  Artiodactyla Bovidae Ammotragus lervia    4.68 4.8 e4 75   \n 6 AF        extant  Artiodactyla Bovidae Antidorcas marsupi…  4.59 3.90e4 60   \n 7 AF        extinct Artiodactyla Bovidae Antidorcas bondi     4.53 3.4 e4 1    \n 8 AF        extinct Artiodactyla Bovidae Antidorcas austral…  4.6  4   e4 2    \n 9 AF        extant  Artiodactyla Bovidae Bos        taurus    5.95 9   e5 -999 \n10 AF        extant  Artiodactyla Bovidae Capra      walie     5    1   e5 -999 \n# … with 5,721 more rows\n\n\nAs seen, the name of the first column now reads Continent instead of X1. One can similarly rename other columns as well.\n\n\n3.2.5 Excel tables\nFinally, although their use is discouraged in science, one should know how to read in data from an Excel spreadsheet. To do this, one needs to load the readxl package. This package is part of the tidyverse, but does not get automatically loaded when executing library(tidyverse). Therefore, we first load the package:\n\nlibrary(readxl)\n\nWe can now load Excel files with the function read_excel(). At the start, we downloaded an Excel version of the data from Goldberg et al. (2010), called Goldberg2010_data.xlsx. It holds the exact same data as the original CSV file, just saved in Excel format for instructive purposes. Let us load this file:\n\nread_excel(\"Goldberg2010_data.xlsx\")\n\n# A tibble: 356 × 2\n   Species                   Status\n   <chr>                      <dbl>\n 1 Acnistus_arborescens           1\n 2 Anisodus_tanguticus            1\n 3 Atropa_belladonna              1\n 4 Brachistus_stramonifolius      1\n 5 Brugmansia_aurea               0\n 6 Brugmansia_sanguinea           0\n 7 Capsicum_annuum                1\n 8 Capsicum_baccatum              1\n 9 Capsicum_cardenasii            2\n10 Capsicum_chacoense             1\n# … with 346 more rows\n\n\nThe functions read_csv, read_tsv, and read_excel have several further options. For example, given an Excel table with multiple sheets, one can specify which one to import, using the sheet argument. Check the help pages of these functions, and experiment with their options.\n\n\n3.2.6 Writing data to files\nFinally, data can not only be read from a file, but also written out to one. Then, instead of read_csv, read_tsv and the like, one uses write_csv, write_tsv, and so on. For instance, to save dat in CSV form:\n\nwrite_csv(dat, \"/path/to/file.csv\")\n\nwhere /path/to/file.csv should be replaced by the path and file name with which the data should be saved."
  },
  {
    "objectID": "Data_reading.html#exercises",
    "href": "Data_reading.html#exercises",
    "title": "3  Reading tabular data from disk",
    "section": "3.3 Exercises",
    "text": "3.3 Exercises\n\nLoad the data from the file Smith2003_data.txt. Note that the file is tab-separated and lacks headers!\nThe columns of this file are, in order: Continent (AF=Africa, etc.), Status (extinct, historical, introduction, or extant), Order, Family, Genus, Species, Base-10 Log Mass, Combined Mass (grams), and Reference (numbers, referring to a numerically ordered list of published works – no need to worry about the details). Rename each column appropriately, using the rename() function.\n\n\n\n\n\nGoldberg, Emma E., Joshua R. Kohn, Russell Lande, Kelly A. Robertson, Stephen A. Smith, and Boris Igić. 2010. “Species Selection Maintains Self-Incompatibility.” Science 330 (6003): 493–95. https://doi.org/10.1126/science.1194513.\n\n\nSmith, Felisa A., S. Kathleen Lyons, S. K. Morgan Ernest, Kate E. Jones, Dawn M. Kaufman, Tamar Dayan, Pablo A. Marquet, James H. Brown, and John P. Haskell. 2003. “Body Mass of Late Quaternary Mammals.” Ecology 84 (12): 3403. https://doi.org/10.1890/02-9003."
  },
  {
    "objectID": "Basic_data_wrangling.html",
    "href": "Basic_data_wrangling.html",
    "title": "4  Basic data manipulation",
    "section": "",
    "text": "Let us start by loading tidyverse, in case you have not done so yet:\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.0 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nAs you can see from the message output above, the dplyr package is part of tidyverse, which gets loaded by default. It allows one to arrange and manipulate data efficiently. The basic functions one should know are rename, select, filter, slice, arrange, and mutate. The first of these we have already looked at in Section 3.2.4. Let us then see some examples of the latter ones. First, we will load the Goldberg2010_data.csv data file (also discussed in the previous chapter):\n\ndat <- read_csv(\"Goldberg2010_data.csv\")\nprint(dat)\n\n# A tibble: 356 × 2\n   Species                   Status\n   <chr>                      <dbl>\n 1 Acnistus_arborescens           1\n 2 Anisodus_tanguticus            1\n 3 Atropa_belladonna              1\n 4 Brachistus_stramonifolius      1\n 5 Brugmansia_aurea               0\n 6 Brugmansia_sanguinea           0\n 7 Capsicum_annuum                1\n 8 Capsicum_baccatum              1\n 9 Capsicum_cardenasii            2\n10 Capsicum_chacoense             1\n# … with 346 more rows\n\n\nNow we will give examples of each of the functions select, filter, slice, arrange, and mutate. They are similar to our earlier rename in that the first argument they take is the data, in the form of a tibble. Their other arguments, and what they each do, are explained below.\n\n\nThis function chooses columns of the data. The second and subsequent arguments of the function are the columns which should be retained. For example, select(dat, Species) will keep only the Species column of dat:\n\nselect(dat, Species)\n\n# A tibble: 356 × 1\n   Species                  \n   <chr>                    \n 1 Acnistus_arborescens     \n 2 Anisodus_tanguticus      \n 3 Atropa_belladonna        \n 4 Brachistus_stramonifolius\n 5 Brugmansia_aurea         \n 6 Brugmansia_sanguinea     \n 7 Capsicum_annuum          \n 8 Capsicum_baccatum        \n 9 Capsicum_cardenasii      \n10 Capsicum_chacoense       \n# … with 346 more rows\n\n\nIt is also possible to deselect columns, by prepending a minus sign (-) in front of the column names. To drop the Species column, we can type:\n\nselect(dat, -Species)\n\n# A tibble: 356 × 1\n   Status\n    <dbl>\n 1      1\n 2      1\n 3      1\n 4      1\n 5      0\n 6      0\n 7      1\n 8      1\n 9      2\n10      1\n# … with 346 more rows\n\n\nSince there were only two columns in the data to begin with, only the Status column remained in the data after removing Species.\n\n\n\nWhile select chooses columns, filter chooses rows from the data. As with all these functions, the first argument of filter is the data. The second argument is a logical condition on the columns. Those rows which satisfy the condition are retained; the rest are dropped. Thus, filter keeps only those rows of the data which fulfill some condition.\nRecall that in the Goldberg2010_data.csv dataset, a Status of 0 means self-incompatibility; a Status of 1 means self-compatibility, and Status values between 2 and 5 refer to various, more complex selfing mechanisms. So in case we wanted to focus only on those species which exhibit complex selfing, we could filter the data like this:\n\nfilter(dat, Status >= 2)\n\n# A tibble: 44 × 2\n   Species               Status\n   <chr>                  <dbl>\n 1 Capsicum_cardenasii        2\n 2 Capsicum_pubescens         2\n 3 Dunalia_solanacea          4\n 4 Lycium_arenicola           4\n 5 Lycium_californicum        3\n 6 Lycium_exsertum            4\n 7 Lycium_fremontii           4\n 8 Lycium_gariepense          4\n 9 Lycium_horridum            4\n10 Lycium_strandveldense      4\n# … with 34 more rows\n\n\n\n\n\nWith slice, one can choose rows of the data, just like with filter. Unlike with filter however, slice receives a vector of row indices instead of a condition to be tested on each row. So, for example, if one wanted to keep only the first, second, and fifth rows, then one can do so with slice:\n\nslice(dat, c(1, 2, 5))\n\n# A tibble: 3 × 2\n  Species              Status\n  <chr>                 <dbl>\n1 Acnistus_arborescens      1\n2 Anisodus_tanguticus       1\n3 Brugmansia_aurea          0\n\n\n(Note: the numbers in front of the rows in the output generated by tibbles always pertain to the row numbers of the current table, not the one from which they were created. So the row labels 1, 2, and 3 above simply enumerate the rows of the sliced data. The actual rows still correspond to rows 1, 2, and 5 in the original dat.)\n\n\n\nThis function rearranges the rows of the data, in increasing order of the column given as the second argument. For example, to arrange in increasing order of Status, we write:\n\narrange(dat, Status)\n\n# A tibble: 356 × 2\n   Species                 Status\n   <chr>                    <dbl>\n 1 Brugmansia_aurea             0\n 2 Brugmansia_sanguinea         0\n 3 Cuatresia_exiguiflora        0\n 4 Cuatresia_riparia            0\n 5 Dunalia_brachyacantha        0\n 6 Dyssochroma_viridiflora      0\n 7 Eriolarynx_lorentzii         0\n 8 Grabowskia_duplicata         0\n 9 Iochroma_australe            0\n10 Iochroma_cyaneum             0\n# … with 346 more rows\n\n\nTo arrange in decreasing order, apply the desc function to Status within arrange, like this:\n\narrange(dat, desc(Status))\n\n# A tibble: 356 × 2\n   Species               Status\n   <chr>                  <dbl>\n 1 Solanum_wendlandii         5\n 2 Dunalia_solanacea          4\n 3 Lycium_arenicola           4\n 4 Lycium_exsertum            4\n 5 Lycium_fremontii           4\n 6 Lycium_gariepense          4\n 7 Lycium_horridum            4\n 8 Lycium_strandveldense      4\n 9 Lycium_tetrandrum          4\n10 Lycium_villosum            4\n# … with 346 more rows\n\n\nIt is also perfectly possible to arrange by a column whose type is character string. In that case, the system will automagically sort the rows in alphabetical order—or reverse alphabetical order, in case desc is applied. For example, to sort in reverse alphabetical order of species binomials:\n\narrange(dat, desc(Species))\n\n# A tibble: 356 × 2\n   Species                   Status\n   <chr>                      <dbl>\n 1 Witheringia_solanacea          2\n 2 Witheringia_mexicana           0\n 3 Witheringia_meiantha           0\n 4 Witheringia_macrantha          0\n 5 Witheringia_cuneata            0\n 6 Witheringia_coccoloboides      0\n 7 Withania_somnifera             1\n 8 Withania_coagulans             4\n 9 Vassobia_breviflora            1\n10 Symonanthus_bancroftii         4\n# … with 346 more rows\n\n\nNotice that when we sorted the rows by Status, there are many ties—rows with the same value of Status. In those cases, arrange will not be able to decide which rows should come earlier, and so any ordering that was present before invoking arrange will be retained. In case we would like to break the ties, we can give further sorting variables, as the third, fourth, etc. arguments to arrange. To sort the data by Status, and to resolve ties in alphabetical order of Species, we write:\n\narrange(dat, Status, Species)\n\n# A tibble: 356 × 2\n   Species                 Status\n   <chr>                    <dbl>\n 1 Brugmansia_aurea             0\n 2 Brugmansia_sanguinea         0\n 3 Cuatresia_exiguiflora        0\n 4 Cuatresia_riparia            0\n 5 Dunalia_brachyacantha        0\n 6 Dyssochroma_viridiflora      0\n 7 Eriolarynx_lorentzii         0\n 8 Grabowskia_duplicata         0\n 9 Iochroma_australe            0\n10 Iochroma_cyaneum             0\n# … with 346 more rows\n\n\nThis causes the table to be sorted primarily by Status, but in case there are ties (equal Status between multiple rows), they will be resolved in priority of alphabetical order—first those starting with “A” (if they exist), then “B”, and so on.\n\n\n\nThe mutate function allows us to create new columns from existing ones. We may apply any function or operator we learned about to existing columns, and the result of the computation will go into the new column. We do this in the second argument of mutate (the first, as always, is the data), by first giving a name to the column, then writing =, and then the desired computation. For example, we may find it strange that the selfing status of the species is encoded with a number ranging from 0 to 5, instead of 1 to 6. This is easy to fix however, using mutate:\n\nmutate(dat, NewStatus = Status + 1)\n\n# A tibble: 356 × 3\n   Species                   Status NewStatus\n   <chr>                      <dbl>     <dbl>\n 1 Acnistus_arborescens           1         2\n 2 Anisodus_tanguticus            1         2\n 3 Atropa_belladonna              1         2\n 4 Brachistus_stramonifolius      1         2\n 5 Brugmansia_aurea               0         1\n 6 Brugmansia_sanguinea           0         1\n 7 Capsicum_annuum                1         2\n 8 Capsicum_baccatum              1         2\n 9 Capsicum_cardenasii            2         3\n10 Capsicum_chacoense             1         2\n# … with 346 more rows\n\n\nThe original columns of the data are retained, but we now also have the additional NewStatus column.\nPerhaps more interestingly, we could create a new column indicating whether the selfing mechanism of the species is simple (Status either 0 or 1) or complex (Status between 2 and 5). We can do this using an ifelse function within mutate:\n\nmutate(dat, SelfingMechanism = ifelse(Status < 2, \"simple\", \"complex\"))\n\n# A tibble: 356 × 3\n   Species                   Status SelfingMechanism\n   <chr>                      <dbl> <chr>           \n 1 Acnistus_arborescens           1 simple          \n 2 Anisodus_tanguticus            1 simple          \n 3 Atropa_belladonna              1 simple          \n 4 Brachistus_stramonifolius      1 simple          \n 5 Brugmansia_aurea               0 simple          \n 6 Brugmansia_sanguinea           0 simple          \n 7 Capsicum_annuum                1 simple          \n 8 Capsicum_baccatum              1 simple          \n 9 Capsicum_cardenasii            2 complex         \n10 Capsicum_chacoense             1 simple          \n# … with 346 more rows"
  },
  {
    "objectID": "Basic_data_wrangling.html#using-pipes-to-our-advantage",
    "href": "Basic_data_wrangling.html#using-pipes-to-our-advantage",
    "title": "4  Basic data manipulation",
    "section": "4.2 Using pipes to our advantage",
    "text": "4.2 Using pipes to our advantage\nWhen composing multiple tidyverse functions together, things can get unwieldy quite quickly. Let us take the same data, and create the new column SelfingMechanism as above. What happens if we then filter for only those entries with complex selfing mechanism, and finally, we select the column Species only? Here is the solution:\n\nselect(\n  filter(\n    mutate(dat, SelfingMechanism = ifelse(Status < 2, \"simple\", \"complex\")),\n    SelfingMechanism == \"complex\"\n  ),\n  Species\n)\n\n# A tibble: 44 × 1\n   Species              \n   <chr>                \n 1 Capsicum_cardenasii  \n 2 Capsicum_pubescens   \n 3 Dunalia_solanacea    \n 4 Lycium_arenicola     \n 5 Lycium_californicum  \n 6 Lycium_exsertum      \n 7 Lycium_fremontii     \n 8 Lycium_gariepense    \n 9 Lycium_horridum      \n10 Lycium_strandveldense\n# … with 34 more rows\n\n\nThe expression is highly unpleasant: to a human reader, it is not at all obvious what is happening above. To clarify, we have two options. One is to rely on repeated assignment:\n\nmutatedDat <- mutate(dat, SelfingMechanism = ifelse(Status < 2, \"simple\", \"complex\"))\nfilteredDat <- filter(mutatedDat, SelfingMechanism == \"complex\")\nonlySpeciesDat <- select(filteredDat, Species)\nprint(onlySpeciesDat)\n\n# A tibble: 44 × 1\n   Species              \n   <chr>                \n 1 Capsicum_cardenasii  \n 2 Capsicum_pubescens   \n 3 Dunalia_solanacea    \n 4 Lycium_arenicola     \n 5 Lycium_californicum  \n 6 Lycium_exsertum      \n 7 Lycium_fremontii     \n 8 Lycium_gariepense    \n 9 Lycium_horridum      \n10 Lycium_strandveldense\n# … with 34 more rows\n\n\nThis, however, requires inventing arbitrary variable names at every step, or else overwriting variables. For such a short example, this is not problematic, but doing the same for a long pipeline of dozens of steps could get confusing, as well as dangerous due to the repeatedly modified variables.\nIt turns out that one can get the best of both worlds: the safety of function composition with the conceptual clarity of repeated assignments. This only requires that we make use of the pipe operator %>% that we learned about earlier. As a reminder, for any function f and function argument x, f(x, y, ...) is the same as x %>% f(y, ...), where the ... denote potential further arguments to f. That is, the first argument of the function can be moved from the argument list to in front of the function, before the pipe symbol. The tidyverse functions take the data as their first argument, which means that the use of pipes allow us to very conveniently chain together multiple steps of data analysis. In our case, we can rewrite the original\n\nselect(\n  filter(\n    mutate(dat, SelfingMechanism = ifelse(Status < 2, \"simple\", \"complex\")),\n    SelfingMechanism == \"complex\"\n  ),\n  Species\n)\n\nwith the use of pipes, in a much more transparent way:\n\ndat %>%\n  mutate(SelfingMechanism = ifelse(Status < 2, \"simple\", \"complex\")) %>%\n  filter(SelfingMechanism == \"complex\") %>%\n  select(Species)\n\nAgain, the pipe %>% should be pronounced then. We take the data, then we mutate it, then we filter for complex selfing, and then we select one of the columns. In performing these steps, each function both receives and returns data. Thus, by starting out with the original dat, we no longer need to write out the data argument of the functions explicitly. Instead, the pipe takes care of that automatically for us, making the functions receive as their first input argument the piped-in data, and in turn producing transformed data as their output—which becomes the input for the next function in line.\nIn fact, there is technically no need to even assign dat. The pipe can just as well start with the read_csv call to import the dataset:\n\nread_csv(\"Goldberg2010_data.csv\") %>%\n  mutate(SelfingMechanism = ifelse(Status < 2, \"simple\", \"complex\")) %>%\n  filter(SelfingMechanism == \"complex\") %>%\n  select(Species)\n\n# A tibble: 44 × 1\n   Species              \n   <chr>                \n 1 Capsicum_cardenasii  \n 2 Capsicum_pubescens   \n 3 Dunalia_solanacea    \n 4 Lycium_arenicola     \n 5 Lycium_californicum  \n 6 Lycium_exsertum      \n 7 Lycium_fremontii     \n 8 Lycium_gariepense    \n 9 Lycium_horridum      \n10 Lycium_strandveldense\n# … with 34 more rows"
  },
  {
    "objectID": "Basic_data_wrangling.html#exercises",
    "href": "Basic_data_wrangling.html#exercises",
    "title": "4  Basic data manipulation",
    "section": "4.3 Exercises",
    "text": "4.3 Exercises\n\nThe Smith2003_data.txt dataset we worked with last time occasionally has the entry -999 in its last three columns. This stands for unavailable data. In R, there is a built-in way of referring to such information: by setting a variable to NA. (So, for example, x <- NA will set the variable x to NA.) Modify these columns (using mutate) so that the entries which are equal to -999 are replaced with NA.\nRemove all rows from the data which contain one or more NA values (hint: look up the function drop_na). How many rows are retained? And what was the original number of rows?\n\nThe iris dataset is a built-in table in R. It contains measurements of petal and sepal characteristics from three flower species belonging to the genus Iris (I. setosa, I. versicolor, and I. virginica). If you type iris in the console, you will see the dataset displayed. In solving the problems below, feel free to use the all-important dplyr cheat sheet.\n\nThe format of the data is not a tibble, but a data.frame. As mentioned in the previous chapter, the two are basically the same for practical purposes, though internally, tibbles do offer some advantages. Convert the iris data frame into a tibble. (Hint: look up the as_tibble function.)\nSelect the columns containing petal and sepal length, and species identity.\nGet those rows of the data with petal length less than 4 cm, but sepal length greater than 4 cm.\nSort the data by increasing petal length but decreasing sepal length.\nCreate a new column called MeanLength. It should contain the average of the petal and sepal length (i.e., petal length plus sepal length, divided by 2) of each individual flower.\nPerform the operations from the previous two exercises in a single long function call (using function composition)."
  },
  {
    "objectID": "Summaries_normalization.html",
    "href": "Summaries_normalization.html",
    "title": "5  Summary statistics and data normalization",
    "section": "",
    "text": "Last time we learned the basics of reading writing, and manipulating data. To review, download pop_data.csv from Lisam and set your working directory to the folder where you saved it. This is a comma-separated file (CSV), so you can load it using read_csv. As usual, we first load the tidyverse package:\nAnd now we may use the tidyverse functionalities, such as read_csv:\nThe data we just loaded contain population densities of three species at two spatial patches (A and B) at various points in time, ranging from 1 to 50 in steps of 1:\nOne can now perform various manipulations on these data, by using the functions rename, select, filter, arrange, and mutate we have learned about in Chapter 4. For instance, we could create a new column called total which contains the total community density (sum of the three species’ population densities) at each point in time and each location:\nAs a reminder, this can be written equivalently, using pipes:"
  },
  {
    "objectID": "Summaries_normalization.html#creating-summary-data",
    "href": "Summaries_normalization.html#creating-summary-data",
    "title": "5  Summary statistics and data normalization",
    "section": "5.1 Creating summary data",
    "text": "5.1 Creating summary data\nOne can create summaries of data using the summarise function. This will simply apply some function to a column. For example, to calculate the average population density of species 1 in pop, across both time and patches, one can write\n\npop %>% summarise(meanDensity1 = mean(species1))\n\n# A tibble: 1 × 1\n  meanDensity1\n         <dbl>\n1         5.30\n\n\nHere meanDensity1 is the name of the new column to be created, and the mean function is our summary function, collapsing the data into a single number.\nSo far, this is not particularly interesting; in fact, the exact same effect would have been achieved by typing the shorter mean(pop$species1) instead. The real power of summarise comes through when combined with group_by. This groups the data based on the given grouping variables. Let us see how this works in practice:\n\npop %>% group_by(patch)\n\n# A tibble: 100 × 5\n# Groups:   patch [2]\n    time patch species1 species2 species3\n   <dbl> <chr>    <dbl>    <dbl>    <dbl>\n 1     1 A         8.43     6.62    10.1 \n 2     1 B        10.1      3.28     6.27\n 3     2 A         7.76     6.93    10.3 \n 4     2 B        10.1      3.04     6.07\n 5     3 A         7.09     7.24    10.5 \n 6     3 B        10.1      2.8      5.82\n 7     4 A         6.49     7.54    10.6 \n 8     4 B        10.1      2.56     5.57\n 9     5 A         5.99     7.83    10.7 \n10     5 B        10.1      2.33     5.32\n# … with 90 more rows\n\n\nSeemingly nothing has happened; the only difference is the extra line of comment above, before the printed table, saying Groups: patch [2]. What this means is that the rows of the data were internally split into two groups. The first have \"A\" as their patch, and the second have \"B\". Whenever one groups data using group_by, rows which share the same unique combination of the grouping variables now belong together, and subsequent operations will act separately on each group instead of acting on the table as a whole (which is what we have been doing so far). That is, group_by does not actually alter the data; it only alters the behaviour of the functions applied to the grouped data.\nIf we group not just by patch but also by time, the comment above the table will read Groups: patch, time [100]:\n\npop %>% group_by(patch, time)\n\n# A tibble: 100 × 5\n# Groups:   patch, time [100]\n    time patch species1 species2 species3\n   <dbl> <chr>    <dbl>    <dbl>    <dbl>\n 1     1 A         8.43     6.62    10.1 \n 2     1 B        10.1      3.28     6.27\n 3     2 A         7.76     6.93    10.3 \n 4     2 B        10.1      3.04     6.07\n 5     3 A         7.09     7.24    10.5 \n 6     3 B        10.1      2.8      5.82\n 7     4 A         6.49     7.54    10.6 \n 8     4 B        10.1      2.56     5.57\n 9     5 A         5.99     7.83    10.7 \n10     5 B        10.1      2.33     5.32\n# … with 90 more rows\n\n\nThis is because there are 100 unique combinations of patch and time: two different patch values (\"A\" and \"B\"), and fifty points in time (1, 2, …, 50). So we have “patch A, time 1” as group 1, “patch B, time 1” as group 2, “patch A, time 3” as group 3, and so on until “patch B, time 50” as our group 100.\nAs mentioned, functions that are applied to grouped data will act on the groups separately. To return to the example of calculating the mean population density of species 1 in the two patches, we can write:\n\npop %>%\n  group_by(patch) %>%\n  summarise(meanDensity1 = mean(species1))\n\n# A tibble: 2 × 2\n  patch meanDensity1\n  <chr>        <dbl>\n1 A             5.29\n2 B             5.32\n\n\nOne may obtain multiple summary statistics within the same summarize function. Below we compute both the mean and the standard deviation of the densities per patch:\n\npop %>%\n  group_by(patch) %>%\n  summarise(meanDensity1 = mean(species1), sdDensity1 = sd(species1))\n\n# A tibble: 2 × 3\n  patch meanDensity1 sdDensity1\n  <chr>        <dbl>      <dbl>\n1 A             5.29      0.833\n2 B             5.32      3.81 \n\n\nLet us see what happens if we calculate the mean density of species 1—but grouping by time instead of patch:\n\npop %>%\n  group_by(time) %>%\n  summarise(meanDensity1 = mean(species1))\n\n# A tibble: 50 × 2\n    time meanDensity1\n   <dbl>        <dbl>\n 1     1         9.28\n 2     2         8.94\n 3     3         8.60\n 4     4         8.3 \n 5     5         8.04\n 6     6         7.84\n 7     7         7.68\n 8     8         7.54\n 9     9         7.44\n10    10         7.35\n# … with 40 more rows\n\n\nThe resulting table has 50 rows—half the number of rows in the original data, but many more than the two rows we get after grouping by patch. The reason is that there are 50 unique time points, and so the average is now computed over those rows which share time. But there are only two rows per moment of time: the rows corresponding to patch A and patch B. When we call summarise after having grouped by time, the averages are computed over the densities in these two rows only, per group. That is why here we end up with a table which has a single row per point in time.\n\n\n\n\n\n\nWarning\n\n\n\nAn easy mistake to make when one first meets with grouping and summaries is to assume that if we call group_by(patch), then the subsequent summaries will be taken over patches. This is not the case, and be sure to take a moment to understand why. When we apply group_by(patch), we are telling R to treat different patch values as group indicators. Therefore, when creating a summary, only the patch identities are retained from the original data (apart from the new summary statistics we calculate, of course). This means that the subsequent summaries are taken over everything except the patches. This should be clear after comparing the outputs of\n\npop %>% group_by(patch) %>% summarise(meanDensity1 = mean(species1))\n\nand\n\npop %>% group_by(time) %>% summarise(meanDensity1 = mean(species1))\n\nThe first distinguishes the rows of the data only by patch, and therefore the average is taken over time. The second distinguishes the rows by time, so the average is taken over the patches. Run the two expressions again to see the difference between them!\n\n\nWe can use functions such as mutate or filter on grouped data. For example, we might want to know the deviation of species 1’s density from its average in each patch. Doing the following does not quite do what we want:\n\npop %>% mutate(species1Dev = species1 - mean(species1))\n\n# A tibble: 100 × 6\n    time patch species1 species2 species3 species1Dev\n   <dbl> <chr>    <dbl>    <dbl>    <dbl>       <dbl>\n 1     1 A         8.43     6.62    10.1        3.13 \n 2     1 B        10.1      3.28     6.27       4.83 \n 3     2 A         7.76     6.93    10.3        2.46 \n 4     2 B        10.1      3.04     6.07       4.82 \n 5     3 A         7.09     7.24    10.5        1.79 \n 6     3 B        10.1      2.8      5.82       4.82 \n 7     4 A         6.49     7.54    10.6        1.19 \n 8     4 B        10.1      2.56     5.57       4.81 \n 9     5 A         5.99     7.83    10.7        0.685\n10     5 B        10.1      2.33     5.32       4.80 \n# … with 90 more rows\n\n\nThis will put the difference of species 1’s density from its mean density across both time and patches into the new column species1Dev. Which is not the same as calculating the difference from the mean in a given patch—patch A for rows corresponding to patch A, and patch B for the others. To achieve this, all one needs to do is to group the data by patch before invoking mutate:\n\npop %>%\n  group_by(patch) %>%\n  mutate(species1Dev = species1 - mean(species1))\n\n# A tibble: 100 × 6\n# Groups:   patch [2]\n    time patch species1 species2 species3 species1Dev\n   <dbl> <chr>    <dbl>    <dbl>    <dbl>       <dbl>\n 1     1 A         8.43     6.62    10.1        3.14 \n 2     1 B        10.1      3.28     6.27       4.81 \n 3     2 A         7.76     6.93    10.3        2.47 \n 4     2 B        10.1      3.04     6.07       4.80 \n 5     3 A         7.09     7.24    10.5        1.80 \n 6     3 B        10.1      2.8      5.82       4.80 \n 7     4 A         6.49     7.54    10.6        1.20 \n 8     4 B        10.1      2.56     5.57       4.79 \n 9     5 A         5.99     7.83    10.7        0.702\n10     5 B        10.1      2.33     5.32       4.78 \n# … with 90 more rows\n\n\nComparing this with the previous table, we see that the values in the species1Dev column are now different, because this time the differences are taken with respect to the average densities per each patch.\nFinally, since group_by changes subsequent behaviour, we might eventually want to get rid of the grouping in our data. To do so, one must use ungroup. For example:\n\npop %>%\n  group_by(patch) %>%\n  summarise(meanDensity1 = mean(species1), sdDensity1 = sd(species1)) %>%\n  ungroup()\n\n# A tibble: 2 × 3\n  patch meanDensity1 sdDensity1\n  <chr>        <dbl>      <dbl>\n1 A             5.29      0.833\n2 B             5.32      3.81 \n\n\nIt is good practice to always ungroup the data after we have calculated what we wanted using the group structure."
  },
  {
    "objectID": "Summaries_normalization.html#data-normalization",
    "href": "Summaries_normalization.html#data-normalization",
    "title": "5  Summary statistics and data normalization",
    "section": "5.2 Data normalization",
    "text": "5.2 Data normalization\nIn science, we often strive to work with so-called normalized data. A dataset is normalized if:\n\nEach variable is in its own column;\nEach observation is in its own row.\n\nNormalized data are suitable for performing operations, statistics, and plotting on. Furthermore, normalized data have a certain tidy feel to them, in the sense that their organization always follows the same general pattern regardless of the type of dataset one studies. (By contrast, every non-normalized dataset tends to be messy in its own unique way.) The tidyverse offers a simple and convenient way to normalize data.\nFor example, the pop table from the previous section is not normalized. This is because although each variable is in its own column, it is not true that each observation is in its own row. In fact, each row contains three observations: the densities of species 1, 2, and 3 at a given time and place. To normalize these data, we create key-value pairs. We merge the columns for species densities into just two new ones. The first of these (the key) indicates whether it is species 1, or 2, or 3 which the given row refers to. The second column (the value) contains the population density of the given species. Such key-value pairs are created by the function pivot_longer:\n\npop %>% pivot_longer(cols = 3:5, names_to = \"species\", values_to = \"density\")\n\n# A tibble: 300 × 4\n    time patch species  density\n   <dbl> <chr> <chr>      <dbl>\n 1     1 A     species1    8.43\n 2     1 A     species2    6.62\n 3     1 A     species3   10.1 \n 4     1 B     species1   10.1 \n 5     1 B     species2    3.28\n 6     1 B     species3    6.27\n 7     2 A     species1    7.76\n 8     2 A     species2    6.93\n 9     2 A     species3   10.3 \n10     2 B     species1   10.1 \n# … with 290 more rows\n\n\nThe function pivot_longer takes three arguments (apart, of course, from the first data argument that we may also pipe in, like above). First, cols is the list of columns to be converted into key-value pairs. One can refer to the columns by number: 3:5 is the same as c(3, 4, 5) and selects the third, fourth, and fifth columns—the ones corresponding to the population densities. We could also have written c(\"species1\", \"species2\", \"species3\") instead, choosing columns by their names. This can give greater clarity, albeit at the cost of more typing. Second, the argument names_to is the name of the new key column. Finally, values_to is the name of the new value column.\nNotice that the above table is now normalized: each column records a single variable, and each row contains a single observation. Notice also that, unlike the original pop which had 100 rows and 5 columns, the normalized version has 300 rows and 4 columns. This is natural: since the number of rows was reduced, there must be some extra rows to prevent the loss of information.\nIt is possible to “undo” the effect pivot_longer. To do so, use pivot_wider:\n\npop %>%\n  pivot_longer(cols = 3:5, names_to = \"species\", values_to = \"density\") %>%\n  pivot_wider(names_from = \"species\", values_from = \"density\")\n\n# A tibble: 100 × 5\n    time patch species1 species2 species3\n   <dbl> <chr>    <dbl>    <dbl>    <dbl>\n 1     1 A         8.43     6.62    10.1 \n 2     1 B        10.1      3.28     6.27\n 3     2 A         7.76     6.93    10.3 \n 4     2 B        10.1      3.04     6.07\n 5     3 A         7.09     7.24    10.5 \n 6     3 B        10.1      2.8      5.82\n 7     4 A         6.49     7.54    10.6 \n 8     4 B        10.1      2.56     5.57\n 9     5 A         5.99     7.83    10.7 \n10     5 B        10.1      2.33     5.32\n# … with 90 more rows\n\n\nThe two named arguments of pivot_wider above are names_from (which specifies the column from which the names for the new columns will be taken), and values_from (the column whose values will be used to fill in the rows under those new columns).\nAs a remark, one could make the data even “wider”, by not only making columns out of the population densities, but the densities at a given patch. Doing so is very simple: one just needs to specify both the species and patch columns from which the new column names will be compiled:\n\npop %>%\n  pivot_longer(cols = 3:5, names_to = \"species\", values_to = \"density\") %>%\n  pivot_wider(names_from = c(\"species\", \"patch\"), values_from = \"density\")\n\n# A tibble: 50 × 7\n    time species1_A species2_A species3_A species1_B species2_B species3_B\n   <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>\n 1     1       8.43       6.62      10.1        10.1       3.28       6.27\n 2     2       7.76       6.93      10.3        10.1       3.04       6.07\n 3     3       7.09       7.24      10.5        10.1       2.8        5.82\n 4     4       6.49       7.54      10.6        10.1       2.56       5.57\n 5     5       5.99       7.83      10.7        10.1       2.33       5.32\n 6     6       5.58       8.1       10.7        10.1       2.12       5.08\n 7     7       5.27       8.34      10.6        10.1       1.92       4.86\n 8     8       5.02       8.54      10.4        10.1       1.74       4.64\n 9     9       4.82       8.7       10.0        10.0       1.58       4.43\n10    10       4.66       8.82       9.66       10.0       1.43       4.23\n# … with 40 more rows\n\n\nIf normalized data are what we strive for, what is the practical use of pivot_wider? There are two answers to this question. First, while non-normalized data are indeed less efficient from a computational and data analysis standpoint, they are often more human-readable. For example, the pop table is easy to read despite the lack of normalization, because each row corresponds to a given time and place. By normalizing the data, information referring to any given time and place will be spread out over multiple (in our case, three) rows—one for each species. While this is preferable from a data analysis point of view, it can be more difficult to digest visually. Second, wide data lend themselves very well to one particular class of statistical techniques called multivariate analysis. In case one wants to perform multivariate analysis, wide-format data are often better than normalized data.\nFinally, it is worth noting the power of normalized data in, e.g., generating summary statistics. To obtain the mean and the standard deviation of the population densities for each species in each patch, all one has to do is this:\n\npop %>%\n  pivot_longer(3:5, names_to = \"species\", values_to = \"density\") %>% # Normalize data\n  group_by(patch, species) %>% # Group data by both species and patch\n  summarise(meanDensity = mean(density), sdDensity = sd(density)) %>% # Obtain statistics\n  ungroup() # Don't forget to ungroup the data at the end\n\n# A tibble: 6 × 4\n  patch species  meanDensity sdDensity\n  <chr> <chr>          <dbl>     <dbl>\n1 A     species1        5.29     0.833\n2 A     species2        8.05     0.559\n3 A     species3        7.51     1.56 \n4 B     species1        5.32     3.81 \n5 B     species2        1.07     0.737\n6 B     species3        6.57     2.48"
  },
  {
    "objectID": "Summaries_normalization.html#exercises",
    "href": "Summaries_normalization.html#exercises",
    "title": "5  Summary statistics and data normalization",
    "section": "5.3 Exercises",
    "text": "5.3 Exercises\nThe exercises below use the iris dataset—the same that we used for last chapter’s data wrangling exercises. Convert the iris data to a tibble with the as_tibble() function, and assign it to a variable.\n\nCreate a new column in the iris dataset which contains the deviation of petal lengths from the average of the whole dataset.\nCreate a new column in the iris dataset which contains the deviation of petal lengths from the average of each species. (Hint: group_by the species and then mutate!)\nCreate a table where the rows are the three species, and the columns are: average petal length, variance of petal length, average sepal length, and variance of sepal length.\nCreate key-value pairs in the iris dataset for the petal characteristics. In other words, have a column called Petal.Trait (whose values are either Petal.Length or Petal.Width), and another column called Petal.Value (with the length/width values).\nRepeat the same exercise, but now for sepal traits.\nFinally, do it for both petal and sepal traits simultaneously, to obtain a fully normalized form of the iris data. That is, the key column (call it Flower.Trait) will have the values Petal.Length, Petal.Width, Sepal.Length, and Sepal.Width. And the value column (which you can call Trait.Value) will have the corresponding measurements."
  },
  {
    "objectID": "Creating_figures.html",
    "href": "Creating_figures.html",
    "title": "6  Creating publication-grade figures",
    "section": "",
    "text": "In science, we want good, clear plots. Each figure should make it obvious what data you are plotting, what the axes, colors, shapes, and size differences represent, and the overall message the figure is conveying. When writing a scientific paper, or a report for an environmental consulting company, etc., remember that your future readers are busy people. They often do not have the time to delve into the subtleties of overly refined verbal arguments. Instead, they will most often glance at the figures to learn what your work is about. You will want to create figures which makes this possible for them to do.\nHere we will learn how to create accessible, publication-quality scientific graphs in a simple way. We do this using the R package ggplot2 which is a standard part of the tidyverse. The ggplot2 package follows a very special philosophy for creating figures that was originally proposed by Leland Wilkinson (2006). The essence of this view is that, just like the grammar of sentences, graphs have fixed “grammatical” components whose specification defines the plot. The grand idea is that the data ought not be changed in order to display it in different formats. For instance, the same data should be possible to represent either as a box plot, or as a histogram, without changing the data format.\nThis last claim needs to be qualified somewhat. It is more accurate to say that one should not need to change the data format as long as the data are normalized. As a reminder, “normalized data” means that every variable is in its own column, and every observation in its own row. In case the data are not normalized, one should first wrangle them into such form, for example by using pivot_wider(). For certain kinds of graphs though, this step is not required. But when working with larger datasets, it can be very useful to normalize the data before analyzing and plotting them—we will see examples of this in the exercises.\nTo see how ggplot2 works, let us load tidyverse, and then use the built-in iris dataset to create some figures. As a reminder, here is what the data look like:\nLet us, as a first step, create a plot where sepal length (x-axis) is plotted against petal length (y-axis), with the points referring to different species shown in different colors:\nHere we defined the data (iris; feel free to use pipes, as in iris %>% ggplot() + ...), then the aesthetic mappings with aes(), and finally the geometry of how to display the data with geom_point(). The important thing to remember is that the aesthetic mappings are all those aspects of the figure that are governed by your data. For instance, if you wanted to set the color of all points to blue, this would not be an aesthetic mapping, because it applies regardless of what the data are (in case you want to do this, you would have to specify geom_point(colour = \"blue\") in the last line). The geometry of your plot, on the other hand, governs the overall visual arrangement of your data (points, lines, histograms, etc). There are many different geom_s; we will learn about some here, but when in doubt, Google and a ggplot2 cheat sheet are your best friends.\nNotice that the different “grammatical” components of the plot (aesthetics, geometry) are added to the plot, using the + symbol for addition. This is due to somewhat of an unfortunate historical accident. As it happens, ggplot2 is older than the rest of the tidyverse, and so when it was first designed, the pipe operator %>% did not yet exist. This means that within the creating of a ggplot graph, one must use + to compose the various graph elements; but outside that, the usual %>% is used for function composition.\nTo look at a different kind of geometry, let us create a histogram of the petal lengths. This is done using geom_histogram:\nWe see two clusters of data. Why is that? One might suspect that this is because of a species-level difference. To check if that is the case, let us color the histogram by species:\n(A color legend was created automatically on the right.) This changes the color of the outline of the histograms, but not the color of their fill. To do so, we need to change the fill color as well, which is a separate aesthetic property:\nThis is fine, but now the histogram bars of different species are stacked on top of one another. This means that, for example at around a petal length of 5 cm where individuals of both Iris versicolor and I. virginica are found, the green bar on top of the blue one does not begin from the bottom of the y-axis at 0, but from wherever the blue bar ends and the green one begins (in our case, from around 10 upwards). So the number of I. versicolor individuals in the 5 cm bin is not 12, but only 2.\nIt may be easier to interpret the histogram if all bars start at the bottom, even if this means that the bars will now overlap to an extent. To achieve this overlap, one simply has to add the argument position = \"identity\" to geom_histogram:\n(There are other options as well, such as position = \"dodge\" – feel free to experiment. In general, you can learn what options each of the geom_s have by invoking the Help, or asking Google.) The figure above is almost perfect; the one remaining problem is that, due to the lack of transparency, the bars belonging to different species cover each other completely. So let us change the fill’s transparency, which is called alpha in ggplot. The alpha property can take on any value between 0 (fully transparent object, to the point of invisibility) to 1 (fully solid object with no transparency, like above; this is the default). Setting it to 0.2 (say) will finally fully reveal the distribution of each species:\nThere are plenty of other geom_s as well, such as geom_boxplot, geom_violin, etc. Let us try geom_boxplot for instance. Let us create one box plot of the distribution of petal lengths for each species. Putting the species along the x-axis and petal length along the y-axis:\nAlthough this is sufficient, one should feel free to make the plots prettier. For instance, one could use colors, like before. One can also use different themes. There are pre-defined themes such as theme_classic(), theme_bw(), and so on. For example, using theme_bw() gets rid of the default gray background and replaces it with a white one. The version of the graph below applies this theme, and also changes the color and fill to be based on species identity:"
  },
  {
    "objectID": "Creating_figures.html#sec-CI",
    "href": "Creating_figures.html#sec-CI",
    "title": "6  Creating publication-grade figures",
    "section": "6.1 Summaries and confidence intervals",
    "text": "6.1 Summaries and confidence intervals\nProviding appropriate information on experimental errors is a hallmark of any credible scientific graph. Choose a type of error based on the conclusion that you want the reader to draw. While the standard deviation (SD) represents the dispersion of the data, the standard error of the mean (SEM) and confidence intervals (CI) report the certainty of the estimate of a value (e.g., certainty in estimating the mean). Let us see examples of each.\nLet us first obtain the mean and the standard deviation of the petal lengths for each species. We then would like to plot them. How to proceed? By creating a workflow which first uses group_by and summarise to obtain the required statistics (means and standard deviations), and then uses these data for plotting. One can include all these steps in a logical workflow:\n\niris %>%\n  group_by(Species) %>% # Perform summary calculations for each species\n  summarise(mP = mean(Petal.Length), # Mean petal length\n            sP = sd(Petal.Length)) %>% # Standard deviation of petal length\n  ungroup() %>% # Ungroup data\n  ggplot() + # Start plotting\n  aes(x = Species, y = mP, ymin = mP - sP, ymax = mP + sP) + # Mean +/- sd\n  geom_col() + # This takes the y aesthetic, for plotting the mean\n  geom_errorbar(width = 0.2) # Takes the ymin and ymax aesthetics\n\n\n\n\nNote that the y-axis label is not very descriptive. This is because it inherited the name of the corresponding data column, mP. There are multiple ways to change it; the simplest is to add ylab(\"Petal length\") to the plot. Another way of doing so is duscussed in Chapter 7.\nIn case we want to calculate the 95% confidence intervals of the mean values, we first obtain some necessary summary statistics: the number of observations (sample size) in each group; the standard error of the mean (standard deviation divided by the square root of the sample size); and finally, the confidence interval itself (read off from the t-distribution, with one fewer degrees of freedom than the sample size). We can then include these confidence intervals on top of the mean values:\n\niris %>%\n  group_by(Species) %>%\n  summarise(mP = mean(Petal.Length), # Mean petal length per species\n            sP = sd(Petal.Length), # Standard deviation per species\n            N = n(), # Sample size (number of observations) per species\n            SEM = sP / sqrt(N), # Standard error of the mean\n            CI = SEM * qt(0.975, N - 1)) %>% # Confidence interval, read off\n                 # at 0.975 because plus/minus 2.5% adds up to 5%\n  ggplot() +\n  aes(x = Species, y = mP, ymin = mP - CI, ymax = mP + CI) +\n  geom_col(alpha = 0.4, colour = \"steelblue\", fill = \"steelblue\") +\n  geom_errorbar(width = 0.2, colour = \"steelblue\") +\n  ylab(\"Petal length\") +\n  theme_bw()\n\n\n\n\nOne final note. The above type of bar-and-whisker plot is common in the literature, and therefore one should be aware of how to make and read them. That said, there are several reasons why displaying summary data like this is not a good idea. By starting the bars from zero, the plot implies that zero is a natural point of comparison for all the data. Unfortunately, this can also visually distort the information we wish to convey. Consider the following graph:\n\ntibble(Species = c(\"species 1\", \"species 2\"),\n       Average = c(148, 152),\n       CI = c(0.8, 0.9)) %>%\n  ggplot() +\n  aes(x = Species, y = Average, ymin = Average - CI, ymax = Average + CI) +\n  geom_col(alpha = 0.4, colour = \"steelblue\", fill = \"steelblue\") +\n  geom_errorbar(width = 0.2, colour = \"steelblue\") +\n  ylab(\"Trait value\") +\n  theme_bw()\n\n\n\n\nIt is impossible to see whether there are any relevant differences between the two species. The following is exactly the same, but with the mean values shown with points instead of bars:\n\ntibble(Species = c(\"species 1\", \"species 2\"),\n       Average = c(148, 152),\n       CI = c(0.8, 0.9)) %>%\n  ggplot() +\n  aes(x = Species, y = Average, ymin = Average - CI, ymax = Average + CI) +\n  geom_point(colour = \"steelblue\") +\n  geom_errorbar(width = 0.2, colour = \"steelblue\") +\n  ylab(\"Trait value\") +\n  theme_bw()\n\n\n\n\nIt is now quite obvious that the two observations are distinct. Remaking our earlier bar-and-whisker graph of the iris dataset in this spirit, we simply replace geom_col with geom_point:\n\niris %>%\n  group_by(Species) %>%\n  summarise(mP = mean(Petal.Length), # Mean petal length per species\n            sP = sd(Petal.Length), # Standard deviation per species\n            N = n(), # Sample size (number of observations) per species\n            SEM = sP / sqrt(N), # Standard error of the mean\n            CI = SEM * qt(0.975, N - 1)) %>% # Confidence interval, read off\n                 # at 0.975 because plus/minus 2.5% adds up to 5%\n  ggplot() +\n  aes(x = Species, y = mP, ymin = mP - CI, ymax = mP + CI) +\n  geom_point(colour = \"steelblue\") +\n  geom_errorbar(width = 0.2, colour = \"steelblue\") +\n  ylab(\"Petal length\") +\n  theme_bw()"
  },
  {
    "objectID": "Creating_figures.html#exercises",
    "href": "Creating_figures.html#exercises",
    "title": "6  Creating publication-grade figures",
    "section": "6.2 Exercises",
    "text": "6.2 Exercises\nFauchald et al. (2017) tracked the population size of various herds of caribou in North America over time, and correlated population cycling with the amount of vegetation and sea-ice cover. The part of their data that we will use consists of two files (found on Lisam): pop_size.tsv (data on herd population sizes), and sea_ice.tsv (on sea levels of sea ice cover per year and month).\n\nThe file sea_ice.tsv is in human-readable, wide format. Note however that the rule “each set of observations is stored in its own row” is violated. For computation, we would like to organize the data in a tidy tibble with four columns: Herd, Year, Month, and Cover. To this end, apply the function pivot_longer to columns 3-14 in the tibble, gathering the names of the months in the new column Month and the values in the new column Cover.\nUse pop_size.tsv to make a plot of herd sizes through time. Let the x-axis be Year, the y-axis be population size. Show different herds in different colors. For geometry, use points.\nThe previous plot is actually not that easy to see and interpret. To make it better, add a line geometry as well, which will connect the points with lines.\nMake a histogram out of all population sizes in the data.\nMake the same histogram, but break it down by herd, using a different color and fill for each herd.\nInstead of a histogram, make a density plot with the same data and display (look up geom_density if needed).\nMake box plots of the population size of each herd. Along the x-axis, each herd should be separately displayed; the y-axis should be population size. The box plots should summarize population sizes across all years.\nLet us go back to sea_ice.tsv. Make the following plot. Along the x-axis, have Year. Along the y-axis, Month. Then, for each month-year pair, color the given part of the plot darker for lower ice cover and lighter for more. (Hint: look up geom_tile if needed.) Finally, make sure to do all this only for the herd with the i.d. WAH (filter the data before plotting).\n\n\n\n\n\nFauchald, Per, Taejin Park, Hans Tømmervik, Ranga Myneni, and Vera Helene Hausner. 2017. “Arctic greening from warming promotes declines in caribou populations.” Science Advances 3 (4): e1601365. https://doi.org/10.1126/sciadv.1601365.\n\n\nWilkinson, Leland. 2006. The Grammar of Graphics. Secaucus, NJ, USA: Springer Science & Business Media."
  },
  {
    "objectID": "Further_plotting_options.html",
    "href": "Further_plotting_options.html",
    "title": "7  Some further plotting options",
    "section": "",
    "text": "Last time we learned about aesthetic mappings and various geom_ options, such as geom_point, geom_histogram, and geom_boxplot. Let us explore another type of geom_, which approximates the trend of a set of data points with a line and an error bar that shows the confidence interval of the estimate at each point:\n\nlibrary(tidyverse)\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\nWhile such fits are occasionally useful, we often want a linear least-suqares regression on our data. To get such a linear fit, add the argument method = \"lm\" to geom_smooth() (\"lm\" stands for “linear model”):\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\nLinear regression lines are usually shown without the confidence intervals (the gray band around the regression line). To drop this, set se = FALSE:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nWhat happens if we color the data points by species? Let us add colour = Species to the list of aesthetic mappings:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length, colour = Species) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nNow the regression line is automatically fitted to the data within each of the groups separately—a highly useful behavior.\nNotice also that a color legend was automatically created and put to the right of the graph. This is the default in ggplot2. You can move them to another position by specifying the legend.position option within the theme function that can be added onto the plot:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length, colour = Species) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme(legend.position = \"left\") # \"top\", \"bottom\", \"left\", \"right\", or \"none\"\n\n\n\n\nSpecifying legend.position = \"none\" omits the legend altogether.\nA word of caution: in case the legend positioning is matched with a generic theme such as theme_bw(), one should put the legend position after the main theme definition. The reason is that pre-defined themes like theme_bw() override any specific theme options you might specify. The rule of thumb is: any theme() component to your plot should be added only after the generic theme definition. Otherwise the theme() component will be overridden and will not take effect. For example, this does not work as intended:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length, colour = Species) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme(legend.position = \"left\") + # Position legend at the left\n  theme_bw() # This defines the general theme - and thus overrides the line above...\n\n\n\n\nBut this one does:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length, colour = Species) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw() + # This defines the general theme\n  theme(legend.position = \"left\") # We now override the default legend positioning"
  },
  {
    "objectID": "Further_plotting_options.html#scales",
    "href": "Further_plotting_options.html#scales",
    "title": "7  Some further plotting options",
    "section": "7.2 Scales",
    "text": "7.2 Scales\nThe aesthetic mappings of a graph (x-axis, y-axis, color, fill, size, shape, alpha, …) are automatically rendered into the displayed plot, based on certain default settings within ggplot2. These defaults can be altered, however. Consider the following bare-bones plot:\n\niris %>%\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot()\n\n\n\n\nWe can now change, for example, how the y-axis is displayed. The component to be added to the plot is scale_y_continuous(). Here scale means we are going to change the scaling of some aesthetic mapping, y refers to the y-axis (as expected, it can be replaced with x, colour, fill, etc.), and continuous means that the scaling of the axis is not via discrete values (e.g., either 1 or 2 or 3 but nothing in between), but continuous (every real number is permissible along the y-axis). The plot component scale_y_continuous() takes several arguments; take a look look at its help page to see all possible options. Here we mention a few of these. First, there is the name option, which is used to relabel the axis. The limits argument receives a vector of two values, containing the lower and upper limits of the plot. If any of them is set to NA, the corresponding limit will be determined automatically. Next, the breaks argument controls where the tick marks along the axis go. It is given as a vector, with its entries corresponding to the y-coordinates of the tick marks. Finally, labels determines what actually gets written on the axis at the tick mark points—it is therefore also a vector, its length matching that of breaks.\nAs an example, let us scale the y-axis of the previous graph in the following way. The axis label should read “Petal length [cm]”, instead of the current “Petal.Length”. It should go from 0 to 7, with a break at those two values and also halfway in between at 3.5. Here is how to do this:\n\niris %>%\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot() +\n  scale_y_continuous(name = \"Petal length [cm]\",\n                     limits = c(0, 7),\n                     breaks = c(0, 3.5, 7))\n\n\n\n\nWhat should we do if, for some reason, we would additionally like the “3.5” in the middle to be displayed as “7/2” instead (an exact value)? In that case, we can add an appropriate labels option as an argument to scale_y_continuous:\n\niris %>%\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot() +\n  scale_y_continuous(name = \"Petal length [cm]\",\n                     limits = c(0, 7),\n                     breaks = c(0, 3.5, 7),\n                     labels = c(\"0\", \"7/2\", \"7\"))\n\n\n\n\nThe x-axis can be scaled similarly. One important difference though is that here, the x-axis has a discrete scale. Since we are displaying the species along it, any value must be either setosa or versicolor or virginica; it makes no sense to talk about what is “halfway in between setosa and versicolor”. Therefore, one should use scale_x_discrete(). Its options are similar to those of scale_x_continuous(). For instance, let us override the axis label, spelling out that the three species belong to the genus Iris:\n\niris %>%\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot() +\n  scale_y_continuous(name = \"Petal length [cm]\",\n                     limits = c(0, 7),\n                     breaks = c(0, 3.5, 7),\n                     labels = c(\"0\", \"7/2\", \"7\")) +\n  scale_x_discrete(name = \"Species (genus: Iris)\")\n\n\n\n\nAlternatively, one could also redefine the labels and get an equally good graph:\n\niris %>%\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot() +\n  scale_y_continuous(name = \"Petal length [cm]\",\n                     limits = c(0, 7),\n                     breaks = c(0, 3.5, 7),\n                     labels = c(\"0\", \"7/2\", \"7\")) +\n  scale_x_discrete(labels = c(\"Iris setosa\", \"Iris versicolor\", \"Iris virginica\"))\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn case you would like to display the species names in italics, as is standard requirement when writing binomial nomenclature, feel free to add theme(axis.text.x = element_text(face = \"italic\")) to the end of the plot. We will not be going into more detail on tweaking themes, but feel free to explore the possibilities by looking at the help pages or Googling them.\n\n\nOther aesthetic mappings can also be adjusted, such as colour, fill, size, or alpha. One useful way to do it is through scale_colour_manual(), scale_fill_manual(), and so on. These are like scale_colour_discrete(), scale_fill_discrete() etc., except that they allow one to specify a discrete set of values by hand. Let us do this for color and fill:\n\niris %>%\n  ggplot() +\n  aes(x = Species, y = Petal.Length, colour = Species, fill = Species) +\n  geom_boxplot(alpha = 0.2) +\n  scale_y_continuous(name = \"Petal length [cm]\",\n                     limits = c(0, 7),\n                     breaks = c(0, 3.5, 7),\n                     labels = c(\"0\", \"7/2\", \"7\")) +\n  scale_x_discrete(labels = c(\"Iris setosa\", \"Iris versicolor\", \"Iris virginica\")) +\n  scale_colour_manual(values = c(\"steelblue\", \"goldenrod\", \"forestgreen\")) +\n  scale_fill_manual(values = c(\"steelblue\", \"goldenrod\", \"forestgreen\"))\n\n\n\n\nWe used the built-in color names \"steelblue\", \"goldenrod\", and \"forestgreen\" above. A full R color cheat sheet can be found here, for more options and built-in colors."
  },
  {
    "objectID": "Further_plotting_options.html#sec-faceting",
    "href": "Further_plotting_options.html#sec-faceting",
    "title": "7  Some further plotting options",
    "section": "7.3 Facets",
    "text": "7.3 Facets\nPlots can be faceted (subplots created and arranged in a grid layout) based on some variable or variables. For instance, let us create histograms of petal lengths in the iris dataset, like we did last time:\n\niris %>%\n  ggplot() +\n  aes(x = Petal.Length) +\n  geom_histogram()\n\n\n\n\nThis way, one cannot see which part of the histogram belongs to which species. One fix to this is to color the histogram by species—this is what we have done before. Another is to separate the plot into three facets, each displaying data for one of the species only:\n\niris %>%\n  ggplot() +\n  aes(x = Petal.Length) +\n  geom_histogram() +\n  facet_grid(. ~ Species)\n\n\n\n\nThe component facet_grid(x ~ y) means that the data will be grouped based on columns x and y, with the distinct values of column x making up the rows and those of column y the columns of the grid of plots. If one of them is replaced with a dot (as above), then that variable is ignored, and only the other variable is used in creating a row (or column) of subplots. So, to display the same data but with the facets arranged in one column instead of one row, we simply replace facet_grid(. ~ Species) with facet_grid(Species ~ .):\n\niris %>%\n  ggplot() +\n  aes(x = Petal.Length) +\n  geom_histogram() +\n  facet_grid(Species ~ .)\n\n\n\n\nIn this particular case, the above graph is preferable to the previous one, because the three subplots now share the same x-axis. This makes it easier to compare the distribution of petal lengths across the species.\nTo illustrate how to make a two-dimensional grid of facets, let us normalize the iris dataset using pivot_longer():\n\nas_tibble(iris) %>%\n  pivot_longer(cols = c(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width),\n               names_to = \"Trait\",\n               values_to = \"Measurement\")\n\n# A tibble: 600 × 3\n   Species Trait        Measurement\n   <fct>   <chr>              <dbl>\n 1 setosa  Sepal.Length         5.1\n 2 setosa  Sepal.Width          3.5\n 3 setosa  Petal.Length         1.4\n 4 setosa  Petal.Width          0.2\n 5 setosa  Sepal.Length         4.9\n 6 setosa  Sepal.Width          3  \n 7 setosa  Petal.Length         1.4\n 8 setosa  Petal.Width          0.2\n 9 setosa  Sepal.Length         4.7\n10 setosa  Sepal.Width          3.2\n# … with 590 more rows\n\n\nAs seen, now the Measurement in every row is characterized by two other variables: Species and Trait (i.e., whether the given value refers to the sepal length, petal width etc. of the given species). We can create a histogram of each measured trait for each species now, in a remarkably simple way:\n\nas_tibble(iris) %>%\n  pivot_longer(cols = c(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width),\n               names_to = \"Trait\",\n               values_to = \"Measurement\") %>%\n  ggplot() +\n  aes(x = Measurement) +\n  geom_histogram() +\n  facet_grid(Species ~ Trait)"
  },
  {
    "objectID": "Further_plotting_options.html#saving-plots",
    "href": "Further_plotting_options.html#saving-plots",
    "title": "7  Some further plotting options",
    "section": "7.4 Saving plots",
    "text": "7.4 Saving plots\nTo save the most recently created ggplot figure, simply type\n\nggsave(filename = \"graph.pdf\", width = 4, height = 3)\n\nHere filename is the name (with path and extension) of the file you want to save the figure into. The extension is important: by having specified .pdf, the system automatically saves the figure in PDF format. To use, say, PNG instead:\n\nggsave(filename = \"graph.png\", width = 4, height = 3)\n\nSince PDF is a vectorized file format (i.e., the file contains the instructions for generating the plot elements instead of a pixel representation), it is arbitrarily scalable, and is therefore the preferred way of saving and handling scientific graphs.\nThe width and height parameters specify, in inches, the dimensions of the saved plot. Note that this also scales some other plot elements, such as the size of the axis labels and plot legends. This means you can play with the width and height parameters to save the figure at a size where the labels are clearly visible without being too large.\nIn case you would like to save a figure that is not the last one that was generated, you can specify the plot argument to ggsave(). to do so, first you should assign a plot to a variable. For example:\n\np <- ggplot(iris) + # Assign the ggplot object to the variable p\n  aes(x = Petal.Length) +\n  geom_histogram()\n\nand then\n\nggsave(filename = \"graph.pdf\", plot = p, width = 4, height = 3)"
  },
  {
    "objectID": "Joining_data.html",
    "href": "Joining_data.html",
    "title": "8  Joining data",
    "section": "",
    "text": "So far, we have been working with a single table of data at a time. Often however, information about the same thing is scattered across multiple tables and files. In such cases, we sometimes want to join those separate tables into a single one. To illustrate how this can be done, let us create two simple tables. The first will contain the names of students, along with their chosen subject:\n\nlibrary(tidyverse)\n\nstudies <- tibble(name    = c(\"Sacha\", \"Gabe\", \"Alex\"),\n                  subject = c(\"Physics\", \"Chemistry\", \"Biology\"))\nprint(studies)\n\n# A tibble: 3 × 2\n  name  subject  \n  <chr> <chr>    \n1 Sacha Physics  \n2 Gabe  Chemistry\n3 Alex  Biology  \n\n\nThe second table contains slightly different information: it holds which year a given student is currently into their studies.\n\nstage <- tibble(name = c(\"Sacha\", \"Alex\", \"Jamie\"),\n                year = c(3, 1, 2))\nprint(stage)\n\n# A tibble: 3 × 2\n  name   year\n  <chr> <dbl>\n1 Sacha     3\n2 Alex      1\n3 Jamie     2\n\n\nNotice that, while Sacha and Alex appear in both tables, Gabe is only included in studies and Jamie only in stage. While in such tiny datasets this might seem like an avoidable oversight, such non-perfect alignment of data can be the norm when working with data spanning hundreds, thousands, or more rows. Here, for the purposes of illustration, we use small tables, but the principles we learn here apply in a broader context as well.\nThere are four commonly used ways of joining these tables into one single dataset. All of them follow the same general pattern: the arguments are two tibbles (or data frames) to be joined, plus a by = argument which lists the name(s) of the column(s) based on which the tables should be joined. The output is always a single tibble (data frame), containing some type of joining of the data. Let us now look at each joining method in detail.\n\n\nThe left_join function keeps only those rows that appear in the first of the two tables to be joined:\n\nleft_join(studies, stage, by = \"name\")\n\n# A tibble: 3 × 3\n  name  subject    year\n  <chr> <chr>     <dbl>\n1 Sacha Physics       3\n2 Gabe  Chemistry    NA\n3 Alex  Biology       1\n\n\nThere are two things to notice. First, Jamie is missing from the name column above, even though s/he did appear in the stage tibble. This is exactly the point of left_join: if a row entry in the joining column (specified in by =) does not appear in the first table listed in the arguments (here, the name column of studies), then it is omitted. Second, the year entry for Gabe is NA. This is because Gabe is absent from the stage table, and therefore has no associated year of study. Rather than make up nonsense, R fills out such missing data with NA values.\n\n\n\nThis function works just like left_join, except only those rows are retained which appear in the second of the two tables to be joined:\n\nright_join(studies, stage, by = \"name\")\n\n# A tibble: 3 × 3\n  name  subject  year\n  <chr> <chr>   <dbl>\n1 Sacha Physics     3\n2 Alex  Biology     1\n3 Jamie <NA>        2\n\n\nIn other words, this is exactly the same as calling left_join with its first two arguments reversed:\n\nleft_join(stage, studies, by = \"name\")\n\n# A tibble: 3 × 3\n  name   year subject\n  <chr> <dbl> <chr>  \n1 Sacha     3 Physics\n2 Alex      1 Biology\n3 Jamie     2 <NA>   \n\n\nThe only difference is in the ordering of the columns, but the data contained in the tables are identical.\nIn this case, the columns subject is NA for Jamie. The reason is the same as it was before: since the studies table has no name entry for Jamie, the corresponding subject area is filled in with a missing value NA.\n\n\n\nThis function retains only those rows which appear in both tables to be joined. For our example, since Gabe only appears in studies and Jamie only in stage, they will be dropped by inner_join and only Sacha and Alex are retained (since they appear in both tables):\n\ninner_join(studies, stage, by = \"name\")\n\n# A tibble: 2 × 3\n  name  subject  year\n  <chr> <chr>   <dbl>\n1 Sacha Physics     3\n2 Alex  Biology     1\n\n\n\n\n\nThe complement to inner_join, this function retains all rows in all tables, filling in missing values with NAs everywhere:\n\nfull_join(studies, stage, by = \"name\")\n\n# A tibble: 4 × 3\n  name  subject    year\n  <chr> <chr>     <dbl>\n1 Sacha Physics       3\n2 Gabe  Chemistry    NA\n3 Alex  Biology       1\n4 Jamie <NA>          2\n\n\nA useful table summarizing these options, taken from a more comprehensive (though slightly out-of-date) cheat sheet, is below:\n\n(As you see, apart from the four so-called mutating joins we have learned about, there are also two filtering joins included in this cheat sheet as well. We will not be covering those here, but feel free to check out their help pages.)\n\n\n\nIt is also possible to use the above joining functions specifying multiple columns to join data by. To illustrate how to do this and what this means, imagine that we slightly modify the student data. The first table will contain the name, study area, and year of study for each student. The second table will contain the name and study area of each student, plus whether they have passed their most recent exam:\n\nprogram  <- tibble(name     = c(\"Sacha\", \"Gabe\", \"Alex\"),\n                   subject  = c(\"Physics\", \"Chemistry\", \"Biology\"),\n                   year     = c(1, 3, 2))\nprint(program)\n\n# A tibble: 3 × 3\n  name  subject    year\n  <chr> <chr>     <dbl>\n1 Sacha Physics       1\n2 Gabe  Chemistry     3\n3 Alex  Biology       2\n\n\n\nprogress <- tibble(name     = c(\"Sacha\", \"Gabe\", \"Jamie\"),\n                   subject  = c(\"Physics\", \"Chemistry\", \"Biology\"),\n                   examPass = c(TRUE, FALSE, TRUE))\nprint(progress)\n\n# A tibble: 3 × 3\n  name  subject   examPass\n  <chr> <chr>     <lgl>   \n1 Sacha Physics   TRUE    \n2 Gabe  Chemistry FALSE   \n3 Jamie Biology   TRUE    \n\n\nAnd now, since the tables share not just one but two columns, it makes sense to join them using both. This can be done by specifying each column inside a vector in the by = argument. For example, left-joining program and progress by both name and subject leads to a joint table in which all unique name-subject combinations found in program are retained, but those found only in progress are discarded:\n\nleft_join(program, progress, by = c(\"name\", \"subject\"))\n\n# A tibble: 3 × 4\n  name  subject    year examPass\n  <chr> <chr>     <dbl> <lgl>   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n3 Alex  Biology       2 NA      \n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs mentioned, the columns by which one joins the tables must be included in a vector. Forgetting to do this will either fail or produce faulty output:\n\nleft_join(program, progress, by = \"name\", \"subject\")\n\n# A tibble: 3 × 5\n  name  subject.x  year subject.y examPass\n  <chr> <chr>     <dbl> <chr>     <lgl>   \n1 Sacha Physics       1 Physics   TRUE    \n2 Gabe  Chemistry     3 Chemistry FALSE   \n3 Alex  Biology       2 <NA>      NA      \n\n\nThe problem is that the comma after name is interpreted as the start of the next argument to left_join, instead of being a part of the by = argument. Again, the solution is simple: enclose multiple column names in a vector.\n\n\nThe other joining functions also work as expected:\n\nright_join(program, progress, by = c(\"name\", \"subject\"))\n\n# A tibble: 3 × 4\n  name  subject    year examPass\n  <chr> <chr>     <dbl> <lgl>   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n3 Jamie Biology      NA TRUE    \n\n\n\ninner_join(program, progress, by = c(\"name\", \"subject\"))\n\n# A tibble: 2 × 4\n  name  subject    year examPass\n  <chr> <chr>     <dbl> <lgl>   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n\n\n\nfull_join(program, progress, by = c(\"name\", \"subject\"))\n\n# A tibble: 4 × 4\n  name  subject    year examPass\n  <chr> <chr>     <dbl> <lgl>   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n3 Alex  Biology       2 NA      \n4 Jamie Biology      NA TRUE"
  },
  {
    "objectID": "Joining_data.html#binding-rows-and-columns-to-a-table",
    "href": "Joining_data.html#binding-rows-and-columns-to-a-table",
    "title": "8  Joining data",
    "section": "8.2 Binding rows and columns to a table",
    "text": "8.2 Binding rows and columns to a table\nOccasionally, a simpler problem presents itself: there is a single dataset, but its rows are contained across separate tables. For example, a table containing student names and subject areas might be spread across two tables, like this:\n\nstudies1 <- tibble(name    = c(\"Sacha\", \"Gabe\", \"Alex\"),\n                   subject = c(\"Physics\", \"Chemistry\", \"Biology\"))\nprint(studies1)\n\n# A tibble: 3 × 2\n  name  subject  \n  <chr> <chr>    \n1 Sacha Physics  \n2 Gabe  Chemistry\n3 Alex  Biology  \n\n\n\nstudies2 <- tibble(name    = c(\"Jamie\", \"Ashley\", \"Dallas\", \"Jordan\"),\n                   subject = c(\"Geology\", \"Mathematics\", \"Philosophy\", \"Physics\"))\nprint(studies2)\n\n# A tibble: 4 × 2\n  name   subject    \n  <chr>  <chr>      \n1 Jamie  Geology    \n2 Ashley Mathematics\n3 Dallas Philosophy \n4 Jordan Physics    \n\n\nThe tables have the exact same structure, in that the column names and types are identical. It’s just that the rows are, for some reason, disparate. To combine them together, we could recourse to full-joining the tables by both their columns:\n\nfull_join(studies1, studies2, by = c(\"name\", \"subject\"))\n\n# A tibble: 7 × 2\n  name   subject    \n  <chr>  <chr>      \n1 Sacha  Physics    \n2 Gabe   Chemistry  \n3 Alex   Biology    \n4 Jamie  Geology    \n5 Ashley Mathematics\n6 Dallas Philosophy \n7 Jordan Physics    \n\n\nThis, however, is not necessary. Whenever all we need to do is take two tables and stick their rows together, there is the simpler bind_rows:\n\nbind_rows(studies1, studies2)\n\n# A tibble: 7 × 2\n  name   subject    \n  <chr>  <chr>      \n1 Sacha  Physics    \n2 Gabe   Chemistry  \n3 Alex   Biology    \n4 Jamie  Geology    \n5 Ashley Mathematics\n6 Dallas Philosophy \n7 Jordan Physics    \n\n\nSimilarly, in case two tables have the same number of rows but different columns, one can stick their columns together using bind_cols. For example, suppose we have\n\nstudies <- tibble(name    = c(\"Sacha\", \"Gabe\", \"Alex\"),\n                  subject = c(\"Physics\", \"Chemistry\", \"Biology\"))\nprint(studies)\n\n# A tibble: 3 × 2\n  name  subject  \n  <chr> <chr>    \n1 Sacha Physics  \n2 Gabe  Chemistry\n3 Alex  Biology  \n\n\nas well as a table with year of study and result of last exam only:\n\nyearExam <- tibble(year     = c(3, 1, 2),\n                   examPass = c(FALSE, TRUE, TRUE))\nprint(yearExam)\n\n# A tibble: 3 × 2\n   year examPass\n  <dbl> <lgl>   \n1     3 FALSE   \n2     1 TRUE    \n3     2 TRUE    \n\n\nWe can now join these using bind_cols:\n\nbind_cols(studies, yearExam)\n\n# A tibble: 3 × 4\n  name  subject    year examPass\n  <chr> <chr>     <dbl> <lgl>   \n1 Sacha Physics       3 FALSE   \n2 Gabe  Chemistry     1 TRUE    \n3 Alex  Biology       2 TRUE"
  },
  {
    "objectID": "Joining_data.html#exercises",
    "href": "Joining_data.html#exercises",
    "title": "8  Joining data",
    "section": "8.3 Exercises",
    "text": "8.3 Exercises\nWe have used the data of Fauchald et al. (2017) before in other exercises. As a reminder, they tracked the population size of various herds of caribou in North America over time, and correlated population cycling with the amount of vegetation and sea-ice cover. Two files from their data are on Lisam: pop_size.tsv (herd population sizes), and sea_ice.tsv (sea ice cover per year and month).\n\nLoad these two datasets into two variables. They could be called pop and ice, for instance. Look at the data to familiarize yourself with them. How many rows and columns are in each?\nBefore doing anything else: how many rows will there be in the table that is the left join of pop and ice, based on the two columns Herd and Year? Perform the left join to see if you were correct. Where do you see NAs in the table, and why?\nNow do the same with right-joining, inner-joining, and full-joining pop and ice.\n\n\n\n\n\nFauchald, Per, Taejin Park, Hans Tømmervik, Ranga Myneni, and Vera Helene Hausner. 2017. “Arctic greening from warming promotes declines in caribou populations.” Science Advances 3 (4): e1601365. https://doi.org/10.1126/sciadv.1601365."
  },
  {
    "objectID": "Intro_statistics.html",
    "href": "Intro_statistics.html",
    "title": "9  Introducing statistical inference",
    "section": "",
    "text": "In this chapter we will take a first look at statistical tests of the simplest kind: comparing two groups of data. To illustrate why statistical inference is needed in such comparisons, let us take a look at a fictive dataset which contains an equally fictive set of weight measurements of different bird individuals from the same species. The birds are assumed to come from two islands, a larger and a smaller one. The question is: do the data provide evidence of insular dwarfism—that is, the phenomenon that the body sizes of species tend to decline on small islands?\nWe can load the data:\n\nlibrary(tidyverse)\n\nbird <- read_csv(\"fictive_bird_example.csv\")\nprint(bird, n = Inf)\n\n# A tibble: 40 × 2\n   island  weight\n   <chr>    <dbl>\n 1 smaller  21.8 \n 2 smaller  19.0 \n 3 smaller  19.0 \n 4 smaller  21.2 \n 5 smaller  15.8 \n 6 smaller  14.3 \n 7 smaller  19.9 \n 8 smaller   9.68\n 9 smaller  17.0 \n10 smaller  12.7 \n11 smaller  22.0 \n12 smaller  15.7 \n13 smaller  16.7 \n14 smaller  27.9 \n15 smaller  17.4 \n16 smaller  12.6 \n17 smaller  33.4 \n18 smaller  25.8 \n19 smaller  20.1 \n20 smaller  21.3 \n21 larger   20.4 \n22 larger   28.3 \n23 larger   19.2 \n24 larger   24.1 \n25 larger   27.7 \n26 larger   16.1 \n27 larger   17.3 \n28 larger   21.2 \n29 larger   28.8 \n30 larger   21.9 \n31 larger   17.4 \n32 larger   13.0 \n33 larger   26.6 \n34 larger   27.0 \n35 larger   19.0 \n36 larger   25.2 \n37 larger   16.5 \n38 larger   25.2 \n39 larger   24.4 \n40 larger   24.7 \n\n\nA quick visualization below looks promising, with individuals on the smaller island indeed appearing to be smaller:\n\nggplot(bird) +\n  aes(x = island, y = weight) +\n  geom_boxplot(colour = \"steelblue\", fill = \"steelblue\",\n               alpha = 0.2, outlier.shape = NA) +\n  geom_jitter(alpha = 0.4, width = 0.05, colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the above plot, geom_jitter was used to display the actual data points that are summarized by the boxplots. The function geom_jitter is just like geom_point, except it adds a random sideways displacement to the data points, to reduce visual overlap between them. The width = 0.05 option restricts this convulsion of the points to a relatively narrow band. Since all data points are now displayed, it makes no sense to rely on the feature of box plots which explicitly draws points that are classified as outliers—their plotting is turned off by the outlier.shape = NA argument to geom_boxplot. It is a useful exercise to play around with these settings, to see what the effects of changing them are.\n\n\nFurthermore, the computed difference between the means and medians of the two samples are also clearly different:\n\nbird %>%\n  group_by(island) %>%\n  summarise(mean = mean(weight), median = median(weight))\n\n# A tibble: 2 × 3\n  island   mean median\n  <chr>   <dbl>  <dbl>\n1 larger   22.2   23.0\n2 smaller  19.2   19.0\n\n\nCan we conclude that the two samples are indeed different, and birds on the smaller island tend to be smaller, supporting the insular dwarfism hypothesis? As mentioned above, the data are fictive—they are not based on actual measurements. In fact, these “observations” were created by sampling each data point from the same distribution, regardless of island: a normal distribution with mean 20 and standard deviation 5. This means that any supposedly observed difference between the samples must be accidental. Would various statistical methods detect a difference? One thing we can do is what we have already done in Section 6.1: compute the 95% confidence intervals of the means, and see whether and how much they overlap.\n\nbird %>%\n  group_by(island) %>%\n  summarise(mean = mean(weight), # Means of the two groups\n            sd = sd(weight), # Standard deviations\n            N = n(), # Sample sizes\n            SEM = sd / sqrt(N), # Standard errors of the means\n            CI = qt(0.975, N - 1)) %>% # Confidence intervals\n  ungroup() %>% # Ungroup the data (not necessary here, but a good habit)\n  ggplot(aes(x = island, y = mean, ymin = mean - CI, ymax = mean + CI)) +\n  geom_point(colour = \"steelblue\") +\n  geom_errorbar(colour = \"steelblue\", width = 0.2) +\n  ylab(\"mean and 95% confidence interval\") +\n  theme_bw()\n\n\n\n\nAs seen, there is overlap between the 95% confidence intervals. While that by itself is not conclusive, it is an indication that the difference between the two samples may not be as relevant as it might have initially looked.\nLet us wait no longer, and perform a statistical test. One widely used test to check if two samples differ from one another is the Wilcoxon test (also known as the Mann-Whitney test). Its implementation is very simple:\n\nwilcox.test(weight ~ island, data = bird)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 271, p-value = 0.05589\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe function takes two arguments: a formula, and the data to be analyzed, in the form of a data frame or tibble. The formula in the first argument establishes a relationship between two (or more) columns of the data. We will discuss formulas and their syntax in more detail later. For now: the way to write them is to first type the variable we wish to predict or explain, then a tilde (~), and then the explanatory variable (predictor) by which the data are subdivided into the two groups. In our case, we are trying to explain the difference in weight between the islands, so weight comes before the tilde and the predictor island comes after.\n\n\n\n\n\n\nNote\n\n\n\nWe have used the tilde (~) before, when faceting plots with facet_grid; see Section 7.3. However, the tilde used in facet_grid has nothing to do with the one we use in formulas: the two symbols happen to be identical, but that is just an accident.\n\n\nLet us now look at the output produced by wilcox.test above. Most of it is not particularly relevant for us: we are first informed that a Wilcoxon rank sum exact test is being performed; then we see that we are explaining weight differences by island; then we see the test statistic W itself (we need not concern ourselves with its precise meaning); then the p-value; and finally, a reminder of what the alternative hypothesis is (the null hypothesis is that the shift in location is in fact zero).\nOne piece of datum we are interested in is the p-value. This tells us the probability of observing a result at least as extreme as we see in our data, given that the null hypothesis is true. In our case, we are measuring how different the two samples are from one another, and the null hypothesis is that the only difference we might observe is due purely to chance. We find that the probability of this being the case, given our data, is 0.056. What this number means is that we have a roughly one-in-eighteen chance that the observed difference is nothing but a fluke. Since science leans towards erring on the side of caution (i.e., we would rather miss out on making a discovery than falsely claim having seen an effect), this value is in general a bit too high for comfort. And indeed: since in this case we know that the data were generated by sampling from the same distribution, any distinctiveness between them is incidental.\n\n\n\n\n\n\nNote\n\n\n\nIn many subfields of science, it is standard practice to consider p-values falling below 0.05 as “significant” and those falling above as “non-significant”. Besides the fact that such a one-size-fits-all approach ought to be suspect even under the best of circumstances, a significance threshold of 0.05 is awfully permissive to errors. In fact, we should expect about one out of twenty of all papers ever published which have adopted this cutoff to be wrong! Digging deeper into this issue reveals that the figure is possibly much worse—see, e.g., Colquhoun (2014). One way to ameliorate the problem is to adopt a less exclusive and parochial view of p-values. Instead of having rigid significance thresholds, p-values can simply be reported and interpreted for what they are: the probability that the outcome is at least as extreme as observed, assuming that the null model holds. Treating this information as just one piece of the data puzzle is a first step towards avoiding the erroneous classification of random patterns as results.\n\n\nApart from the p-value (is the observed effect likely to be due to chance alone?), another important piece of information is some measure of the effect size: how different are the two samples? We have already computed the means and medians; their differences across the islands provide one way of measuring this effect size. It is possible to add a calculation of the effect size, as well as the confidence intervals, to the Wilcoxon test. All one needs to do is pass conf.int = TRUE as an argument:\n\nwilcox.test(weight ~ island, data = bird, conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 271, p-value = 0.05589\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -0.018386  6.820811\nsample estimates:\ndifference in location \n              3.436204 \n\n\nAs additional output, we now receive the 95% confidence interval, as well as the explicit difference between the “locations” of the two samples. (A small word of caution: this “difference in location” is neither the difference of the means nor the difference of the medians, but the median of the difference between samples from the two groups of data—feel free to check the help pages by typing ?wilcox.test for more details.) The confidence interval, as seen, includes zero, which can be interpreted as being difficult to rule out the possibility that the observed difference in location is just due to chance.\nThe default confidence level of 95% can be changed via the conf.level argument. For example, to use a 99% confidence interval instead:\n\nwilcox.test(weight ~ island, data = bird, conf.int = TRUE, conf.level = 0.99)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 271, p-value = 0.05589\nalternative hypothesis: true location shift is not equal to 0\n99 percent confidence interval:\n -1.16832  7.70439\nsample estimates:\ndifference in location \n              3.436204 \n\n\nIn summary, the Wilcoxon test failed to yield serious evidence in favor of rejecting the null hypothesis. Therefore, we cannot claim with any confidence that our fictive birds have different sizes across the two islands. While failure to reject the null is not the same as confirming that the null is true (absence of evidence is not evidence of absence!), the notion that the two samples are different could not be supported. In this particular case, since we ourselves have created the original data using the null hypothesis, we have the privilege of knowing that this is the truth. When working with real data, such knowledge is generally not available."
  },
  {
    "objectID": "Intro_statistics.html#some-general-conclusions",
    "href": "Intro_statistics.html#some-general-conclusions",
    "title": "9  Introducing statistical inference",
    "section": "9.2 Some general conclusions",
    "text": "9.2 Some general conclusions\nThe example of Section 9.1 illustrates two important general points.\nFirst, instead of jumping into statistical tests, we started the analysis with qualitative and descriptive data exploration: we plotted the data, computed its means and medians, etc. This is almost always the correct way to go. (The exception is when one analyzes data from a pre-registered experimental design. In that case, one must follow whatever statistical techniques were agreed upon before data collection even started.) To perform tests “blindly”, without visually exploring the data first, is rarely a good idea.\nSecond, we only performed the statistical test after we have made an effort to understand the data, and after setting clear expectations about what we might find. We knew, going into the test, that finding a difference between the two island samples was questionable. And indeed, the test revealed that such a distinction cannot be made in good conscience. Following a similar strategy for all statistical inference can prevent a lot of frustration. To make the point more sharply: do not perform a statistical test without knowing what its result will be! A more nuanced way of saying the same thing is that if you think you see a relationship in your data, then you should also make sure that your observation is not just a mirage, by using a statistical test. But if a relationship is not visually evident, you should first ask the question whether it makes much sense to try to explore it statistically."
  },
  {
    "objectID": "Intro_statistics.html#parametric-versus-non-parametric-tests",
    "href": "Intro_statistics.html#parametric-versus-non-parametric-tests",
    "title": "9  Introducing statistical inference",
    "section": "9.3 Parametric versus non-parametric tests",
    "text": "9.3 Parametric versus non-parametric tests\nThe Wilcoxon test we employed is non-parametric: it does not assume either the data or the residuals to come from any particular distribution. This is both the test’s advantage and disadvantage. It is an advantage because fewer assumptions must be satisfied for the test to be applicable. Non-parametric techniques also tend to fare better when only limited data are available. Since in many areas of biology (such as ecology) data are hard to come by, datasets are correspondingly small, making non-parametric techniques a natural candidate for interrogating the data with. On the downside, non-parametric tests tend to be less powerful than parametric ones: the additional assumptions of parametric tests (when they are actually met by real data) may allow for sharper inference.\nOne of the most common parametric alternatives to the Wilcoxon test is Welch’s t-test. Like the Wilcoxon test, the t-test assumes the independence of the two samples. In addition, it also assumes that the samples are normally distributed. Let us see what the t-test says about our fictive bird data:\n\nt.test(weight ~ island, data = bird)\n\n\n    Welch Two Sample t-test\n\ndata:  weight by island\nt = 1.8704, df = 36.92, p-value = 0.06937\nalternative hypothesis: true difference in means between group larger and group smaller is not equal to 0\n95 percent confidence interval:\n -0.2520327  6.2997175\nsample estimates:\n mean in group larger mean in group smaller \n             22.18479              19.16095 \n\n\nThe format of the output is similar to the one with the Wilcoxon test; the main differences are that the confidence intervals are included by default, and that the observed means of the two samples are also reported. Like with wilcox.test, the confidence level can be adjusted through the argument conf.level:\n\nt.test(weight ~ island, data = bird, conf.level = 0.99)\n\n\n    Welch Two Sample t-test\n\ndata:  weight by island\nt = 1.8704, df = 36.92, p-value = 0.06937\nalternative hypothesis: true difference in means between group larger and group smaller is not equal to 0\n99 percent confidence interval:\n -1.366524  7.414209\nsample estimates:\n mean in group larger mean in group smaller \n             22.18479              19.16095 \n\n\nImportantly, the message conveyed by the p-value of 0.069 is in line with what we saw from the Wilcoxon test: there is no strong evidence to claim that the two samples are different. In this particular case, it does not make any qualitative difference whether one uses the parametric t-test or the non-parametric Wilcoxon test.\nIn other situations, the choice of test may matter more. The following data have also been artificially created as an example. They contain two groups of “measurements”. In the first group (group x), the data have been sampled from a lognormal distribution with mean 0 and standard deviation 1. In the second group (group y), the data have been sampled from a lognormal distribution with mean 1 and standard deviation 1. Thus, we know a priori that the two distributions from which the samples were created are shifted compared to one another, and a statistical test may be able to reveal this shift. However, since the data are heavily non-normal, a t-test will struggle to do so. Let us load and visualize the data first:\n\nexample <- read_csv(\"t_wilcox_example.csv\")\n\nggplot(example, aes(x = measurement)) +\n  geom_histogram(bins = 25, alpha = 0.3,\n                 colour = \"steelblue\", fill = \"steelblue\") +\n  facet_wrap(~ group, labeller = label_both) +\n  theme_bw()\n\n\n\n\nApplying a t-test first:\n\nt.test(measurement ~ group, data = example)\n\n\n    Welch Two Sample t-test\n\ndata:  measurement by group\nt = -1.4796, df = 19.622, p-value = 0.1549\nalternative hypothesis: true difference in means between group x and group y is not equal to 0\n95 percent confidence interval:\n -19.853561   3.388384\nsample estimates:\nmean in group x mean in group y \n        2.33571        10.56830 \n\n\nThe parametric t-test cannot detect a difference between the samples—though, since its assumption of normality is violated, such a test should not have been attempted in the first place. By contrast, the non-parametric Wilcoxon test (correctly) suggests that there might be a difference:\n\nwilcox.test(measurement ~ group, data = example, conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  measurement by group\nW = 106, p-value = 0.01031\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -3.039534 -0.378063\nsample estimates:\ndifference in location \n             -1.798516 \n\n\nThe p-value of 0.01 means that there is a one-in-a-hundred probability that the observed difference is due only to chance."
  },
  {
    "objectID": "Intro_statistics.html#statistical-tests-in-a-data-analysis-pipeline-the-dot-notation",
    "href": "Intro_statistics.html#statistical-tests-in-a-data-analysis-pipeline-the-dot-notation",
    "title": "9  Introducing statistical inference",
    "section": "9.4 Statistical tests in a data analysis pipeline: The dot notation",
    "text": "9.4 Statistical tests in a data analysis pipeline: The dot notation\nOne natural question is whether statistical tests can be included in a data analysis pipeline. The answer is yes, but not without having to consider some complications that are due to historical accidents. Attempting the following results in an error:\n\nbird %>%\n  wilcox.test(weight ~ island, conf.int = TRUE)\n\nError in wilcox.test.default(., weight ~ island, conf.int = TRUE): 'x' must be numeric\n\n\nThe reason for the error is in how the pipe operator %>% works: it substitutes the expression on the left of the pipe into the first argument of the expression on the right. This works splendidly with functions of the tidyverse, since these are designed to always take the data as their first argument. However, a quick glance at the documentation of both wilcox.test and t.test (neither of which are tidyverse functions) reveals that they take the data as their second argument, with the formula coming first. A naive application of the pipe operator, like above, will therefore fail.\nFortunately, there is a way to work around this. One may use the dot (.) in an expression to stand for whatever was being piped into it. The following will therefore work:\n\nbird %>%\n  wilcox.test(weight ~ island, data = ., conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 271, p-value = 0.05589\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -0.018386  6.820811\nsample estimates:\ndifference in location \n              3.436204 \n\n\nThe dot stands for the object being piped in—which, in our case, is simply the tibble bird. The data = . argument therefore evaluates to data = bird.\nThis ought to conclude all there is to using the dot notation, except that there are a few subtleties involved. As we know, the pipe substitutes the object on the left as the first argument of the object on the right. In the above expression, this behavior was overridden: by using the dot, the data were substituted as the second argument instead. This will be the case as long as the dot appears alone, without being nested into some other function call. However, if the dot is part of another function, then the expression to the left of the pipe will additionally be substituted, silently, as the first argument on the right. For example, suppose that we want to remove rows with missing data from the table first, before doing a statistical test. We can do this with the drop_na function. (Since the bird table does not have any rows with NA values, this does not actually do anything in this context, but it is easy to imagine data where some rows do have missing entries, in which case removing them first could be useful.) But then, the following piece of code will fail:\n\nbird %>%\n  wilcox.test(weight ~ island, data = drop_na(.), conf.int = TRUE)\n\nError in wilcox.test.default(., weight ~ island, data = drop_na(.), conf.int = TRUE): 'x' must be numeric\n\n\nHere the dot appears as part of the drop_na function. Thus, bird is not just substituted where the dot is, but also silently as the first function argument to wilcox.test. Which then makes the function think that bird is the formula, and weight ~ island is the data—consequently, we get an error.\nIf necessary, it is possible to override this behavior, by putting curly braces around the expression(s) into which we are piping. The curly braces always prevent the silent substitution of the piped expression, which will therefore only be substituted wherever the dot explicitly appears. The above, faulty code can therefore be fixed by using curly braces:\n\nbird %>%\n  { wilcox.test(weight ~ island, data = drop_na(.), conf.int = TRUE) }\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 271, p-value = 0.05589\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -0.018386  6.820811\nsample estimates:\ndifference in location \n              3.436204 \n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile this behavior of the dot notation may appear arbitrary, there is method to the madness. In most situations, including the dot within another function indeed means that we would like the dot to also be passed as the first argument. For example, we could add row numbers to the bird tibble, by creating a new column whose entries range from one to the number of columns in the data. The number of columns can be obtained with the nrow function. Therefore, the following is possible:\n\nbird %>%\n  mutate(rowID = 1:nrow(.)) %>% # Create new column, numbering the rows\n  select(rowID, island, weight) # Reorder the columns so rowID is first\n\n# A tibble: 40 × 3\n   rowID island  weight\n   <int> <chr>    <dbl>\n 1     1 smaller  21.8 \n 2     2 smaller  19.0 \n 3     3 smaller  19.0 \n 4     4 smaller  21.2 \n 5     5 smaller  15.8 \n 6     6 smaller  14.3 \n 7     7 smaller  19.9 \n 8     8 smaller   9.68\n 9     9 smaller  17.0 \n10    10 smaller  12.7 \n# … with 30 more rows\n\n\nIn this case, the first argument to mutate, as always, is the piped-in tibble. Due to the convention that the dot gets silently substituted as the first argument as well (because the dot appeared within a function call), we did not need to write mutate(., rowID = 1:nrow(.)) above in the second line. Thus, the seemingly arbitrary rules do lead to less awkward code, at least some of the time."
  },
  {
    "objectID": "Intro_statistics.html#exercises",
    "href": "Intro_statistics.html#exercises",
    "title": "9  Introducing statistical inference",
    "section": "9.5 Exercises",
    "text": "9.5 Exercises\n\nAtmospheric ozone levels are important for urban gardens, because ozone concentrations above 80 parts per hundred million can damage lettuce plants. The file ozone.csv contains measured ozone concentrations in gardens distributed around a city. The gardens were either east or west of the city center. Is there a difference in average ozone concentration between eastern vs. western gardens? Create a plot of the concentrations first and try to guess the answer, before performing any statistical analyses. Then do the statistics, using both nonparametric and parametric methods. What result do you find? Do the outcomes of the different methods agree with one another?\nIn the built-in iris dataset, the sepal and petal characteristics of the various species differ. Let us focus on just petal length here. Is there a detectable difference between the petal lengths of Iris versicolor and I. virginica? (Since we are not working with I. setosa in this exercise, make sure to remove them from the data first—e.g., by using filter.) Like before, start with a graph which you can base a hypothesis on. Then perform both nonparametric and parametric tests to see if your hypothesis can be supported.\nRepeat the above exercise for any and all of the possible trait-species pair combinations in the iris dataset. E.g., one can compare the sepal widths of I. setosa and I. virginica, or the petal widths of I. versicolor and I. setosa, and so on.\n\n\n\n\n\nColquhoun, David. 2014. “An investigation of the false discovery rate and the misinterpretation of p-values.” Royal Society Open Science 1 (3): 140216. https://doi.org/10.1098/rsos.140216."
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "10  References",
    "section": "",
    "text": "Colquhoun, David. 2014. “An investigation of\nthe false discovery rate and the misinterpretation of\np-values.” Royal Society Open Science 1 (3):\n140216. https://doi.org/10.1098/rsos.140216.\n\n\nFauchald, Per, Taejin Park, Hans Tømmervik, Ranga Myneni, and Vera\nHelene Hausner. 2017. “Arctic greening from\nwarming promotes declines in caribou populations.”\nScience Advances 3 (4): e1601365. https://doi.org/10.1126/sciadv.1601365.\n\n\nGillespie, John H. 2004. Population Genetics: A Concise Guide.\nBaltimore, MD, USA: Johns Hopkins University Press.\n\n\nGoldberg, Emma E., Joshua R. Kohn, Russell Lande, Kelly A. Robertson,\nStephen A. Smith, and Boris Igić. 2010. “Species Selection\nMaintains Self-Incompatibility.” Science 330\n(6003): 493–95. https://doi.org/10.1126/science.1194513.\n\n\nSmith, Felisa A., S. Kathleen Lyons, S. K. Morgan Ernest, Kate E. Jones,\nDawn M. Kaufman, Tamar Dayan, Pablo A. Marquet, James H. Brown, and John\nP. Haskell. 2003. “Body Mass of Late\nQuaternary Mammals.” Ecology 84 (12): 3403. https://doi.org/10.1890/02-9003.\n\n\nWilkinson, Leland. 2006. The Grammar of\nGraphics. Secaucus, NJ, USA: Springer Science & Business\nMedia."
  }
]