[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to data analysis and visualization with R",
    "section": "",
    "text": "Welcome\nLecture notes for NBIC58 (Analysis of Biological Data) at Linköping University."
  },
  {
    "objectID": "Intro_R_RStudio.html#overview",
    "href": "Intro_R_RStudio.html#overview",
    "title": "1  Introduction to R and RStudio",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nThis chapter introduces R and RStudio. R is a free and open-source programming language for statistics, graphing, and modeling, originally developed by statisticians. In recent years, R has become extremely popular among biologists, and you will almost certainly encounter it as part of real-world research projects. In this course, we will be learning some of the ways in which R can be used for efficient data analysis and visualization.\nRStudio is an “integrated development environment” (IDE) for R, which means it is a software application that lets you write, run, and interact graphically with programs. RStudio integrates a text editor, the R console (where you run R commands), package management, plotting, help, and more."
  },
  {
    "objectID": "Intro_R_RStudio.html#installing-r-and-rstudio",
    "href": "Intro_R_RStudio.html#installing-r-and-rstudio",
    "title": "1  Introduction to R and RStudio",
    "section": "1.2 Installing R and RStudio",
    "text": "1.2 Installing R and RStudio\nYou can download the most up-to-date R distribution for free here:\nhttp://www.r-project.org\nRun the installer as directed and you should be set to go. We will not interact with the installed R application directly, but the R software components you install will be used by RStudio under the hood.\nTo install RStudio on your computer, download it from here:\nhttps://www.rstudio.com/products/rstudio/download/\nOn a Mac, open the downloaded disk image and drag the RStudio application into your Applications folder. On Windows, run the installer you downloaded."
  },
  {
    "objectID": "Intro_R_RStudio.html#getting-around-rstudio",
    "href": "Intro_R_RStudio.html#getting-around-rstudio",
    "title": "1  Introduction to R and RStudio",
    "section": "1.3 Getting Around RStudio",
    "text": "1.3 Getting Around RStudio\nRStudio should be available in the usual places: the Applications folder (on a Mac) or the Start menu (on Windows). When you start it up, you will see four sections of the screen. In short:\n\nUpper-left: an area for viewing and editing text files\nLower-left: Console, where you send commands to R\nUpper-right:\n\nEnvironment: for loading, saving, and examining data\nHistory: a list of commands you have typed\nConnections: for establishing remote connections with data sources\nTutorial: resources for learning R\n\nLower-right:\n\nFiles: a list of files in the “working directory” (more on this later)\nPlots: where the plots (graphs) you draw show up\nPackages: an area for managing installed R packages\nHelp: access to all the official documentation for R\nViewer: you can view certain objects here, e.g., formatted tables of data\n\n\n\n\n\nRStudio starting screen\n\n\n\n1.3.1 A Simple Calculation\nEven if you don’t know R, you can start by typing some simple calculations into the console. The > symbol indicates that R is waiting for you to type something. Click on the console, type 2 + 2, hit Enter (or Return on a Mac), and you should see R produce the right answer:\n\n> 2 + 2\n[1] 4\n>\n\nNow, press the Up arrow on the keyboard. You will notice that the 2 + 2 you typed before shows up. You can use the Up and Down arrows to go back and forth through past things you have typed, which can save a lot of repetitive typing when you are trying things out. You can change the text in these historical commands: change the 2 to a 3 and press Enter (Return, on a Mac) again:\n\n> 2 + 3\n[1] 5\n>\n\n(The [1] at the beginning of the result just means that the following number is at position 1 of a vector. In this case, the vector only has one element, but when R needs to print out a long vector, it splits it into into multiple lines tells you at the beginning of each line what position you are at.)\nBefore going deeper into R programming, we need to discuss a few things to enable you to get around R and RStudio more easily.\n\n\n1.3.2 Writing R scripts\nThe upper-right part of RStudio is a simple text editor where you can write R code. But, instead of having to enter it one line at a time as we did in the console above, you can string long sequences of R instructions together that build on one another. You can save such a text file (Ctrl-S on Windows; Cmd-S on a Mac), giving it an appropriate name. It is then known as an R script, a text file containing R code that can be run from within R.\nAs an example, enter the following code. Do not worry about how or why it works just yet. It is a simple simulation and visualization of regulated (logistic) population growth:\n\ntime <- 1:15\ngrowthFun <- function(x, y, lambda = 1.8) lambda * x * (1 - x)\npop <- Reduce(growthFun, rep(0.01, times = length(time)), accumulate = TRUE)\nplot(time, pop, xlab = \"time\", ylab = \"population density\", type = \"b\")\n\nAfter having typed this, highlight all lines (either with a mouse, or by pressing Ctrl-A on Windows / Cmd-A on a Mac, or by using the arrow keys while holding the Shift key down). Then, to send these instructions to R for processing, press Ctrl-Enter (Windows) or Cmd-Return (Mac). If all went well, the lower right screen section should have jumped to the Plots panel, showing the following graph:\n\n\n\n\n\n\n\n1.3.3 Setting the Working Directory\nWhen you ask R to run a program or load data from a file, it needs to know where to find the file. Unless you specify the complete path to the file on your machine, it assumes that file names are relative to what is called the “working directory”.\nThe first thing you should do when starting a project is to create a directory (folder) on your computer to store all the files related to the project, and then tell R to set the working directory to that location.\nThe R function setwd(\"/path/to/directory\") is used to set the working directory, where you substitute in the actual path and directory name in place of path/to/directory. In turn, getwd() tells you what the current working directory is. The path can be found using File Explorer on a Windows PC, but you will need to change backslashes to forward slashes (\\ to /). On a Mac, you can find the path by selecting a folder and choosing File > Get Info (the path is under “Where:”).\nThere is also a convenient graphical way to set the working directory in RStudio. In the Files panel, you can navigate around the computer’s file space. You can do this either in the panel itself, or using the ellipsis (…) to bring up the system-standard file browser. Looking around there does not immediately set the working directory, but you can set it by clicking the “More” button and choosing “Set as Working Directory”.\n\n\n\n\n\n\nNote\n\n\n\nIt is worth repeating: finding the appropriate directory in the Files panel is not enough. It will not set the working directory automatically. You need to actually choose “Set as Working Directory” by clicking on it.\n\n\nYou may have also noticed that you don’t actually need to type getwd(): the RStudio Console panel shows the current working directory below the word “Console”.\n\n\n1.3.4 Packages\nOne of the primary reasons ecologists use R is the availability of hundreds of free, user-contributed pieces of software, called packages. Packages are generally created by people who wanted to solve a particular problem for their own research and then realized that other people might find their code useful. Take a moment to browse the packages available on the main R site:\nhttp://cran.r-project.org/web/packages/\nTo install a package, you take its name, put it in quotes, and put it in between the parentheses of install.packages(). For example, to install the package tidyverse (which we will be relying on later), you type:\n\ninstall.packages(\"tidyverse\")\n\nand press Enter (Return, on a Mac). Note that some packages can take quite a while to install. If you are installing tidyverse for instance, it could take anywhere between five minutes to an hour (!) depending on your computer setup. This is normal, and the good news is that you only need to do this once on a computer. Once the package is installed, it will stick around.\nHowever, to actually use a previously installed package in an R session, you need to load it from your disk directly into the computer’s memory. That can be done like this:\n\nlibrary(tidyverse)\n\nNote the lack of quotation marks when loading a package.\nRStudio also makes package management a bit easier. In the Packages panel (top line of lower right portion of the screen) you can see a list of all installed packages. You can also load and unload packages simply by checking a checkbox, and you can install new packages using a graphical interface (although you will still need to know the name of the package you want to install)."
  },
  {
    "objectID": "Intro_R_RStudio.html#additional-reading",
    "href": "Intro_R_RStudio.html#additional-reading",
    "title": "1  Introduction to R and RStudio",
    "section": "1.4 Additional Reading",
    "text": "1.4 Additional Reading\nR:\n\nR website\nCRAN package index\n\nRStudio:\n\nRStudio documentation"
  },
  {
    "objectID": "Intro_R_RStudio.html#exercises",
    "href": "Intro_R_RStudio.html#exercises",
    "title": "1  Introduction to R and RStudio",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\n\nCreate a folder named analysis_of_biological_data on your computer and set it as the working directory in R. Make certain that the working directory has indeed been set properly.\nUse the RStudio file browser to set the working directory somewhere else on your hard drive, and then set it back to the analysis_of_biological_data folder you created earlier. Make sure it is being set properly at each step.\nInstall an R package called vegan, using install.packages as discussed in Section 1.3.4. (The vegan package contains various utilities for community ecology.)\nLoad the vegan package invoking library(vegan). Afterwards, try unloading and then loading the vegan package again, using the Packages panel in RStudio this time."
  },
  {
    "objectID": "R_programming_basics.html#using-r-as-a-calculator",
    "href": "R_programming_basics.html#using-r-as-a-calculator",
    "title": "2  R programming basics",
    "section": "2.1 Using R as a calculator",
    "text": "2.1 Using R as a calculator\nAs we have seen before, R can be used as a glorified pocket calculator. Elementary operations work as expected: + and - are symbols for addition and subtraction, while * and / are multiplication and division. Thus, we can enter things such as\n\n3 * 4 - 6 / 2 + 1\n\nin the console, and press Enter (Return, on a Mac) to get the result, 10. One even has exponentiation, denoted by the symbol ^. To raise 2 to the 5th power), we enter\n\n2^5\n\nand obtain the expected 32. Furthermore, one is not restricted to integers. It is possible to calculate with fractional numbers:\n\n1.62 * 34.56\n\nwhose result is 55.9872. Note: in line with Anglo-Saxon tradition, R uses decimal points instead of decimal commas. Entering 1,62 * 34,56 will throw an error.\nR also has many basic mathematical functions built into it. For example, sqrt() is the square root function; cos() is the cosine function, log() is the (natural) logarithm, exp() is the exponential function, and so on. The following tables summarize the symbols for various arithmetic operations and basic mathematical functions built into R:\n\n\n\nSymbol\nMeaning\nExample\nForm in R\n\n\n\n\n+\naddition\n2 + 3\n2 + 3\n\n\n-\nsubtraction\n5 - 1\n5 - 1\n\n\n*\nmultiplication\n2 · 6\n2 * 6\n\n\n/\ndivision\n9 / 5\n9 / 5\n\n\n^\nraise to power\n32\n3 ^ 2\n\n\n\n\n\n\nFunction\nMeaning\nExample\nForm in R\n\n\n\n\nlog()\nnatural log\nlog(4)\nlog(4)\n\n\nexp()\nexponential\ne4\nexp(4)\n\n\nsqrt()\nsquare root\n√4\nsqrt(4)\n\n\nlog2()\nbase-2 log\nlog2(4)\nlog2(4)\n\n\nlog10()\nbase-10 log\nlog10(4)\nlog10(4)\n\n\nsin()\nsine (radians!)\nsin(4)\nsin(4)\n\n\nabs()\nabsolute value\n|-4|\nabs(-4)\n\n\n\nExpressions built from these basic blocks can be freely combined. Try to calculate 3log(4) - sin(e2) for instance. To do so, we simply type\n\n3^log(4) - sin(exp(2))\n\nand press Enter to get the result, 3.692108. Now obtain e1.3(4 - sin(π/3)). Notice the parentheses enclosing 4 - sin(π/3). This means, as usual, that this expression is evaluated first, before any of the other computations. It can be implemented in R the same way, by using parentheses:\n\nexp(1.3) * (4 - sin(3.14159 / 3))\n\nNote also that you do need to indicate the symbol for multiplication between closing and opening parentheses: omitting this results in an error. Try it: entering exp(1.3)(4 - sin(3.14159/3)) instead of exp(1.3)*(4 - sin(3.14159/3)) throws an error message. Also, be mindful that exp(1.3)*(4 - sin(3.14159/3)) is not the same as exp(1.3)*4 - sin(3.14159/3). This is because multiplication takes precedence over addition and subtraction, meaning that multiplications and divisions are performed first, and additions/subtractions get executed only afterwards—unless, of course, we override this behaviour with parentheses. In general, whenever you are uncertain about the order of execution of operations, it can be useful to explicitly use parentheses, even if it turns out they aren’t really necessary. For instance, you might be uncertain whether 3 * 6 + 2 first multiplies 3 by 6 and then adds 2 to the result, or if it first adds 2 to 6 and then multiplies that by 3. In that case, if you want to be absolutely sure that you perform the multiplication first, just write (3 * 6) + 2, explicitly indicating with the parentheses that the multiplication should be performed first—even though doing so would not be strictly necessary in this case.\nIncidentally, you do not need to type out 3.14159 to approximate π in the mathematical expressions above. R has a built-in constant, pi, that you can use instead. Therefore, exp(1.3)*(4 - sin(pi/3)) produces the same result as our earlier exp(1.3)*(4 - sin(3.14159/3)).\nAnother thing to note is that the number of spaces between various operations is irrelevant. 4*(9-6) is the same as 4*(9 - 6), or 4 * (9 - 6), or, for that matter, 4   * (9-    6). To the machine, they are all the same—it is only us, the human users, who might get confused by that last form…\nIt is possible to get help on any function from the system itself. Type either help(asin) or the shorter ?asin in the console to get information on the function asin, for instance. Whenever you are not sure how to use a certain function, just ask the computer."
  },
  {
    "objectID": "R_programming_basics.html#variables-and-types",
    "href": "R_programming_basics.html#variables-and-types",
    "title": "2  R programming basics",
    "section": "2.2 Variables and types",
    "text": "2.2 Variables and types\n\n2.2.1 Numerical variables and variable names\nYou can assign a value to a named variable, and then whenever you call on that variable, the assigned value will be substituted. For instance, to obtain the square root of 9, you can simply type sqrt(9); or you can assign the value 9 to a variable first:\n\nx <- 9\nsqrt(x)\n\nThis will calculate the square root of x, and since x was defined as 9, we get sqrt(9), or 3.\nThe name for a variable can be almost anything, but a few restrictions apply. First, the name must consist only of letters, numbers, the period (.), and the underscore (_) character. Second, the variable’s name cannot start with a number or an underscore. So one_result or one.result are fine variable names, but 1_result or _one_result are not. Similarly, the name crowns to $ is not valid because of the spaces and the dollar ($) symbol, neither of which are numbers, letters, period, or the underscore.\nAdditionally, there are a few reserved words which have a special meaning in R, and therefore cannot be used as variable names. Examples are: if, NA, TRUE, FALSE, NULL, and function. You can see the complete list by typing ?Reserved.\nHowever, one can override all these rules and give absolutely any name to a variable by enclosing it in backward tick marks (` `). So while crowns to $ and function are not valid variable names, `crowns to $` and `function` are! For instance, you could type\n\n`crowns to $` <- 0.09 # Approximate SEK-to-USD exchange rate\nmy_money <- 123 # Assumed to be given in Swedish crowns\nmy_money_in_USD <- my_money * `crowns to $`\nprint(my_money_in_USD)\n\n[1] 11.07\n\n\nto get our money’s worth in US dollars. Note that the freedom of naming our variables whatever we wish comes at the price of having to always include them between back ticks to refer to them. It is entirely up to you whether you would like to use this feature or avoid it; however, be sure to recognize what it means when looking at R code written by others.\nNotice also that the above chunk of code includes comments, prefaced by the hash (#) symbol. Anything that comes after the hash symbol on a line is ignored by R; it is only there for other humans to read.\n\n\n\n\n\n\nWarning\n\n\n\nThe variable my_money_in_USD above was defined in terms of the two variables my_money and `crowns to $`. You might be wondering: if we change my_money to a different value by executing my_money <- 1000 (say), does my_money_in_USD also get automatically updated? The answer is no: the value of my_money_in_USD will remain unchanged. In other words, variables are not automatically recalculated the way Excel formula cells are. To recompute my_money_in_USD, you will need to execute my_money_in_USD <- my_money * `crowns to $` again. This leads to a recurring theme in programming: while assigning variables is convenient, it also carries some dangers, in case we forget to appropriately update them. In this course, we will be emphasizing a style of programming which avoids relying on (re-)assigning variables as much as possible.\n\n\n\n\n2.2.2 Strings\nSo far we have worked with numerical data. R can also work with textual information. In computer science, these are called character strings, or just strings for short. To assign a string to a variable, one has to enclose the text in quotes. For instance,\n\ns <- \"Hello World!\"\n\nassigns the literal text Hello World! to the variable s. You can print it to screen either by just typing s at the console and pressing Enter, or typing print(s) and pressing Enter.\nOne useful function that works on strings is paste(), which makes a single string out of several ones (in computer lingo, this is known as string concatenation). For example, try\n\ns1 <- \"Hello\"\ns2 <- \"World!\"\nmessage <- paste(s1, s2)\nprint(message)\n\n[1] \"Hello World!\"\n\n\nThe component strings are separated by a space, but this can be changed with the optional sep argument to the paste() function:\n\nmessage <- paste(s1, s2, sep = \"\")\nprint(message)\n\n[1] \"HelloWorld!\"\n\n\nThis results in message becoming HelloWorld!, without the space in between. Between the quotes, you can put any character (including nothing, like above), which will be used as a separator when merging the strings s1 and s2. So specifying sep = \"-\" would have set message equal to Hello-World! (try it out and see how it works).\nIt is important to remember that quotes distinguish information to be treated as text from information to be treated as numbers. Consider the following two variable assignments:\n\na <- 6.7\nb <- \"6.7\"\n\nAlthough they look superficially similar, a is the number 6.7 while b is the string “6.7”, and the two are not equal! For instance, executing 2 * a results in 13.4, but 2 * b throws an error, because it does not make sense to multiply a bunch of text by 2.\n\n\n2.2.3 Logical values\nLet us type the following into the console, and press Enter:\n\n2 > 1\n\n[1] TRUE\n\n\nWe are asking the computer whether 2 is larger than 1. And it returns the answer: TRUE. By contrast, if we ask whether two is less than one, we get FALSE:\n\n2 < 1\n\n[1] FALSE\n\n\nSimilar to “greater than” and “less than”, there are other logical operations as well, such as “greater than or equal to”, “equal to”, “not equal to”, and others. The table below lists the most common options.\n\n\n\nSymbol\nMeaning\nExample in R\nResult\n\n\n\n\n<\nless than\n1 < 2\nTRUE\n\n\n>\ngreater than\n1 > 2\nFALSE\n\n\n<=\nless than or equal\n2 <= 5.3\nTRUE\n\n\n>=\ngreater than or equal\n4.2 >= 3.6\nTRUE\n\n\n==\nequal to\n5 == 6\nFALSE\n\n\n!=\nnot equal to\n5 != 6\nTRUE\n\n\n!\nnot\n!FALSE\nTRUE\n\n\n&\nand\n(1 > 2) & (1 < 2)\nFALSE\n\n\n|\nor\n(1 > 2) | (1 < 2)\nTRUE\n\n\n%in%\nis element of set\n2 %in% c(1, 2, 3)\nTRUE\n\n\n\nThe == and != operators can also be used with strings: \"Hello World\" == \"Hello World!\" returns FALSE, because the two strings are not exactly identical, differing in the final exclamation mark. Similarly, \"Hello World\" != \"Hello World!\" returns TRUE, because it is indeed true that the two strings are unequal.\nLogical values can either be TRUE or FALSE, with no other options.1 This is in contrast with numbers and character strings, which can take on a myriad different values. Note that TRUE and FALSE must be capitalized: true, False, or anything other than the fully capitalized forms will result in an error. Just like in the case of strings and numbers, logical values can be assigned to variables:\n\nlgl <- 3 > 4 # Since 3 > 4 is FALSE, lgl will be assigned FALSE\nprint(!lgl) # lgl is FALSE, so !lgl (\"not lgl\") will be TRUE\n\n[1] TRUE\n\n\nThe function ifelse takes advantage of logical values, doing different things depending on whether some condition is TRUE or FALSE (“if the condition is true then do something, else do some other thing”). It takes three arguments: the first is a condition, the second is the expression that gets executed only if the condition is true, and the third is the expression that executes only if the condition is false. To illustrate its use, we can apply it in a program that simulates a coin toss. R will generate n random numbers between 0 and 1 by invoking runif(n). Here runif is a shorthand for “random-uniform”, randomly generated numbers from a uniform distribution between 0 and 1. The function call runif(1) therefore produces a single random number, and we can interpret values less than 0.5 as having tossed heads, and other values as having tossed tails. The following lines implement this:\n\n\n\n\ntoss <- runif(1)\ncoin <- ifelse(toss < 0.5, \"heads\", \"tails\")\nprint(coin)\n\n[1] \"heads\"\n\n\nThis time we happened to have tossed heads, but try re-running the above three lines over and over again, to see that the results keep coming up at random."
  },
  {
    "objectID": "R_programming_basics.html#vectors",
    "href": "R_programming_basics.html#vectors",
    "title": "2  R programming basics",
    "section": "2.3 Vectors",
    "text": "2.3 Vectors\nA vector is simply a sequence of variables of the same type. That is, the sequence may consist of numbers or strings or logical values, but one cannot intermix them. The c() function will create a vector in the following way:\n\nx <- c(2, 5, 1, 6, 4, 4, 3, 3, 2, 5)\n\nThis is a vector of numbers. If, after entering this line, you type x or print(x) and press Enter, all the values in the vector will appear on screen:\n\nx\n\n [1] 2 5 1 6 4 4 3 3 2 5\n\n\nWhat can you do if you want to display only the third entry? The way to do this is by applying brackets:\n\nx[3]\n\n[1] 1\n\n\nNever forget that vectors and its elements are simply variables! To show this, calculate the value of x[1] * (x[2] + x[3]), but before pressing Enter, guess what the result will be. Then check if you were correct. You can also try typing x * 2:\n\nx * 2\n\n [1]  4 10  2 12  8  8  6  6  4 10\n\n\nWhat happened? Now you performed an operation on the vector as a whole, i.e., you multiplied each element of the vector by two. Remember: you can perform all the elementary operations on vectors as well, and then the result will be obtained by applying the operation on each element separately.\nCertain functions are specific to vectors. Try mean(x) and var(x) for instance (if you are not sure what these do, just ask by typing ?mean or ?var). Some others to try: max, min, length, and sum.\nOne can quickly generate vectors of sequences of values, using one of two ways. First, the notation 1:10 generates a vector of integers ranging from 1 to 10, in steps of 1. (Similarly, 2:7 generates the same vector as c(2, 3, 4, 5, 6, 7), and so on). Second, the function seq() generates sequences, starting with the first argument, ending with the last, in steps defined by an optional by argument. So calling\n\nseq(0, 10, by = 0.1)\n\n  [1]  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.0  1.1  1.2  1.3  1.4\n [16]  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9\n [31]  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9  4.0  4.1  4.2  4.3  4.4\n [46]  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4  5.5  5.6  5.7  5.8  5.9\n [61]  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9  7.0  7.1  7.2  7.3  7.4\n [76]  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4  8.5  8.6  8.7  8.8  8.9\n [91]  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9 10.0\n\n\ncreates a vector of numbers ranging from 0 to 10, in steps of 0.1.\nJust as one can create a vector of numerical values, it is also possible to create a vector of character strings of logical values. For example:\n\nstringVec <- c(\"I am the first string\", \"I am the second\", \"And I am the 3rd\")\n\nNow stringVec[1] is simply equal to the string \"I am the first string\", stringVec[2] is equal to \"I am the second\", and so on. Similarly, defining\n\nlogicVec <- c(TRUE, FALSE, TRUE, TRUE)\n\ngives a vector whose second entry, logicVec[2], is equal to FALSE, and its other three entries are TRUE."
  },
  {
    "objectID": "R_programming_basics.html#functions",
    "href": "R_programming_basics.html#functions",
    "title": "2  R programming basics",
    "section": "2.4 Functions",
    "text": "2.4 Functions\nA function in R can be thought of as a black box which receives inputs and, depending on those inputs, produces some output. Vending machines provide a good working model of what a “function” is in computer science: depending on the inputs they receive (in the form of coins of various denomination, plus the buttons you press for a particular item) they give you some output (Mars bars, Coke, and the like). It’s just that computer scientists like to refer to the inputs as “function arguments” or simply “arguments” instead of coins, and to the output as the “return value” instead of Red Bull. Arguments are often also referred to as “parameters” to the function.\nWe have already seen some functions at work in R: sqrt and log are functions, but so are setwd (which, as you may recall, will set your working directory) and library (which loads R packages). The general workings of a function are illustrated below:\n                 --------------\nargument 1 ----> |            |\nargument 2 ----> |            |\nargument 3 ----> |  FUNCTION  | ----> return value\n...              |            |\nargument n ----> |            |\n                 --------------\nWhen you ask a function to do something, you’re calling the function. The arguments of functions are always enclosed in parentheses. For example, executing sqrt(9), calls the built-in square root function. Its argument (or input, or parameter) is 9, and its return value is the square root of 9, which is 3.\n\n2.4.1 User-defined functions\nThus far, we have been using many built-in functions in R, such as exp(), log(), sqrt(), setwd(), and others. However, it is also possible to define our own functions, which can then be used just like any built-in function. The way to do this is to use the function keyword, followed by the function’s arguments in parentheses, and then the R code comprising the function’s body enclosed in curly braces {}. For example, here is a function which calculates the area of a circle with radius r:\n\ncircleArea <- function(r) {\n  area <- r^2 * pi\n  return(area)\n}\n\nThe function implements the formula that the area of a circle is equal to π times its radius squared. The return keyword determines what result the function will output when it finishes executing. In this case, the function returns the value of area that is created within the function. After running the above lines, the computer now “knows” the function. Calling circleArea(3) will, for example, calculate the area of a circle with radius 3, which is approximately 28.27433.\nOne can define functions with more than one argument. For instance, here is a function that calculates the volume of a cylinder with radius r and height h:\n\ncylinderVol <- function(r, h) {\n  baseArea <- circleArea(r)\n  volume <- baseArea * h\n  return(volume)\n}\n\nHere we used the fact that the volume of a cylinder is the area of its base circle, times its height. Notice also that we made use of our earlier circleArea function within the body of cylinderVol. While this was not a necessity and we could have simply written volume <- r^2 * pi * h above, this is generally speaking good practice: by constructing functions to solve smaller problems, you can write slightly more complicated functions which make use of those simpler ones. Then, you will be able to write even more complex functions using the slightly more complex ones in turn—and so on. We will discuss this principle in more detail below, in Section 2.4.3.\nOne very important property of functions is that any variables defined within them (such as volume above) are local to that function. This means that they are not visible from outside: even after calling the function, the variable volume will not be accessible to the rest of the program, despite the fact that it was declared in the function. This helps us create programs with modular structure, where functions operate as black boxes: we can use them without looking inside.\nWhen calling a function, it is optional but possible to name the arguments explicitly. This means that calling circleArea(3) is the same as calling circleArea(r = 3), and calling cylinderVol(2, 3) is the same as calling cylinderVol(r = 2, h = 3). Even more is true: since naming the arguments removes any ambiguity about which argument is which, one may even call cylinderVol(h = 3, r = 2), with the arguments in reverse order, and this will still be equivalent to cylinderVol(2, 3). As mentioned, naming arguments this way is optional, but it can be useful to do so, because it can increase the clarity of our programs. To give an example from a built-in function in R, take rep(5, 3). Does this function create a vector with 5 entries, each equal to 3, or does it make a vector with 3 entries, each equal to 5? While reading the documentation (or simply executing these two function calls and comparing the outputs) reveals that it is the latter, one can clarify this easily, because the second argument of rep() is called times, as seen from reading the help after typing ?rep. We can then write rep(5, times = 3), which is now easy to interpret: it is a vector with the number 5 repeated 3 times.\n\nrep(5, times = 3)\n\n[1] 5 5 5\n\n\nOne may even define default values for one or more of the arguments to any function. If defaults are given, the user does not even have to specify the value for that argument. It will then automatically be set to the default value instead. For example, one could rewrite the cylinderVol() function to specify default values for r and h. Making these defaults be 1 means we can write:\n\ncylinderVol <- function(r = 1, h = 1) {\n  baseArea <- circleArea(r)\n  volume <- baseArea * h\n  return(volume)\n}\n\nIf we now call cylinderVol() without specifying arguments, the defaults will be substituted for r and h. Since both are equal to 1, the cylinder volume will simply be π (about 3.14159), which is the result we will get back. Alternatively, if we call cylinderVol(r = 2), then the function returns 4π (approximately 12.56637), because the default value of 1 is substituted in place of the unspecified height argument h. Importantly, if we don’t define default values and yet omit to specify one or more of those parameters, we get back an error message. For example, our earlier circleArea function had no default value for its argument r, so leaving it unspecified throws an error:\n\ncircleArea()\n\nError in circleArea(): argument \"r\" is missing, with no default\n\n\n\n\n2.4.2 Naming rules for functions and the concept of syntactic sugar\nThe rules for naming functions is the same as for naming variables. A valid function name is a combination of letters, numbers, and underscores (_), as long as the first character is not a number or underscore. Additionally, a function’s name cannot be one of the reserved words (see ?Reserved). Just like in the case of variables, one can override this and give any name whatsoever to functions if one encloses the name between back ticks. So while crowns to $ is not a valid function name, `crowns to $` is.\nOne thing to know about R is that even elementary operations are treated as function calls internally. When we write down even something as innocuous as 2 + 5, what really happens is that R calls the function called +, with arguments 2 and 5. In fact, we can write it that way too: 2 + 5 is completely equivalent to writing `+`(2, 5). Note the back ticks around `+`: these are required because + is not a letter, number, or underscore. Whenever we write down 2 + 5, the system internally converts it into `+`(2, 5) first, and then proceeds with the execution. Thus, the fact that we can add two numbers by writing 2 + 5 is just a convenience, a way of entering addition in a way that we tend to be more used to. Such constructions have a name in computer science: they are called syntactic sugar. Writing 2 + 5 is just syntactic sugar for the actual internal form `+`(2, 5), because the latter would be stranger to write. Of course, the same holds for all other elementary operations: `-`, `*`, `/`, and `^` are also functions in R. This means that, e.g., writing `-`(`^`(2, 3), `*`(4, 2)) is equivalent to 2^3 - 4 * 2.\nAnother example for the fact that internally R treats operations as functions is the subsetting of vectors or matrices. As we have learned, given the vector x, typing x[3] will extract the third entry of the vector. In fact, this is again syntactic sugar for easier use. Internally, an expression such as x[3] is actually interpreted as `[`(x, 3). The function `[` (note the back ticks, which are necessary due to the fact that the symbol [ is not a letter, number, or underscore) takes two arguments: a vector, and the index (or indices) which we request from that vector.\nWhile generally speaking, one would never actually want to type `[`(x, 3) instead of x[3] (the reason we have the syntactic sugar is to make our lives easier!), there are situations where being aware of these details of the internal workings of R can be helpful. We will see an example later in this chapter.\n\n\n2.4.3 Function composition\nA function is like a vending machine: we give it some input(s), and it produces some output. The output itself may then be fed as input to another function—which in turn produces an output, which can be fed to yet another function, and so on. Chaining functions together in this manner is called the composition of functions. For example, we might need to take the square root of a number, then calculate the logarithm of the output, and finally, obtain the cosine of the result. This is as simple as writing cos(log(sqrt(9))), if the number we start with is 9. More generally, one might even define a new function (let us call it cls(), after the starting letters of cos, log, and sqrt) like this:\n\ncls <- function(x) {\n  return(cos(log(sqrt(x))))\n}\n\nA remarkable property of composition is that the composed function (in this case, cls) is in many ways just like its constituents: it is also a black box which takes a single number as input and produces another number as its output. Putting it differently, if one did not know that the function cls() was defined by me manually as the composition of three more “elementary” functions, and instead claimed it was just another elementary built-in function in R, there would be no way to tell the difference just based on the behaviour of the function itself. The composition of functions thus has the important property of self-similarity: if we manage to solve a problem through the composition of functions, then that solution itself will behave like an “elementary” function, and so can be used to solve even more complex problems via composition—and so on.\nIf we conceive of a program written in R as a large lego building, then one can think of functions as the lego blocks out of which the whole construction is made. Lego pieces are designed to fit well together, one can always combine them in various ways. Furthermore, any combination of lego pieces itself behaves like a more elementary lego piece: it can be fitted together with other pieces in much the same way. Thus, the composition of functions is analogous to building larger lego blocks out of simpler ones. Remarkably, just as the size of a lego block does not hamper our ability to stick them together, the composability of functions is retained regardless of how many more elementary pieces each of them consist of. Thus, the composition of functions is an excellent way (some claim it is the way) to handle the complexity of large software systems.\n\n\n2.4.4 Function piping\nOne problem with composing many functions together is that the order of application must be read backwards. An expression such as sqrt(sin(cos(log(1)))) means: “take the square root of the sine of the cosine of the natural logarithm of 1”. But it is more convenient for the human brain to think of it the other way round: we first take the log of 1, then the cosine of the result, then the sine of what we got, and finally the square root. The problem of interpreting composed functions gets more difficult when the functions have more than one argument. Even something as relatively simple as\n\nexp(mean(log(seq(-3, 11, by = 2)), na.rm = TRUE))\n\n[1] 4.671655\n\n\nmay cause one to stop and have to think about what this expression actually does—and it only involves the composition of four simple functions. One can imagine the difficulties of having to parse the composition of dozens of functions in this style.\nThe above piece of R code generates the numeric sequence -3, -1, 1, …, 11 (jumping in steps of 2), and computes their geometric mean. To do so, it takes the logarithms of each value, takes their mean, and finally, exponentiates the result back. The problem is that the logarithm of negative numbers does not exist (more precisely, they are not real numbers), and therefore, log(-3) and log(-1) both produce undefined results. Thus, when taking the mean of the logarithms, we must remove any such undefined values. This can be accomplished via an extra argument to mean, called na.rm (“NA-remove”). By default, this is set to FALSE, but by changing it to TRUE, undefined values are simply ignored when computing the mean. For example mean(c(1, 2, 3, NA)) returns NA, because of the undefined entry in the vector; but mean(c(1, 2, 3, NA), na.rm = TRUE) returns 2, the result one gets after discarding the NA entry.\nAll the above is difficult to see when looking at the expression\n\nexp(mean(log(seq(-3, 11, by = 2)), na.rm = TRUE))\n\nPart of the reason is the awkward “backwards order” of function applications, and that it is hard to see which function the argument na.rm = TRUE belongs to. Fortunately, there is a simple operator in R called a pipe (written %>%), which allows one to write the same code in a more streamlined way. The pipe was originally provided by the magrittr package,2 but invoking tidyverse will also load it automatically:\n\nlibrary(tidyverse)\n\nThe pipe allows one to write function application in reverse order (first the argument and then the function), making the code more transparent. Formally, x %>% f() is equivalent to f(x) for any function f. For example, sqrt(9) can also be written 9 %>% sqrt(). Thus, sqrt(sin(cos(log(1)))) can be written as 1 %>% log() %>% cos %>% sin() %>% sqrt(), which reads straightforwardly as “start with the number 1; then take its log; then take the cosine of the result; then take the sine of that result; and then, finally, take the square root to obtain the final output”. In general, it helps to pronounce %>% as “then”.\nThe pipe also works for functions with multiple arguments. In that case, x %>% f(y, ...) is equivalent to f(x, y, ...). That is, the pipe refers to the function’s first argument (though it is possible to override this). Instead of mean(log(seq(-3, 11, by = 2)), na.rm = TRUE), we can therefore write:\n\nseq(-3, 11, by = 2) %>%\n  log() %>%\n  mean(na.rm = TRUE) %>%\n  exp()\n\n[1] 4.671655\n\n\nThis is fully equivalent to the traditional form, but is much more readable, because the functions are written in the order in which they actually get applied. Moreover, even though the program is built only from the composition of functions, it reads straightforwardly as if it was a sequence of imperative instructions: we start from the vector of integers c(-3, -1, 1, 3, 5, 7, 9, 11); then we take the logarithm of each; then we take their average, discarding any invalid entries (produced in this case by taking the logarithm of negative numbers); and then, finally, we exponentiate back the result to obtain the geometric mean."
  },
  {
    "objectID": "R_programming_basics.html#exercises",
    "href": "R_programming_basics.html#exercises",
    "title": "2  R programming basics",
    "section": "2.5 Exercises",
    "text": "2.5 Exercises\n\nWhich of the variable names below are valid, and why?\n\nfirst.result.of_computation\n2nd.result.of_computation\ndsaqwerty\ndsaq werty\n`dsaq werty`\nbreak\nis this valid?...\n`is this valid?...`\nis_this_valid?...\n\nCreate a vector called z, with entries 1.2, 5, 3, 13.7, 6.66, and 4.2 (in that order). Then, by applying functions to this vector, obtain:\n\nIts smallest entry.\nIts largest entry.\nThe sum of all its entries.\nThe number of entries in the vector.\nThe vector’s entries sorted in increasing order (Hint: look up the help for the built-in function sort).\nThe vector’s entries sorted in decreasing order.\nThe product of the fourth entry with the difference of the third and sixth entries. Then take the absolute value of the result.\n\nDefine a vector of strings, called s, with the three entries \"the fat cat\", \"sat on\", and \"the mat\".\n\nCombine these three strings into a single string, and print it on the screen. (Hint: look up the help for the paste function, in particular its collapse argument.)\nReverse the entries of s, so they come in the order \"the mat\", \"sat on\", and \"the fat cat\". (Hint: check out the rev function.) Then merge the three strings again into a single one, and print it on the screen.\n\nAssume you have a population of some organism in which one given allele of some gene is the only one available in the gene pool. If a new mutant organism with a different, selectively advantageous allele appears, it would be reasonable to conclude that the new allele will fix in the population and eliminate the original one over time. This, however, is not necessarily true, because a very rare allele might succumb to being eliminated by chance, regardless of how advantageous it is. According to Motoo Kimura’s famous formula, the probability of such a new allele eventually fixing in the population is given as: \\[ P = \\frac{1 - \\text{e}^{-s}}{1 - \\text{e}^{-2Ns}} \\] (Gillespie 2004). Here P is the probability of eventual fixation, s is the selection differential (the degree to which the new allele is advantageous over the original one), and N is the (effective) population size.\n\nWrite a function that implements this formula. It should take the selection differential s and the population size N as parameters, and return the fixation probability as its result.\nA selection differential of 0.5 is very strong (though not unheard of). What is the likelihood that an allele with that level of advantage will fix in a population of 1000 individuals? Interpret the result.\n\nA text is palindromic if it reads backwards the same as it reads forwards. For example, “racecar”, “deified”, and “rotator” are all palindromic words. Assume that you are given a word in all lowercase, broken up by characters. For instance, you could be given the vector c(\"r\", \"a\", \"c\", \"e\", \"c\", \"a\", \"r\") (a palindrome) or c(\"h\", \"e\", \"l\", \"l\", \"o\") (not a palindrome).\n\nWrite a function which checks whether the vector encodes a palindromic text. The function should return TRUE if the text is a palindrome, and FALSE otherwise. (Hint: reverse the text, collapse both the original and the reversed vectors into single strings, and then compare them using logical equality.)\nModify the function to allow for both upper- and lowercase text, treating case as irrelevant (i.e., \"A\" is treated to be equal to \"a\" when evaluating whether the text is palindromic). One simple way to do this is to convert each character of the text into uppercase (or lowercase; it doesn’t matter which), and use this standardized text for reversing and comparing with. Look up the functions toupper and tolower, and implement this improvement in your palindrome checker function.\nIf you haven’t done so already: try to rewrite your function to rely as much on function composition as possible.\n\n\n\n\n\n\nGillespie, John H. 2004. Population Genetics: A Concise Guide. Baltimore, MD, USA: Johns Hopkins University Press."
  },
  {
    "objectID": "Data_reading.html#the-tidyverse-package-suite",
    "href": "Data_reading.html#the-tidyverse-package-suite",
    "title": "3  Reading tabular data from disk",
    "section": "3.1 The tidyverse package suite",
    "text": "3.1 The tidyverse package suite\nA suite of R packages, sharing the same design philosophy, are collected under the name tidyverse. In case this is not yet installed on your computer, type\n\ninstall.packages(\"tidyverse\")\n\nat the R console and press Enter. After making sure that the package is installed, you must load it. This is done via the function call\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──\n\n\n✔ ggplot2 3.4.0      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nAs you see, eight packages are now loaded, called ggplot2, tibble, and so on. We will get to know these in more detail throughout the course.\nThere are actually even more packages that are part of the tidyverse. Typing and executing tidyverse_packages() will show all such packages. Of all these options, only eight are loaded by default when invoking library(tidyverse). The others must be loaded separately. For example, readxl is a tidyverse package for loading Excel files in R. To use it, execute library(readxl).\nIn general, it is a good idea to load all necessary packages at the top of your R script. There are two reasons for this. First, once you close RStudio, it forgets the packages, which do not get automatically reloaded after reopening RStudio. Second, often other users will run the scripts you write on their own computers, and they will not be able to do so unless the proper packages are loaded first. It is then helpful to others if the necessary packages are all listed right at the top, showing what is needed to run your program."
  },
  {
    "objectID": "Data_reading.html#reading-tabular-data",
    "href": "Data_reading.html#reading-tabular-data",
    "title": "3  Reading tabular data from disk",
    "section": "3.2 Reading tabular data",
    "text": "3.2 Reading tabular data\nOne of the packages loaded by default with tidyverse is called readr. This package contains tools for loading data files, and writing them to disk. To see how it works, download the files Goldberg2010_data.csv, Goldberg2010_data.xlsx, and Smith2003_data.txt from Lisam. Then set the working directory in RStudio to the folder where you have saved them (as a reminder, you can do this by executing setwd(/path/to/files), where you should substitute in your own path in place of /path/to/files).\n\n3.2.1 The CSV file format\nGoldberg et al. (2010) collected data on self-incompatibility in the family Solanaceae (nightshades). It contains a list of 356 species, along with a flag determining self-incompatibility status (0: self-incompatible; 1: self-compatible; 2-5: more complicated selfing scenarios). The data are in the file Goldberg2010_data.csv. This is a so-called comma-separated value (CSV) file, meaning that the different columns of the data are separated by commas. One can see this by viewing the file in any simple text editor. For example, this can be done in RStudio itself, by clicking on the file in the Files panel in the lower right part of the RStudio window, and then choosing the option “View file” (ignore the other option called “Import dataset…”). Having done this, a new tab opens in your editor panel (upper left region) where you should see something like the following:\nSpecies,Status\nAcnistus_arborescens,1\nAnisodus_tanguticus,1\nAtropa_belladonna,1\nBrachistus_stramonifolius,1\nBrugmansia_aurea,0\nBrugmansia_sanguinea,0\nCapsicum_annuum,1\nCapsicum_baccatum,1\nCapsicum_cardenasii,2\nCapsicum_chacoense,1\nAnd so on. As you can see, the first line (Species,Status) is actually an indicator of what the corresponding columns of data will contain: the first column has the species name, and the second one the numerical flag indicating self-compatibility status. The subsequent rows hold the actual data. Notice that the boundary between the columns is always indicated by a comma. This is what gave rise to the name “comma-separated value” (CSV) file.\nThe above raw format is not yet amenable to processing within R. To make it so, we first need to import the data. For comma-separated value files, there is a convenient function, read_csv, that makes this especially simple:1\n\nread_csv(\"Goldberg2010_data.csv\")\n\n# A tibble: 356 × 2\n   Species                   Status\n   <chr>                      <dbl>\n 1 Acnistus_arborescens           1\n 2 Anisodus_tanguticus            1\n 3 Atropa_belladonna              1\n 4 Brachistus_stramonifolius      1\n 5 Brugmansia_aurea               0\n 6 Brugmansia_sanguinea           0\n 7 Capsicum_annuum                1\n 8 Capsicum_baccatum              1\n 9 Capsicum_cardenasii            2\n10 Capsicum_chacoense             1\n# … with 346 more rows\n\n\n(We will interpret the output in the next subsection.) The above line loads the data, but does not save it into a variable. That is perfectly fine in case we immediately start performing operations on it via function composition (we will see many, many examples later on). However, in case we do want to assign the result to a variable, we can do so without problems. For instance, to put the table into the variable dat, we simply write:\n\ndat <- read_csv(\"Goldberg2010_data.csv\")\n\n\n\n3.2.2 The tibble data structure\nLook at the output produced by read_csv(\"Goldberg2010_data.csv\") above. You can mostly ignore the top part of that output—it simply provides information on how it interpreted the data it just read in. Instead, the interesting part starts with A tibble: 356 x 2. A tibble (or data frame2) is the R-equivalent of an Excel-style spreadsheets. In this case, it has 356 rows and 2 columns (hence the 356 x 2). The simplest way to conceive of a tibble is as a collection of vectors, glued together side-by-side to form a table of data. Importantly, although each vector must consist of entries of the same type, as usual (e.g., they can be vectors of numbers, vectors of strings, or vectors of logical values), the different columns need not share types. For example, in the above table, the first column consists of character strings, but the second one consists of numerical values. This can be seen right below the header information. Below Species, you can see <chr>, which stands for “character string”. Below Status, we have <dbl> which, confusing as it may look at first sight, refers simply to ordinary numbers.3 In turn, columns comprising of logical values would have the tag <lgl> underneath them (in this case though, we don’t have such a column). The point is that by looking at the type information below the header, you can see how R has interpreted each of the columns at a glance.\nThe fact that the individual columns are simply vectors can be made explicit, by relying on the $-notation. To access a given column of the table as a vector, we write the name of the table, followed by the $ symbol, followed by the name of the column in question. For example, we can access the Status column from the dat table as a vector of numbers like this:\n\ndat$Status\n\n  [1] 1 1 1 1 0 0 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 0 0 1 1 1 1 0 4 0 0 0 1 1 1 0\n [38] 0 0 0 0 1 1 1 1 1 1 0 0 0 0 4 0 3 0 0 4 0 4 4 0 4 1 0 0 0 4 4 4 0 1 1 0 1\n [75] 1 1 1 0 0 1 1 1 1 1 1 1 0 1 2 1 1 1 1 1 1 2 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1\n[112] 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 2 0 0 2 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0\n[149] 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 4 1 1 0 0 1 0 0 0 1 1 1 1 1 0 4 0 4 0 1 2 0\n[186] 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 4 4 1 1 1 4 0 0 0 0 1 1 0 1 1 0 0 1 2 1 1 0\n[223] 1 2 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 4 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0\n[260] 1 0 1 1 2 1 1 1 1 0 0 0 1 2 1 0 0 1 1 4 1 0 2 0 4 1 1 0 0 0 1 2 4 1 1 1 1\n[297] 1 1 1 1 1 1 1 0 2 0 1 1 0 2 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 2 4\n[334] 0 4 0 1 1 1 1 1 0 1 1 5 4 4 1 4 1 0 0 0 0 0 2\n\n\nHere dat$Status is really just a vector, and can be treated as such. For example, to get the 9th entry of this vector, we can use the usual bracket notation:\n\ndat$Status[9]\n\n[1] 2\n\n\nThe result is an ordinary numerical value.\nFinally, let us take one more look at the output again:\n\nprint(dat)\n\n# A tibble: 356 × 2\n   Species                   Status\n   <chr>                      <dbl>\n 1 Acnistus_arborescens           1\n 2 Anisodus_tanguticus            1\n 3 Atropa_belladonna              1\n 4 Brachistus_stramonifolius      1\n 5 Brugmansia_aurea               0\n 6 Brugmansia_sanguinea           0\n 7 Capsicum_annuum                1\n 8 Capsicum_baccatum              1\n 9 Capsicum_cardenasii            2\n10 Capsicum_chacoense             1\n# … with 346 more rows\n\n\nWhen displaying large tibbles, R will not dump all the data at you. Instead, it will display the first 10 rows, with a message indicating how many more rows remain (in our case, we have ...with 346 more rows written at the end of the printout). The system is still aware of the other rows; it just does not show them. To get a full view of a tibble in a more digestible, spreadsheet-like style, one can use the view function. Try running view(dat) and see what happens!\n\n\n3.2.3 The TSV file format\nAnother type of file is one where the columns are separated by tabulators instead of commas. These are called tab-separated value (TSV) files. An example is provided by the file associated with data from Smith et al. (2003). The authors compiled a database of the body mass of mammals of the late Quaternary period. The data file is Smith2003_data.txt. Its rows are the different mammal species; its columns are: the species’ native continent; whether the species is still alive or extinct; the order, family, genus, and species names; the base-10 log body mass; the actual body mass (in grams); and numbered references representing research papers which served as the source of the data.\nViewing the TSV file Smith2003_data.txt in its raw form begins something like the following:\nAF  extant  Artiodactyla    Bovidae Addax   nasomaculatus   4.85    70000.3 60\nAF  extant  Artiodactyla    Bovidae Aepyceros   melampus    4.72    52500.1 \"63, 70\"\nAF  extant  Artiodactyla    Bovidae Alcelaphus  buselaphus  5.23    171001.5    \"63, 70\"\nAF  extant  Artiodactyla    Bovidae Ammodorcas  clarkei 4.45    28049.8 60\nAF  extant  Artiodactyla    Bovidae Ammotragus  lervia  4.68    48000   75\nAF  extant  Artiodactyla    Bovidae Antidorcas  marsupialis 4.59    39049.9 60\nAF  extinct Artiodactyla    Bovidae Antidorcas  bondi   4.53    34000   1\nAF  extinct Artiodactyla    Bovidae Antidorcas  australis   4.6 40000   2\nAF  extant  Artiodactyla    Bovidae Bos taurus  5.95    900000  -999\nAF  extant  Artiodactyla    Bovidae Capra   walie   5   100000  -999\nSince the file is tab- and not comma-separated, trying to load it using read_csv will not work correctly:\n\nread_csv(\"Smith2003_data.txt\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n\n\nRows: 5730 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): AF extant  Artiodactyla    Bovidae Addax   nasomaculatus   4.85    70000.3 60\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 5,730 × 1\n   `AF\\textant\\tArtiodactyla\\tBovidae\\tAddax\\tnasomaculatus\\t4.85\\t70000.3\\t60` \n   <chr>                                                                        \n 1 \"AF\\textant\\tArtiodactyla\\tBovidae\\tAepyceros\\tmelampus\\t4.72\\t52500.1\\t\\\"63…\n 2 \"AF\\textant\\tArtiodactyla\\tBovidae\\tAlcelaphus\\tbuselaphus\\t5.23\\t171001.5\\t…\n 3 \"AF\\textant\\tArtiodactyla\\tBovidae\\tAmmodorcas\\tclarkei\\t4.45\\t28049.8\\t60\"  \n 4 \"AF\\textant\\tArtiodactyla\\tBovidae\\tAmmotragus\\tlervia\\t4.68\\t48000\\t75\"     \n 5 \"AF\\textant\\tArtiodactyla\\tBovidae\\tAntidorcas\\tmarsupialis\\t4.59\\t39049.9\\t…\n 6 \"AF\\textinct\\tArtiodactyla\\tBovidae\\tAntidorcas\\tbondi\\t4.53\\t34000\\t1\"      \n 7 \"AF\\textinct\\tArtiodactyla\\tBovidae\\tAntidorcas\\taustralis\\t4.6\\t40000\\t2\"   \n 8 \"AF\\textant\\tArtiodactyla\\tBovidae\\tBos\\ttaurus\\t5.95\\t900000\\t-999\"         \n 9 \"AF\\textant\\tArtiodactyla\\tBovidae\\tCapra\\twalie\\t5\\t100000\\t-999\"           \n10 \"AF\\textant\\tArtiodactyla\\tBovidae\\tCapra\\tibex\\t5\\t100999.7\\t65\"            \n# … with 5,720 more rows\n\n\nAs you can see, there is even a warning at the top about “One or more parsing issues”, meaning that read_csv had a hard time reading in the file. Below the message, you can also see that the attempted read is a mess, with all data in the rows treated as being part of a single column.\nInstead, to correctly read TSV files, one should use read_tsv:\n\nread_tsv(\"Smith2003_data.txt\")\n\n# A tibble: 5,730 × 9\n   AF    extant  Artiodactyla Bovidae Addax      nasomacu…¹ `4.85` 70000…² `60` \n   <chr> <chr>   <chr>        <chr>   <chr>      <chr>       <dbl>   <dbl> <chr>\n 1 AF    extant  Artiodactyla Bovidae Aepyceros  melampus     4.72  52500. 63, …\n 2 AF    extant  Artiodactyla Bovidae Alcelaphus buselaphus   5.23 171002. 63, …\n 3 AF    extant  Artiodactyla Bovidae Ammodorcas clarkei      4.45  28050. 60   \n 4 AF    extant  Artiodactyla Bovidae Ammotragus lervia       4.68  48000  75   \n 5 AF    extant  Artiodactyla Bovidae Antidorcas marsupial…   4.59  39050. 60   \n 6 AF    extinct Artiodactyla Bovidae Antidorcas bondi        4.53  34000  1    \n 7 AF    extinct Artiodactyla Bovidae Antidorcas australis    4.6   40000  2    \n 8 AF    extant  Artiodactyla Bovidae Bos        taurus       5.95 900000  -999 \n 9 AF    extant  Artiodactyla Bovidae Capra      walie        5    100000  -999 \n10 AF    extant  Artiodactyla Bovidae Capra      ibex         5    101000. 65   \n# … with 5,720 more rows, and abbreviated variable names ¹​nasomaculatus,\n#   ²​`70000.3`\n\n\nThis is now a neatly formatted table, with 9 columns as needed.\nThere is a problem though. This file does not contain a header—a row, which is the first in a data file, specifying the names of the various columns. (Recall that the first row of Goldberg2010_data.csv contained not data, but the names of the columns.) Instead, the first row is itself part of the data. To override the default behavior of treating the first row as one of column names, one can use the col_names = FALSE option:\n\nread_tsv(\"Smith2003_data.txt\", col_names = FALSE)\n\n# A tibble: 5,731 × 9\n   X1    X2      X3           X4      X5         X6              X7     X8 X9   \n   <chr> <chr>   <chr>        <chr>   <chr>      <chr>        <dbl>  <dbl> <chr>\n 1 AF    extant  Artiodactyla Bovidae Addax      nasomaculat…  4.85 7.00e4 60   \n 2 AF    extant  Artiodactyla Bovidae Aepyceros  melampus      4.72 5.25e4 63, …\n 3 AF    extant  Artiodactyla Bovidae Alcelaphus buselaphus    5.23 1.71e5 63, …\n 4 AF    extant  Artiodactyla Bovidae Ammodorcas clarkei       4.45 2.80e4 60   \n 5 AF    extant  Artiodactyla Bovidae Ammotragus lervia        4.68 4.8 e4 75   \n 6 AF    extant  Artiodactyla Bovidae Antidorcas marsupialis   4.59 3.90e4 60   \n 7 AF    extinct Artiodactyla Bovidae Antidorcas bondi         4.53 3.4 e4 1    \n 8 AF    extinct Artiodactyla Bovidae Antidorcas australis     4.6  4   e4 2    \n 9 AF    extant  Artiodactyla Bovidae Bos        taurus        5.95 9   e5 -999 \n10 AF    extant  Artiodactyla Bovidae Capra      walie         5    1   e5 -999 \n# … with 5,721 more rows\n\n\nThe col_names argument is set by default to TRUE; in case we wish to override this, we must explicitly change is, just like above.\n\n\n3.2.4 Renaming columns\nWhile the above works, the column names now default to the moderately informative labels X1, X2, and so on. Fortunately, columns can be renamed using the rename function from the tidyverse. This function takes a tibble as its first argument, and a renaming instruction as its second, of the form new_name = old_name. For example, to rename the first column (which, as you may remember, refers to the continent of the corresponding mammal):\n\nsmithData <- read_tsv(\"Smith2003_data.txt\", col_names = FALSE)\nrename(smithData, Continent = X1) # Rename column \"X1\" to \"Continent\"\n\n# A tibble: 5,731 × 9\n   Continent X2      X3           X4      X5         X6          X7     X8 X9   \n   <chr>     <chr>   <chr>        <chr>   <chr>      <chr>    <dbl>  <dbl> <chr>\n 1 AF        extant  Artiodactyla Bovidae Addax      nasomac…  4.85 7.00e4 60   \n 2 AF        extant  Artiodactyla Bovidae Aepyceros  melampus  4.72 5.25e4 63, …\n 3 AF        extant  Artiodactyla Bovidae Alcelaphus buselap…  5.23 1.71e5 63, …\n 4 AF        extant  Artiodactyla Bovidae Ammodorcas clarkei   4.45 2.80e4 60   \n 5 AF        extant  Artiodactyla Bovidae Ammotragus lervia    4.68 4.8 e4 75   \n 6 AF        extant  Artiodactyla Bovidae Antidorcas marsupi…  4.59 3.90e4 60   \n 7 AF        extinct Artiodactyla Bovidae Antidorcas bondi     4.53 3.4 e4 1    \n 8 AF        extinct Artiodactyla Bovidae Antidorcas austral…  4.6  4   e4 2    \n 9 AF        extant  Artiodactyla Bovidae Bos        taurus    5.95 9   e5 -999 \n10 AF        extant  Artiodactyla Bovidae Capra      walie     5    1   e5 -999 \n# … with 5,721 more rows\n\n\nAs seen, the name of the first column now reads Continent instead of X1. One can similarly rename other columns as well.\n\n\n3.2.5 Excel tables\nFinally, although their use is discouraged in science, one should know how to read in data from an Excel spreadsheet. To do this, one needs to load the readxl package. This package is part of the tidyverse, but does not get automatically loaded when executing library(tidyverse). Therefore, we first load the package:\n\nlibrary(readxl)\n\nWe can now load Excel files with the function read_excel(). At the start, we downloaded an Excel version of the data from Goldberg et al. (2010), called Goldberg2010_data.xlsx. It holds the exact same data as the original CSV file, just saved in Excel format for instructive purposes. Let us load this file:\n\nread_excel(\"Goldberg2010_data.xlsx\")\n\n# A tibble: 356 × 2\n   Species                   Status\n   <chr>                      <dbl>\n 1 Acnistus_arborescens           1\n 2 Anisodus_tanguticus            1\n 3 Atropa_belladonna              1\n 4 Brachistus_stramonifolius      1\n 5 Brugmansia_aurea               0\n 6 Brugmansia_sanguinea           0\n 7 Capsicum_annuum                1\n 8 Capsicum_baccatum              1\n 9 Capsicum_cardenasii            2\n10 Capsicum_chacoense             1\n# … with 346 more rows\n\n\nThe functions read_csv, read_tsv, and read_excel have several further options. For example, given an Excel table with multiple sheets, one can specify which one to import, using the sheet argument. Check the help pages of these functions, and experiment with their options.\n\n\n3.2.6 Writing data to files\nFinally, data can not only be read from a file, but also written out to one. Then, instead of read_csv, read_tsv and the like, one uses write_csv, write_tsv, and so on. For instance, to save dat in CSV form:\n\nwrite_csv(dat, \"/path/to/file.csv\")\n\nwhere /path/to/file.csv should be replaced by the path and file name with which the data should be saved."
  },
  {
    "objectID": "Data_reading.html#exercises",
    "href": "Data_reading.html#exercises",
    "title": "3  Reading tabular data from disk",
    "section": "3.3 Exercises",
    "text": "3.3 Exercises\n\nLoad the data from the file Smith2003_data.txt. Note that the file is tab-separated and lacks headers!\nThe columns of this file are, in order: Continent (AF=Africa, etc.), Status (extinct, historical, introduction, or extant), Order, Family, Genus, Species, Base-10 Log Mass, Combined Mass (grams), and Reference (numbers, referring to a numerically ordered list of published works – no need to worry about the details). Rename each column appropriately, using the rename() function.\n\n\n\n\n\nGoldberg, Emma E., Joshua R. Kohn, Russell Lande, Kelly A. Robertson, Stephen A. Smith, and Boris Igić. 2010. “Species Selection Maintains Self-Incompatibility.” Science 330 (6003): 493–95. https://doi.org/10.1126/science.1194513.\n\n\nSmith, Felisa A., S. Kathleen Lyons, S. K. Morgan Ernest, Kate E. Jones, Dawn M. Kaufman, Tamar Dayan, Pablo A. Marquet, James H. Brown, and John P. Haskell. 2003. “Body Mass of Late Quaternary Mammals.” Ecology 84 (12): 3403. https://doi.org/10.1890/02-9003."
  },
  {
    "objectID": "Basic_data_wrangling.html#selecting-and-manipulating-data",
    "href": "Basic_data_wrangling.html#selecting-and-manipulating-data",
    "title": "4  Basic data manipulation",
    "section": "4.1 Selecting and manipulating data",
    "text": "4.1 Selecting and manipulating data\nLet us start by loading tidyverse, in case you have not done so yet:\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──\n\n\n✔ ggplot2 3.4.0      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nAs you can see from the message output above, the dplyr package is part of tidyverse, which gets loaded by default. It allows one to arrange and manipulate data efficiently. The basic functions one should know are rename, select, filter, slice, arrange, and mutate. The first of these we have already looked at in Section 3.2.4. Let us then see some examples of the latter ones. First, we will load the Goldberg2010_data.csv data file (also discussed in the previous chapter):\n\ndat <- read_csv(\"Goldberg2010_data.csv\")\nprint(dat)\n\n# A tibble: 356 × 2\n   Species                   Status\n   <chr>                      <dbl>\n 1 Acnistus_arborescens           1\n 2 Anisodus_tanguticus            1\n 3 Atropa_belladonna              1\n 4 Brachistus_stramonifolius      1\n 5 Brugmansia_aurea               0\n 6 Brugmansia_sanguinea           0\n 7 Capsicum_annuum                1\n 8 Capsicum_baccatum              1\n 9 Capsicum_cardenasii            2\n10 Capsicum_chacoense             1\n# … with 346 more rows\n\n\nNow we will give examples of each of the functions select, filter, slice, arrange, and mutate. They are similar to our earlier rename in that the first argument they take is the data, in the form of a tibble. Their other arguments, and what they each do, are explained below.\n\n4.1.1 select\nThis function chooses columns of the data. The second and subsequent arguments of the function are the columns which should be retained. For example, select(dat, Species) will keep only the Species column of dat:\n\nselect(dat, Species)\n\n# A tibble: 356 × 1\n   Species                  \n   <chr>                    \n 1 Acnistus_arborescens     \n 2 Anisodus_tanguticus      \n 3 Atropa_belladonna        \n 4 Brachistus_stramonifolius\n 5 Brugmansia_aurea         \n 6 Brugmansia_sanguinea     \n 7 Capsicum_annuum          \n 8 Capsicum_baccatum        \n 9 Capsicum_cardenasii      \n10 Capsicum_chacoense       \n# … with 346 more rows\n\n\nIt is also possible to deselect columns, by prepending a minus sign (-) in front of the column names. To drop the Species column, we can type:\n\nselect(dat, -Species)\n\n# A tibble: 356 × 1\n   Status\n    <dbl>\n 1      1\n 2      1\n 3      1\n 4      1\n 5      0\n 6      0\n 7      1\n 8      1\n 9      2\n10      1\n# … with 346 more rows\n\n\nSince there were only two columns in the data to begin with, only the Status column remained in the data after removing Species.\n\n\n4.1.2 filter\nWhile select chooses columns, filter chooses rows from the data. As with all these functions, the first argument of filter is the data. The second argument is a logical condition on the columns. Those rows which satisfy the condition are retained; the rest are dropped. Thus, filter keeps only those rows of the data which fulfill some condition.\nRecall that in the Goldberg2010_data.csv dataset, a Status of 0 means self-incompatibility; a Status of 1 means self-compatibility, and Status values between 2 and 5 refer to various, more complex selfing mechanisms. So in case we wanted to focus only on those species which exhibit complex selfing, we could filter the data like this:\n\nfilter(dat, Status >= 2)\n\n# A tibble: 44 × 2\n   Species               Status\n   <chr>                  <dbl>\n 1 Capsicum_cardenasii        2\n 2 Capsicum_pubescens         2\n 3 Dunalia_solanacea          4\n 4 Lycium_arenicola           4\n 5 Lycium_californicum        3\n 6 Lycium_exsertum            4\n 7 Lycium_fremontii           4\n 8 Lycium_gariepense          4\n 9 Lycium_horridum            4\n10 Lycium_strandveldense      4\n# … with 34 more rows\n\n\n\n\n4.1.3 slice\nWith slice, one can choose rows of the data, just like with filter. Unlike with filter however, slice receives a vector of row indices instead of a condition to be tested on each row. So, for example, if one wanted to keep only the first, second, and fifth rows, then one can do so with slice:\n\nslice(dat, c(1, 2, 5))\n\n# A tibble: 3 × 2\n  Species              Status\n  <chr>                 <dbl>\n1 Acnistus_arborescens      1\n2 Anisodus_tanguticus       1\n3 Brugmansia_aurea          0\n\n\n(Note: the numbers in front of the rows in the output generated by tibbles always pertain to the row numbers of the current table, not the one from which they were created. So the row labels 1, 2, and 3 above simply enumerate the rows of the sliced data. The actual rows still correspond to rows 1, 2, and 5 in the original dat.)\n\n\n4.1.4 arrange\nThis function rearranges the rows of the data, in increasing order of the column given as the second argument. For example, to arrange in increasing order of Status, we write:\n\narrange(dat, Status)\n\n# A tibble: 356 × 2\n   Species                 Status\n   <chr>                    <dbl>\n 1 Brugmansia_aurea             0\n 2 Brugmansia_sanguinea         0\n 3 Cuatresia_exiguiflora        0\n 4 Cuatresia_riparia            0\n 5 Dunalia_brachyacantha        0\n 6 Dyssochroma_viridiflora      0\n 7 Eriolarynx_lorentzii         0\n 8 Grabowskia_duplicata         0\n 9 Iochroma_australe            0\n10 Iochroma_cyaneum             0\n# … with 346 more rows\n\n\nTo arrange in decreasing order, apply the desc function to Status within arrange, like this:\n\narrange(dat, desc(Status))\n\n# A tibble: 356 × 2\n   Species               Status\n   <chr>                  <dbl>\n 1 Solanum_wendlandii         5\n 2 Dunalia_solanacea          4\n 3 Lycium_arenicola           4\n 4 Lycium_exsertum            4\n 5 Lycium_fremontii           4\n 6 Lycium_gariepense          4\n 7 Lycium_horridum            4\n 8 Lycium_strandveldense      4\n 9 Lycium_tetrandrum          4\n10 Lycium_villosum            4\n# … with 346 more rows\n\n\nIt is also perfectly possible to arrange by a column whose type is character string. In that case, the system will automagically sort the rows in alphabetical order—or reverse alphabetical order, in case desc is applied. For example, to sort in reverse alphabetical order of species binomials:\n\narrange(dat, desc(Species))\n\n# A tibble: 356 × 2\n   Species                   Status\n   <chr>                      <dbl>\n 1 Witheringia_solanacea          2\n 2 Witheringia_mexicana           0\n 3 Witheringia_meiantha           0\n 4 Witheringia_macrantha          0\n 5 Witheringia_cuneata            0\n 6 Witheringia_coccoloboides      0\n 7 Withania_somnifera             1\n 8 Withania_coagulans             4\n 9 Vassobia_breviflora            1\n10 Symonanthus_bancroftii         4\n# … with 346 more rows\n\n\nNotice that when we sorted the rows by Status, there are many ties—rows with the same value of Status. In those cases, arrange will not be able to decide which rows should come earlier, and so any ordering that was present before invoking arrange will be retained. In case we would like to break the ties, we can give further sorting variables, as the third, fourth, etc. arguments to arrange. To sort the data by Status, and to resolve ties in alphabetical order of Species, we write:\n\narrange(dat, Status, Species)\n\n# A tibble: 356 × 2\n   Species                 Status\n   <chr>                    <dbl>\n 1 Brugmansia_aurea             0\n 2 Brugmansia_sanguinea         0\n 3 Cuatresia_exiguiflora        0\n 4 Cuatresia_riparia            0\n 5 Dunalia_brachyacantha        0\n 6 Dyssochroma_viridiflora      0\n 7 Eriolarynx_lorentzii         0\n 8 Grabowskia_duplicata         0\n 9 Iochroma_australe            0\n10 Iochroma_cyaneum             0\n# … with 346 more rows\n\n\nThis causes the table to be sorted primarily by Status, but in case there are ties (equal Status between multiple rows), they will be resolved in priority of alphabetical order—first those starting with “A” (if they exist), then “B”, and so on.\n\n\n4.1.5 mutate\nThe mutate function allows us to create new columns from existing ones. We may apply any function or operator we learned about to existing columns, and the result of the computation will go into the new column. We do this in the second argument of mutate (the first, as always, is the data), by first giving a name to the column, then writing =, and then the desired computation. For example, we may find it strange that the selfing status of the species is encoded with a number ranging from 0 to 5, instead of 1 to 6. This is easy to fix however, using mutate:\n\nmutate(dat, NewStatus = Status + 1)\n\n# A tibble: 356 × 3\n   Species                   Status NewStatus\n   <chr>                      <dbl>     <dbl>\n 1 Acnistus_arborescens           1         2\n 2 Anisodus_tanguticus            1         2\n 3 Atropa_belladonna              1         2\n 4 Brachistus_stramonifolius      1         2\n 5 Brugmansia_aurea               0         1\n 6 Brugmansia_sanguinea           0         1\n 7 Capsicum_annuum                1         2\n 8 Capsicum_baccatum              1         2\n 9 Capsicum_cardenasii            2         3\n10 Capsicum_chacoense             1         2\n# … with 346 more rows\n\n\nThe original columns of the data are retained, but we now also have the additional NewStatus column.\nPerhaps more interestingly, we could create a new column indicating whether the selfing mechanism of the species is simple (Status either 0 or 1) or complex (Status between 2 and 5). We can do this using an ifelse function within mutate:\n\nmutate(dat, SelfingMechanism = ifelse(Status < 2, \"simple\", \"complex\"))\n\n# A tibble: 356 × 3\n   Species                   Status SelfingMechanism\n   <chr>                      <dbl> <chr>           \n 1 Acnistus_arborescens           1 simple          \n 2 Anisodus_tanguticus            1 simple          \n 3 Atropa_belladonna              1 simple          \n 4 Brachistus_stramonifolius      1 simple          \n 5 Brugmansia_aurea               0 simple          \n 6 Brugmansia_sanguinea           0 simple          \n 7 Capsicum_annuum                1 simple          \n 8 Capsicum_baccatum              1 simple          \n 9 Capsicum_cardenasii            2 complex         \n10 Capsicum_chacoense             1 simple          \n# … with 346 more rows"
  },
  {
    "objectID": "Basic_data_wrangling.html#using-pipes-to-our-advantage",
    "href": "Basic_data_wrangling.html#using-pipes-to-our-advantage",
    "title": "4  Basic data manipulation",
    "section": "4.2 Using pipes to our advantage",
    "text": "4.2 Using pipes to our advantage\nWhen composing multiple tidyverse functions together, things can get unwieldy quite quickly. Let us take the same data, and create the new column SelfingMechanism as above. What happens if we then filter for only those entries with complex selfing mechanism, and finally, we select the column Species only? Here is the solution:\n\nselect(\n  filter(\n    mutate(dat, SelfingMechanism = ifelse(Status < 2, \"simple\", \"complex\")),\n    SelfingMechanism == \"complex\"\n  ),\n  Species\n)\n\n# A tibble: 44 × 1\n   Species              \n   <chr>                \n 1 Capsicum_cardenasii  \n 2 Capsicum_pubescens   \n 3 Dunalia_solanacea    \n 4 Lycium_arenicola     \n 5 Lycium_californicum  \n 6 Lycium_exsertum      \n 7 Lycium_fremontii     \n 8 Lycium_gariepense    \n 9 Lycium_horridum      \n10 Lycium_strandveldense\n# … with 34 more rows\n\n\nThe expression is highly unpleasant: to a human reader, it is not at all obvious what is happening above. To clarify, we have two options. One is to rely on repeated assignment:\n\nmutatedDat <- mutate(dat, SelfingMechanism = ifelse(Status < 2, \"simple\", \"complex\"))\nfilteredDat <- filter(mutatedDat, SelfingMechanism == \"complex\")\nonlySpeciesDat <- select(filteredDat, Species)\nprint(onlySpeciesDat)\n\n# A tibble: 44 × 1\n   Species              \n   <chr>                \n 1 Capsicum_cardenasii  \n 2 Capsicum_pubescens   \n 3 Dunalia_solanacea    \n 4 Lycium_arenicola     \n 5 Lycium_californicum  \n 6 Lycium_exsertum      \n 7 Lycium_fremontii     \n 8 Lycium_gariepense    \n 9 Lycium_horridum      \n10 Lycium_strandveldense\n# … with 34 more rows\n\n\nThis, however, requires inventing arbitrary variable names at every step, or else overwriting variables. For such a short example, this is not problematic, but doing the same for a long pipeline of dozens of steps could get confusing, as well as dangerous due to the repeatedly modified variables.\nIt turns out that one can get the best of both worlds: the safety of function composition with the conceptual clarity of repeated assignments. This only requires that we make use of the pipe operator %>% that we learned about earlier. As a reminder, for any function f and function argument x, f(x, y, ...) is the same as x %>% f(y, ...), where the ... denote potential further arguments to f. That is, the first argument of the function can be moved from the argument list to in front of the function, before the pipe symbol. The tidyverse functions take the data as their first argument, which means that the use of pipes allow us to very conveniently chain together multiple steps of data analysis. In our case, we can rewrite the original\n\nselect(\n  filter(\n    mutate(dat, SelfingMechanism = ifelse(Status < 2, \"simple\", \"complex\")),\n    SelfingMechanism == \"complex\"\n  ),\n  Species\n)\n\nwith the use of pipes, in a much more transparent way:\n\ndat %>%\n  mutate(SelfingMechanism = ifelse(Status < 2, \"simple\", \"complex\")) %>%\n  filter(SelfingMechanism == \"complex\") %>%\n  select(Species)\n\nAgain, the pipe %>% should be pronounced then. We take the data, then we mutate it, then we filter for complex selfing, and then we select one of the columns. In performing these steps, each function both receives and returns data. Thus, by starting out with the original dat, we no longer need to write out the data argument of the functions explicitly. Instead, the pipe takes care of that automatically for us, making the functions receive as their first input argument the piped-in data, and in turn producing transformed data as their output—which becomes the input for the next function in line.\nIn fact, there is technically no need to even assign dat. The pipe can just as well start with the read_csv call to import the dataset:\n\nread_csv(\"Goldberg2010_data.csv\") %>%\n  mutate(SelfingMechanism = ifelse(Status < 2, \"simple\", \"complex\")) %>%\n  filter(SelfingMechanism == \"complex\") %>%\n  select(Species)\n\n# A tibble: 44 × 1\n   Species              \n   <chr>                \n 1 Capsicum_cardenasii  \n 2 Capsicum_pubescens   \n 3 Dunalia_solanacea    \n 4 Lycium_arenicola     \n 5 Lycium_californicum  \n 6 Lycium_exsertum      \n 7 Lycium_fremontii     \n 8 Lycium_gariepense    \n 9 Lycium_horridum      \n10 Lycium_strandveldense\n# … with 34 more rows"
  },
  {
    "objectID": "Basic_data_wrangling.html#exercises",
    "href": "Basic_data_wrangling.html#exercises",
    "title": "4  Basic data manipulation",
    "section": "4.3 Exercises",
    "text": "4.3 Exercises\n\nThe Smith2003_data.txt dataset we worked with last time occasionally has the entry -999 in its last three columns. This stands for unavailable data. In R, there is a built-in way of referring to such information: by setting a variable to NA. (So, for example, x <- NA will set the variable x to NA.) Modify these columns (using mutate) so that the entries which are equal to -999 are replaced with NA.\nRemove all rows from the data which contain one or more NA values (hint: look up the function drop_na). How many rows are retained? And what was the original number of rows?\n\nThe iris dataset is a built-in table in R. It contains measurements of petal and sepal characteristics from three flower species belonging to the genus Iris (I. setosa, I. versicolor, and I. virginica). If you type iris in the console, you will see the dataset displayed. In solving the problems below, feel free to use the all-important dplyr cheat sheet.\n\nThe format of the data is not a tibble, but a data.frame. As mentioned in the previous chapter, the two are basically the same for practical purposes, though internally, tibbles do offer some advantages. Convert the iris data frame into a tibble. (Hint: look up the as_tibble function.)\nSelect the columns containing petal and sepal length, and species identity.\nGet those rows of the data with petal length less than 4 cm, but sepal length greater than 4 cm.\nSort the data by increasing petal length but decreasing sepal length.\nCreate a new column called MeanLength. It should contain the average of the petal and sepal length (i.e., petal length plus sepal length, divided by 2) of each individual flower.\nPerform the operations from the previous two exercises in a single long function call (using function composition)."
  },
  {
    "objectID": "Summaries_normalization.html#creating-summary-data",
    "href": "Summaries_normalization.html#creating-summary-data",
    "title": "5  Summary statistics and data normalization",
    "section": "5.1 Creating summary data",
    "text": "5.1 Creating summary data\nOne can create summaries of data using the summarise function. This will simply apply some function to a column. For example, to calculate the average population density of species 1 in pop, across both time and patches, one can write\n\npop %>% summarise(meanDensity1 = mean(species1))\n\n# A tibble: 1 × 1\n  meanDensity1\n         <dbl>\n1         5.30\n\n\nHere meanDensity1 is the name of the new column to be created, and the mean function is our summary function, collapsing the data into a single number.\nSo far, this is not particularly interesting; in fact, the exact same effect would have been achieved by typing the shorter mean(pop$species1) instead. The real power of summarise comes through when combined with group_by. This groups the data based on the given grouping variables. Let us see how this works in practice:\n\npop %>% group_by(patch)\n\n# A tibble: 100 × 5\n# Groups:   patch [2]\n    time patch species1 species2 species3\n   <dbl> <chr>    <dbl>    <dbl>    <dbl>\n 1     1 A         8.43     6.62    10.1 \n 2     1 B        10.1      3.28     6.27\n 3     2 A         7.76     6.93    10.3 \n 4     2 B        10.1      3.04     6.07\n 5     3 A         7.09     7.24    10.5 \n 6     3 B        10.1      2.8      5.82\n 7     4 A         6.49     7.54    10.6 \n 8     4 B        10.1      2.56     5.57\n 9     5 A         5.99     7.83    10.7 \n10     5 B        10.1      2.33     5.32\n# … with 90 more rows\n\n\nSeemingly nothing has happened; the only difference is the extra line of comment above, before the printed table, saying Groups: patch [2]. What this means is that the rows of the data were internally split into two groups. The first have \"A\" as their patch, and the second have \"B\". Whenever one groups data using group_by, rows which share the same unique combination of the grouping variables now belong together, and subsequent operations will act separately on each group instead of acting on the table as a whole (which is what we have been doing so far). That is, group_by does not actually alter the data; it only alters the behaviour of the functions applied to the grouped data.\nIf we group not just by patch but also by time, the comment above the table will read Groups: patch, time [100]:\n\npop %>% group_by(patch, time)\n\n# A tibble: 100 × 5\n# Groups:   patch, time [100]\n    time patch species1 species2 species3\n   <dbl> <chr>    <dbl>    <dbl>    <dbl>\n 1     1 A         8.43     6.62    10.1 \n 2     1 B        10.1      3.28     6.27\n 3     2 A         7.76     6.93    10.3 \n 4     2 B        10.1      3.04     6.07\n 5     3 A         7.09     7.24    10.5 \n 6     3 B        10.1      2.8      5.82\n 7     4 A         6.49     7.54    10.6 \n 8     4 B        10.1      2.56     5.57\n 9     5 A         5.99     7.83    10.7 \n10     5 B        10.1      2.33     5.32\n# … with 90 more rows\n\n\nThis is because there are 100 unique combinations of patch and time: two different patch values (\"A\" and \"B\"), and fifty points in time (1, 2, …, 50). So we have “patch A, time 1” as group 1, “patch B, time 1” as group 2, “patch A, time 3” as group 3, and so on until “patch B, time 50” as our group 100.\nAs mentioned, functions that are applied to grouped data will act on the groups separately. To return to the example of calculating the mean population density of species 1 in the two patches, we can write:\n\npop %>%\n  group_by(patch) %>%\n  summarise(meanDensity1 = mean(species1))\n\n# A tibble: 2 × 2\n  patch meanDensity1\n  <chr>        <dbl>\n1 A             5.29\n2 B             5.32\n\n\nOne may obtain multiple summary statistics within the same summarize function. Below we compute both the mean and the standard deviation of the densities per patch:\n\npop %>%\n  group_by(patch) %>%\n  summarise(meanDensity1 = mean(species1), sdDensity1 = sd(species1))\n\n# A tibble: 2 × 3\n  patch meanDensity1 sdDensity1\n  <chr>        <dbl>      <dbl>\n1 A             5.29      0.833\n2 B             5.32      3.81 \n\n\nLet us see what happens if we calculate the mean density of species 1—but grouping by time instead of patch:\n\npop %>%\n  group_by(time) %>%\n  summarise(meanDensity1 = mean(species1))\n\n# A tibble: 50 × 2\n    time meanDensity1\n   <dbl>        <dbl>\n 1     1         9.28\n 2     2         8.94\n 3     3         8.60\n 4     4         8.3 \n 5     5         8.04\n 6     6         7.84\n 7     7         7.68\n 8     8         7.54\n 9     9         7.44\n10    10         7.35\n# … with 40 more rows\n\n\nThe resulting table has 50 rows—half the number of rows in the original data, but many more than the two rows we get after grouping by patch. The reason is that there are 50 unique time points, and so the average is now computed over those rows which share time. But there are only two rows per moment of time: the rows corresponding to patch A and patch B. When we call summarise after having grouped by time, the averages are computed over the densities in these two rows only, per group. That is why here we end up with a table which has a single row per point in time.\n\n\n\n\n\n\nWarning\n\n\n\nAn easy mistake to make when one first meets with grouping and summaries is to assume that if we call group_by(patch), then the subsequent summaries will be taken over patches. This is not the case, and be sure to take a moment to understand why. When we apply group_by(patch), we are telling R to treat different patch values as group indicators. Therefore, when creating a summary, only the patch identities are retained from the original data (apart from the new summary statistics we calculate, of course). This means that the subsequent summaries are taken over everything except the patches. This should be clear after comparing the outputs of\n\npop %>% group_by(patch) %>% summarise(meanDensity1 = mean(species1))\n\nand\n\npop %>% group_by(time) %>% summarise(meanDensity1 = mean(species1))\n\nThe first distinguishes the rows of the data only by patch, and therefore the average is taken over time. The second distinguishes the rows by time, so the average is taken over the patches. Run the two expressions again to see the difference between them!\n\n\nWe can use functions such as mutate or filter on grouped data. For example, we might want to know the deviation of species 1’s density from its average in each patch. Doing the following does not quite do what we want:\n\npop %>% mutate(species1Dev = species1 - mean(species1))\n\n# A tibble: 100 × 6\n    time patch species1 species2 species3 species1Dev\n   <dbl> <chr>    <dbl>    <dbl>    <dbl>       <dbl>\n 1     1 A         8.43     6.62    10.1        3.13 \n 2     1 B        10.1      3.28     6.27       4.83 \n 3     2 A         7.76     6.93    10.3        2.46 \n 4     2 B        10.1      3.04     6.07       4.82 \n 5     3 A         7.09     7.24    10.5        1.79 \n 6     3 B        10.1      2.8      5.82       4.82 \n 7     4 A         6.49     7.54    10.6        1.19 \n 8     4 B        10.1      2.56     5.57       4.81 \n 9     5 A         5.99     7.83    10.7        0.685\n10     5 B        10.1      2.33     5.32       4.80 \n# … with 90 more rows\n\n\nThis will put the difference of species 1’s density from its mean density across both time and patches into the new column species1Dev. Which is not the same as calculating the difference from the mean in a given patch—patch A for rows corresponding to patch A, and patch B for the others. To achieve this, all one needs to do is to group the data by patch before invoking mutate:\n\npop %>%\n  group_by(patch) %>%\n  mutate(species1Dev = species1 - mean(species1))\n\n# A tibble: 100 × 6\n# Groups:   patch [2]\n    time patch species1 species2 species3 species1Dev\n   <dbl> <chr>    <dbl>    <dbl>    <dbl>       <dbl>\n 1     1 A         8.43     6.62    10.1        3.14 \n 2     1 B        10.1      3.28     6.27       4.81 \n 3     2 A         7.76     6.93    10.3        2.47 \n 4     2 B        10.1      3.04     6.07       4.80 \n 5     3 A         7.09     7.24    10.5        1.80 \n 6     3 B        10.1      2.8      5.82       4.80 \n 7     4 A         6.49     7.54    10.6        1.20 \n 8     4 B        10.1      2.56     5.57       4.79 \n 9     5 A         5.99     7.83    10.7        0.702\n10     5 B        10.1      2.33     5.32       4.78 \n# … with 90 more rows\n\n\nComparing this with the previous table, we see that the values in the species1Dev column are now different, because this time the differences are taken with respect to the average densities per each patch.\nFinally, since group_by changes subsequent behaviour, we might eventually want to get rid of the grouping in our data. To do so, one must use ungroup. For example:\n\npop %>%\n  group_by(patch) %>%\n  summarise(meanDensity1 = mean(species1), sdDensity1 = sd(species1)) %>%\n  ungroup()\n\n# A tibble: 2 × 3\n  patch meanDensity1 sdDensity1\n  <chr>        <dbl>      <dbl>\n1 A             5.29      0.833\n2 B             5.32      3.81 \n\n\nIt is good practice to always ungroup the data after we have calculated what we wanted using the group structure."
  },
  {
    "objectID": "Summaries_normalization.html#data-normalization",
    "href": "Summaries_normalization.html#data-normalization",
    "title": "5  Summary statistics and data normalization",
    "section": "5.2 Data normalization",
    "text": "5.2 Data normalization\nIn science, we often strive to work with so-called normalized data. A dataset is normalized if:\n\nEach variable is in its own column;\nEach observation is in its own row.\n\nNormalized data are suitable for performing operations, statistics, and plotting on. Furthermore, normalized data have a certain tidy feel to them, in the sense that their organization always follows the same general pattern regardless of the type of dataset one studies. (By contrast, every non-normalized dataset tends to be messy in its own unique way.) The tidyverse offers a simple and convenient way to normalize data.\nFor example, the pop table from the previous section is not normalized. This is because although each variable is in its own column, it is not true that each observation is in its own row. In fact, each row contains three observations: the densities of species 1, 2, and 3 at a given time and place. To normalize these data, we create key-value pairs. We merge the columns for species densities into just two new ones. The first of these (the key) indicates whether it is species 1, or 2, or 3 which the given row refers to. The second column (the value) contains the population density of the given species. Such key-value pairs are created by the function pivot_longer:\n\npop %>% pivot_longer(cols = 3:5, names_to = \"species\", values_to = \"density\")\n\n# A tibble: 300 × 4\n    time patch species  density\n   <dbl> <chr> <chr>      <dbl>\n 1     1 A     species1    8.43\n 2     1 A     species2    6.62\n 3     1 A     species3   10.1 \n 4     1 B     species1   10.1 \n 5     1 B     species2    3.28\n 6     1 B     species3    6.27\n 7     2 A     species1    7.76\n 8     2 A     species2    6.93\n 9     2 A     species3   10.3 \n10     2 B     species1   10.1 \n# … with 290 more rows\n\n\nThe function pivot_longer takes three arguments (apart, of course, from the first data argument that we may also pipe in, like above). First, cols is the list of columns to be converted into key-value pairs. One can refer to the columns by number: 3:5 is the same as c(3, 4, 5) and selects the third, fourth, and fifth columns—the ones corresponding to the population densities. We could also have written c(\"species1\", \"species2\", \"species3\") instead, choosing columns by their names. This can give greater clarity, albeit at the cost of more typing. Second, the argument names_to is the name of the new key column. Finally, values_to is the name of the new value column.\nNotice that the above table is now normalized: each column records a single variable, and each row contains a single observation. Notice also that, unlike the original pop which had 100 rows and 5 columns, the normalized version has 300 rows and 4 columns. This is natural: since the number of rows was reduced, there must be some extra rows to prevent the loss of information.\nIt is possible to “undo” the effect pivot_longer. To do so, use pivot_wider:\n\npop %>%\n  pivot_longer(cols = 3:5, names_to = \"species\", values_to = \"density\") %>%\n  pivot_wider(names_from = \"species\", values_from = \"density\")\n\n# A tibble: 100 × 5\n    time patch species1 species2 species3\n   <dbl> <chr>    <dbl>    <dbl>    <dbl>\n 1     1 A         8.43     6.62    10.1 \n 2     1 B        10.1      3.28     6.27\n 3     2 A         7.76     6.93    10.3 \n 4     2 B        10.1      3.04     6.07\n 5     3 A         7.09     7.24    10.5 \n 6     3 B        10.1      2.8      5.82\n 7     4 A         6.49     7.54    10.6 \n 8     4 B        10.1      2.56     5.57\n 9     5 A         5.99     7.83    10.7 \n10     5 B        10.1      2.33     5.32\n# … with 90 more rows\n\n\nThe two named arguments of pivot_wider above are names_from (which specifies the column from which the names for the new columns will be taken), and values_from (the column whose values will be used to fill in the rows under those new columns).\nAs a remark, one could make the data even “wider”, by not only making columns out of the population densities, but the densities at a given patch. Doing so is very simple: one just needs to specify both the species and patch columns from which the new column names will be compiled:\n\npop %>%\n  pivot_longer(cols = 3:5, names_to = \"species\", values_to = \"density\") %>%\n  pivot_wider(names_from = c(\"species\", \"patch\"), values_from = \"density\")\n\n# A tibble: 50 × 7\n    time species1_A species2_A species3_A species1_B species2_B species3_B\n   <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>\n 1     1       8.43       6.62      10.1        10.1       3.28       6.27\n 2     2       7.76       6.93      10.3        10.1       3.04       6.07\n 3     3       7.09       7.24      10.5        10.1       2.8        5.82\n 4     4       6.49       7.54      10.6        10.1       2.56       5.57\n 5     5       5.99       7.83      10.7        10.1       2.33       5.32\n 6     6       5.58       8.1       10.7        10.1       2.12       5.08\n 7     7       5.27       8.34      10.6        10.1       1.92       4.86\n 8     8       5.02       8.54      10.4        10.1       1.74       4.64\n 9     9       4.82       8.7       10.0        10.0       1.58       4.43\n10    10       4.66       8.82       9.66       10.0       1.43       4.23\n# … with 40 more rows\n\n\nIf normalized data are what we strive for, what is the practical use of pivot_wider? There are two answers to this question. First, while non-normalized data are indeed less efficient from a computational and data analysis standpoint, they are often more human-readable. For example, the pop table is easy to read despite the lack of normalization, because each row corresponds to a given time and place. By normalizing the data, information referring to any given time and place will be spread out over multiple (in our case, three) rows—one for each species. While this is preferable from a data analysis point of view, it can be more difficult to digest visually. Second, wide data lend themselves very well to one particular class of statistical techniques called multivariate analysis. In case one wants to perform multivariate analysis, wide-format data are often better than normalized data.\nFinally, it is worth noting the power of normalized data in, e.g., generating summary statistics. To obtain the mean and the standard deviation of the population densities for each species in each patch, all one has to do is this:\n\npop %>%\n  pivot_longer(3:5, names_to = \"species\", values_to = \"density\") %>% # Normalize data\n  group_by(patch, species) %>% # Group data by both species and patch\n  summarise(meanDensity = mean(density), sdDensity = sd(density)) %>% # Obtain statistics\n  ungroup() # Don't forget to ungroup the data at the end\n\n# A tibble: 6 × 4\n  patch species  meanDensity sdDensity\n  <chr> <chr>          <dbl>     <dbl>\n1 A     species1        5.29     0.833\n2 A     species2        8.05     0.559\n3 A     species3        7.51     1.56 \n4 B     species1        5.32     3.81 \n5 B     species2        1.07     0.737\n6 B     species3        6.57     2.48"
  },
  {
    "objectID": "Summaries_normalization.html#exercises",
    "href": "Summaries_normalization.html#exercises",
    "title": "5  Summary statistics and data normalization",
    "section": "5.3 Exercises",
    "text": "5.3 Exercises\nThe exercises below use the iris dataset—the same that we used for last chapter’s data wrangling exercises. Convert the iris data to a tibble with the as_tibble() function, and assign it to a variable.\n\nCreate a new column in the iris dataset which contains the deviation of petal lengths from the average of the whole dataset.\nCreate a new column in the iris dataset which contains the deviation of petal lengths from the average of each species. (Hint: group_by the species and then mutate!)\nCreate a table where the rows are the three species, and the columns are: average petal length, variance of petal length, average sepal length, and variance of sepal length.\nCreate key-value pairs in the iris dataset for the petal characteristics. In other words, have a column called Petal.Trait (whose values are either Petal.Length or Petal.Width), and another column called Petal.Value (with the length/width values).\nRepeat the same exercise, but now for sepal traits.\nFinally, do it for both petal and sepal traits simultaneously, to obtain a fully normalized form of the iris data. That is, the key column (call it Flower.Trait) will have the values Petal.Length, Petal.Width, Sepal.Length, and Sepal.Width. And the value column (which you can call Trait.Value) will have the corresponding measurements."
  },
  {
    "objectID": "Creating_figures.html#sec-CI",
    "href": "Creating_figures.html#sec-CI",
    "title": "6  Creating publication-grade figures",
    "section": "6.1 Summaries and confidence intervals",
    "text": "6.1 Summaries and confidence intervals\nProviding appropriate information on experimental errors is a hallmark of any credible scientific graph. Choose a type of error based on the conclusion that you want the reader to draw. While the standard deviation (SD) represents the dispersion of the data, the standard error of the mean (SEM) and confidence intervals (CI) report the certainty of the estimate of a value (e.g., certainty in estimating the mean). Let us see examples of each.\nLet us first obtain the mean and the standard deviation of the petal lengths for each species. We then would like to plot them. How to proceed? By creating a workflow which first uses group_by and summarise to obtain the required statistics (means and standard deviations), and then uses these data for plotting. One can include all these steps in a logical workflow:\n\niris %>%\n  group_by(Species) %>% # Perform summary calculations for each species\n  summarise(mP = mean(Petal.Length), # Mean petal length\n            sP = sd(Petal.Length)) %>% # Standard deviation of petal length\n  ungroup() %>% # Ungroup data\n  ggplot() + # Start plotting\n  aes(x = Species, y = mP, ymin = mP - sP, ymax = mP + sP) + # Mean +/- sd\n  geom_col() + # This takes the y aesthetic, for plotting the mean\n  geom_errorbar(width = 0.2) # Takes the ymin and ymax aesthetics\n\n\n\n\nNote that the y-axis label is not very descriptive. This is because it inherited the name of the corresponding data column, mP. There are multiple ways to change it; the simplest is to add ylab(\"Petal length\") to the plot. Another way of doing so is duscussed in Chapter 7.\nIn case we want to calculate the 95% confidence intervals of the mean values, we first obtain some necessary summary statistics: the number of observations (sample size) in each group; the standard error of the mean (standard deviation divided by the square root of the sample size); and finally, the confidence interval itself (read off from the t-distribution, with one fewer degrees of freedom than the sample size). We can then include these confidence intervals on top of the mean values:\n\niris %>%\n  group_by(Species) %>%\n  summarise(mP = mean(Petal.Length), # Mean petal length per species\n            sP = sd(Petal.Length), # Standard deviation per species\n            N = n(), # Sample size (number of observations) per species\n            SEM = sP / sqrt(N), # Standard error of the mean\n            CI = SEM * qt(0.975, N - 1)) %>% # Confidence interval, read off\n                 # at 0.975 because plus/minus 2.5% adds up to 5%\n  ggplot() +\n  aes(x = Species, y = mP, ymin = mP - CI, ymax = mP + CI) +\n  geom_col(alpha = 0.4, colour = \"steelblue\", fill = \"steelblue\") +\n  geom_errorbar(width = 0.2, colour = \"steelblue\") +\n  ylab(\"Petal length\") +\n  theme_bw()\n\n\n\n\nOne final note. The above type of bar-and-whisker plot is common in the literature, and therefore one should be aware of how to make and read them. That said, there are several reasons why displaying summary data like this is not a good idea. By starting the bars from zero, the plot implies that zero is a natural point of comparison for all the data. Unfortunately, this can also visually distort the information we wish to convey. Consider the following graph:\n\ntibble(Species = c(\"species 1\", \"species 2\"),\n       Average = c(148, 152),\n       CI = c(0.8, 0.9)) %>%\n  ggplot() +\n  aes(x = Species, y = Average, ymin = Average - CI, ymax = Average + CI) +\n  geom_col(alpha = 0.4, colour = \"steelblue\", fill = \"steelblue\") +\n  geom_errorbar(width = 0.2, colour = \"steelblue\") +\n  ylab(\"Trait value\") +\n  theme_bw()\n\n\n\n\nIt is impossible to see whether there are any relevant differences between the two species. The following is exactly the same, but with the mean values shown with points instead of bars:\n\ntibble(Species = c(\"species 1\", \"species 2\"),\n       Average = c(148, 152),\n       CI = c(0.8, 0.9)) %>%\n  ggplot() +\n  aes(x = Species, y = Average, ymin = Average - CI, ymax = Average + CI) +\n  geom_point(colour = \"steelblue\") +\n  geom_errorbar(width = 0.2, colour = \"steelblue\") +\n  ylab(\"Trait value\") +\n  theme_bw()\n\n\n\n\nIt is now quite obvious that the two observations are distinct. Remaking our earlier bar-and-whisker graph of the iris dataset in this spirit, we simply replace geom_col with geom_point:\n\niris %>%\n  group_by(Species) %>%\n  summarise(mP = mean(Petal.Length), # Mean petal length per species\n            sP = sd(Petal.Length), # Standard deviation per species\n            N = n(), # Sample size (number of observations) per species\n            SEM = sP / sqrt(N), # Standard error of the mean\n            CI = SEM * qt(0.975, N - 1)) %>% # Confidence interval, read off\n                 # at 0.975 because plus/minus 2.5% adds up to 5%\n  ggplot() +\n  aes(x = Species, y = mP, ymin = mP - CI, ymax = mP + CI) +\n  geom_point(colour = \"steelblue\") +\n  geom_errorbar(width = 0.2, colour = \"steelblue\") +\n  ylab(\"Petal length\") +\n  theme_bw()"
  },
  {
    "objectID": "Creating_figures.html#exercises",
    "href": "Creating_figures.html#exercises",
    "title": "6  Creating publication-grade figures",
    "section": "6.2 Exercises",
    "text": "6.2 Exercises\nFauchald et al. (2017) tracked the population size of various herds of caribou in North America over time, and correlated population cycling with the amount of vegetation and sea-ice cover. The part of their data that we will use consists of two files (found on Lisam): pop_size.tsv (data on herd population sizes), and sea_ice.tsv (on sea levels of sea ice cover per year and month).\n\nThe file sea_ice.tsv is in human-readable, wide format. Note however that the rule “each set of observations is stored in its own row” is violated. For computation, we would like to organize the data in a tidy tibble with four columns: Herd, Year, Month, and Cover. To this end, apply the function pivot_longer to columns 3-14 in the tibble, gathering the names of the months in the new column Month and the values in the new column Cover.\nUse pop_size.tsv to make a plot of herd sizes through time. Let the x-axis be Year, the y-axis be population size. Show different herds in different colors. For geometry, use points.\nThe previous plot is actually not that easy to see and interpret. To make it better, add a line geometry as well, which will connect the points with lines.\nMake a histogram out of all population sizes in the data.\nMake the same histogram, but break it down by herd, using a different color and fill for each herd.\nInstead of a histogram, make a density plot with the same data and display (look up geom_density if needed).\nMake box plots of the population size of each herd. Along the x-axis, each herd should be separately displayed; the y-axis should be population size. The box plots should summarize population sizes across all years.\nLet us go back to sea_ice.tsv. Make the following plot. Along the x-axis, have Year. Along the y-axis, Month. Then, for each month-year pair, color the given part of the plot darker for lower ice cover and lighter for more. (Hint: look up geom_tile if needed.) Finally, make sure to do all this only for the herd with the i.d. WAH (filter the data before plotting).\n\n\n\n\n\nFauchald, Per, Taejin Park, Hans Tømmervik, Ranga Myneni, and Vera Helene Hausner. 2017. “Arctic greening from warming promotes declines in caribou populations.” Science Advances 3 (4): e1601365. https://doi.org/10.1126/sciadv.1601365.\n\n\nWilkinson, Leland. 2006. The Grammar of Graphics. Secaucus, NJ, USA: Springer Science & Business Media."
  },
  {
    "objectID": "Further_plotting_options.html#smoothing-and-regression-lines",
    "href": "Further_plotting_options.html#smoothing-and-regression-lines",
    "title": "7  Some further plotting options; introducing factors",
    "section": "7.1 Smoothing and regression lines",
    "text": "7.1 Smoothing and regression lines\nLast time we learned about aesthetic mappings and various geom_ options, such as geom_point, geom_histogram, and geom_boxplot. Let us explore another type of geom_, which approximates the trend of a set of data points with a line and an error bar that shows the confidence interval of the estimate at each point:\n\nlibrary(tidyverse)\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\nWhile such fits are occasionally useful, we often want a linear least-suqares regression on our data. To get such a linear fit, add the argument method = \"lm\" to geom_smooth() (\"lm\" stands for “linear model”):\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\nLinear regression lines are usually shown without the confidence intervals (the gray band around the regression line). To drop this, set se = FALSE:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nWhat happens if we color the data points by species? Let us add colour = Species to the list of aesthetic mappings:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length, colour = Species) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nNow the regression line is automatically fitted to the data within each of the groups separately—a highly useful behavior.\nNotice also that a color legend was automatically created and put to the right of the graph. This is the default in ggplot2. You can move them to another position by specifying the legend.position option within the theme function that can be added onto the plot:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length, colour = Species) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme(legend.position = \"left\") # \"top\", \"bottom\", \"left\", \"right\", or \"none\"\n\n\n\n\nSpecifying legend.position = \"none\" omits the legend altogether.\nA word of caution: in case the legend positioning is matched with a generic theme such as theme_bw(), one should put the legend position after the main theme definition. The reason is that pre-defined themes like theme_bw() override any specific theme options you might specify. The rule of thumb is: any theme() component to your plot should be added only after the generic theme definition. Otherwise the theme() component will be overridden and will not take effect. For example, this does not work as intended:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length, colour = Species) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme(legend.position = \"left\") + # Position legend at the left\n  theme_bw() # This defines the general theme - and thus overrides the line above...\n\n\n\n\nBut this one does:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length, colour = Species) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw() + # This defines the general theme\n  theme(legend.position = \"left\") # We now override the default legend positioning"
  },
  {
    "objectID": "Further_plotting_options.html#scales",
    "href": "Further_plotting_options.html#scales",
    "title": "7  Some further plotting options; introducing factors",
    "section": "7.2 Scales",
    "text": "7.2 Scales\nThe aesthetic mappings of a graph (x-axis, y-axis, color, fill, size, shape, alpha, …) are automatically rendered into the displayed plot, based on certain default settings within ggplot2. These defaults can be altered, however. Consider the following bare-bones plot:\n\niris %>%\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot()\n\n\n\n\nWe can now change, for example, how the y-axis is displayed. The component to be added to the plot is scale_y_continuous(). Here scale means we are going to change the scaling of some aesthetic mapping, y refers to the y-axis (as expected, it can be replaced with x, colour, fill, etc.), and continuous means that the scaling of the axis is not via discrete values (e.g., either 1 or 2 or 3 but nothing in between), but continuous (every real number is permissible along the y-axis). The plot component scale_y_continuous() takes several arguments; take a look look at its help page to see all possible options. Here we mention a few of these. First, there is the name option, which is used to relabel the axis. The limits argument receives a vector of two values, containing the lower and upper limits of the plot. If any of them is set to NA, the corresponding limit will be determined automatically. Next, the breaks argument controls where the tick marks along the axis go. It is given as a vector, with its entries corresponding to the y-coordinates of the tick marks. Finally, labels determines what actually gets written on the axis at the tick mark points—it is therefore also a vector, its length matching that of breaks.\nAs an example, let us scale the y-axis of the previous graph in the following way. The axis label should read “Petal length [cm]”, instead of the current “Petal.Length”. It should go from 0 to 7, with a break at those two values and also halfway in between at 3.5. Here is how to do this:\n\niris %>%\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot() +\n  scale_y_continuous(name = \"Petal length [cm]\",\n                     limits = c(0, 7),\n                     breaks = c(0, 3.5, 7))\n\n\n\n\nWhat should we do if, for some reason, we would additionally like the “3.5” in the middle to be displayed as “7/2” instead (an exact value)? In that case, we can add an appropriate labels option as an argument to scale_y_continuous:\n\niris %>%\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot() +\n  scale_y_continuous(name = \"Petal length [cm]\",\n                     limits = c(0, 7),\n                     breaks = c(0, 3.5, 7),\n                     labels = c(\"0\", \"7/2\", \"7\"))\n\n\n\n\nThe x-axis can be scaled similarly. One important difference though is that here, the x-axis has a discrete scale. Since we are displaying the species along it, any value must be either setosa or versicolor or virginica; it makes no sense to talk about what is “halfway in between setosa and versicolor”. Therefore, one should use scale_x_discrete(). Its options are similar to those of scale_x_continuous(). For instance, let us override the axis label, spelling out that the three species belong to the genus Iris:\n\niris %>%\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot() +\n  scale_y_continuous(name = \"Petal length [cm]\",\n                     limits = c(0, 7),\n                     breaks = c(0, 3.5, 7),\n                     labels = c(\"0\", \"7/2\", \"7\")) +\n  scale_x_discrete(name = \"Species (genus: Iris)\")\n\n\n\n\nAlternatively, one could also redefine the labels and get an equally good graph:\n\niris %>%\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot() +\n  scale_y_continuous(name = \"Petal length [cm]\",\n                     limits = c(0, 7),\n                     breaks = c(0, 3.5, 7),\n                     labels = c(\"0\", \"7/2\", \"7\")) +\n  scale_x_discrete(labels = c(\"Iris setosa\", \"Iris versicolor\", \"Iris virginica\"))\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn case you would like to display the species names in italics, as is standard requirement when writing binomial nomenclature, feel free to add theme(axis.text.x = element_text(face = \"italic\")) to the end of the plot. We will not be going into more detail on tweaking themes, but feel free to explore the possibilities by looking at the help pages or Googling them.\n\n\nOther aesthetic mappings can also be adjusted, such as colour, fill, size, or alpha. One useful way to do it is through scale_colour_manual(), scale_fill_manual(), and so on. These are like scale_colour_discrete(), scale_fill_discrete() etc., except that they allow one to specify a discrete set of values by hand. Let us do this for color and fill:\n\niris %>%\n  ggplot() +\n  aes(x = Species, y = Petal.Length, colour = Species, fill = Species) +\n  geom_boxplot(alpha = 0.2) +\n  scale_y_continuous(name = \"Petal length [cm]\",\n                     limits = c(0, 7),\n                     breaks = c(0, 3.5, 7),\n                     labels = c(\"0\", \"7/2\", \"7\")) +\n  scale_x_discrete(labels = c(\"Iris setosa\", \"Iris versicolor\", \"Iris virginica\")) +\n  scale_colour_manual(values = c(\"steelblue\", \"goldenrod\", \"forestgreen\")) +\n  scale_fill_manual(values = c(\"steelblue\", \"goldenrod\", \"forestgreen\"))\n\n\n\n\nWe used the built-in color names \"steelblue\", \"goldenrod\", and \"forestgreen\" above. A full R color cheat sheet can be found here, for more options and built-in colors."
  },
  {
    "objectID": "Further_plotting_options.html#sec-factors",
    "href": "Further_plotting_options.html#sec-factors",
    "title": "7  Some further plotting options; introducing factors",
    "section": "7.3 Reordering labels using factors",
    "text": "7.3 Reordering labels using factors\nIn the previous examples, the order of text labels was always automatically determined. In fact, the basic rule in R is that the ordering follows the alphabet: the default is for setosa to precede versicolor, which will be followed by virginica. This default ordering can be inconvenient, however. Consider the following table of data, with hypothetical temperature measurements for each month of the year:\n\ntibble(month = month.abb, temperature = -cos(seq(0, 2 * pi, l = 12)))\n\n# A tibble: 12 × 2\n   month temperature\n   <chr>       <dbl>\n 1 Jan        -1    \n 2 Feb        -0.841\n 3 Mar        -0.415\n 4 Apr         0.142\n 5 May         0.655\n 6 Jun         0.959\n 7 Jul         0.959\n 8 Aug         0.655\n 9 Sep         0.142\n10 Oct        -0.415\n11 Nov        -0.841\n12 Dec        -1    \n\n\n(month.abb is a built-in vector of character strings in R, with the abbreviations of the months’ names from January to December.) So far so good. However, if we plot this with months along the x-axis and temperature along the y-axis, we run into trouble because R displays items by alphabetical instead of chronological order:\n\ntibble(month = month.abb, temperature = -cos(seq(0, 2 * pi, l = 12))) %>%\n  ggplot() +\n  aes(x = month, y = temperature) +\n  geom_point(colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\nTo fix this, one must convert the type of month from a simple vector of character strings to a vector of factors. Factors are categorical variables (i.e., take on well-defined distinct values instead of varying on a continuous scale like double-precision numbers), but with an extra attribute which determines the order of those values. This ordering is often referred to as the levels of the factor, the value which is set to be the first has level 1, the next one level 2, and so on.\nOne very convenient way of assigning factor levels is through the tidyverse function as_factor.1 This function takes a vector of values and, if the values are numeric, assigns them levels based on those numerical values. However, if the values are character strings, then the levels are assigned in order of appearance within the vector. This is perfect for us, because the months are in proper order already within the tibble:\n\ntibble(month = month.abb, temperature = -cos(seq(0, 2 * pi, l = 12))) %>%\n  mutate(month = as_factor(month)) %>%\n  ggplot() +\n  aes(x = month, y = temperature) +\n  geom_point(colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\nIt is also possible to take a factor and reassign its factor levels manually. This can be done with the fct_relevel function:\n\ntibble(month = month.abb, temperature = -cos(seq(0, 2 * pi, l = 12))) %>%\n  mutate(month = fct_relevel(month, \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n                             \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")) %>%\n  ggplot() +\n  aes(x = month, y = temperature) +\n  geom_point(colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\nAlso, the use of fct_relevel need not be this laborious. If all we want to do is place a few factor levels to be first ones without changing any of the others, it is possible to enter just their names. Often, for example, a factor column holds various experimental treatments, one of which is called \"control\". In that case, all we might want is to make the control be the first factor level, without altering any of the others. This could be done with something like fct_relevel(treatment, \"control\"), assuming treatment is the name of the vector or column in a data frame that holds the different experimental treatment names."
  },
  {
    "objectID": "Further_plotting_options.html#sec-faceting",
    "href": "Further_plotting_options.html#sec-faceting",
    "title": "7  Some further plotting options; introducing factors",
    "section": "7.4 Facets",
    "text": "7.4 Facets\nPlots can be faceted (subplots created and arranged in a grid layout) based on some variable or variables. For instance, let us create histograms of petal lengths in the iris dataset, like we did last time:\n\niris %>%\n  ggplot() +\n  aes(x = Petal.Length) +\n  geom_histogram()\n\n\n\n\nThis way, one cannot see which part of the histogram belongs to which species. One fix to this is to color the histogram by species—this is what we have done before. Another is to separate the plot into three facets, each displaying data for one of the species only:\n\niris %>%\n  ggplot() +\n  aes(x = Petal.Length) +\n  geom_histogram() +\n  facet_grid(. ~ Species)\n\n\n\n\nThe component facet_grid(x ~ y) means that the data will be grouped based on columns x and y, with the distinct values of column x making up the rows and those of column y the columns of the grid of plots. If one of them is replaced with a dot (as above), then that variable is ignored, and only the other variable is used in creating a row (or column) of subplots. So, to display the same data but with the facets arranged in one column instead of one row, we simply replace facet_grid(. ~ Species) with facet_grid(Species ~ .):\n\niris %>%\n  ggplot() +\n  aes(x = Petal.Length) +\n  geom_histogram() +\n  facet_grid(Species ~ .)\n\n\n\n\nIn this particular case, the above graph is preferable to the previous one, because the three subplots now share the same x-axis. This makes it easier to compare the distribution of petal lengths across the species.\nTo illustrate how to make a two-dimensional grid of facets, let us normalize the iris dataset using pivot_longer():\n\nas_tibble(iris) %>%\n  pivot_longer(cols = c(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width),\n               names_to = \"Trait\",\n               values_to = \"Measurement\")\n\n# A tibble: 600 × 3\n   Species Trait        Measurement\n   <fct>   <chr>              <dbl>\n 1 setosa  Sepal.Length         5.1\n 2 setosa  Sepal.Width          3.5\n 3 setosa  Petal.Length         1.4\n 4 setosa  Petal.Width          0.2\n 5 setosa  Sepal.Length         4.9\n 6 setosa  Sepal.Width          3  \n 7 setosa  Petal.Length         1.4\n 8 setosa  Petal.Width          0.2\n 9 setosa  Sepal.Length         4.7\n10 setosa  Sepal.Width          3.2\n# … with 590 more rows\n\n\nAs seen, now the Measurement in every row is characterized by two other variables: Species and Trait (i.e., whether the given value refers to the sepal length, petal width etc. of the given species). We can create a histogram of each measured trait for each species now, in a remarkably simple way:\n\nas_tibble(iris) %>%\n  pivot_longer(cols = c(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width),\n               names_to = \"Trait\",\n               values_to = \"Measurement\") %>%\n  ggplot() +\n  aes(x = Measurement) +\n  geom_histogram() +\n  facet_grid(Species ~ Trait)"
  },
  {
    "objectID": "Further_plotting_options.html#saving-plots",
    "href": "Further_plotting_options.html#saving-plots",
    "title": "7  Some further plotting options; introducing factors",
    "section": "7.5 Saving plots",
    "text": "7.5 Saving plots\nTo save the most recently created ggplot figure, simply type\n\nggsave(filename = \"graph.pdf\", width = 4, height = 3)\n\nHere filename is the name (with path and extension) of the file you want to save the figure into. The extension is important: by having specified .pdf, the system automatically saves the figure in PDF format. To use, say, PNG instead:\n\nggsave(filename = \"graph.png\", width = 4, height = 3)\n\nSince PDF is a vectorized file format (i.e., the file contains the instructions for generating the plot elements instead of a pixel representation), it is arbitrarily scalable, and is therefore the preferred way of saving and handling scientific graphs.\nThe width and height parameters specify, in inches, the dimensions of the saved plot. Note that this also scales some other plot elements, such as the size of the axis labels and plot legends. This means you can play with the width and height parameters to save the figure at a size where the labels are clearly visible without being too large.\nIn case you would like to save a figure that is not the last one that was generated, you can specify the plot argument to ggsave(). to do so, first you should assign a plot to a variable. For example:\n\np <- ggplot(iris) + # Assign the ggplot object to the variable p\n  aes(x = Petal.Length) +\n  geom_histogram()\n\nand then\n\nggsave(filename = \"graph.pdf\", plot = p, width = 4, height = 3)"
  },
  {
    "objectID": "Joining_data.html#merging-two-related-tables-into-one",
    "href": "Joining_data.html#merging-two-related-tables-into-one",
    "title": "8  Joining data",
    "section": "8.1 Merging two related tables into one",
    "text": "8.1 Merging two related tables into one\nSo far, we have been working with a single table of data at a time. Often however, information about the same thing is scattered across multiple tables and files. In such cases, we sometimes want to join those separate tables into a single one. To illustrate how this can be done, let us create two simple tables. The first will contain the names of students, along with their chosen subject:\n\nlibrary(tidyverse)\n\nstudies <- tibble(name    = c(\"Sacha\", \"Gabe\", \"Alex\"),\n                  subject = c(\"Physics\", \"Chemistry\", \"Biology\"))\nprint(studies)\n\n# A tibble: 3 × 2\n  name  subject  \n  <chr> <chr>    \n1 Sacha Physics  \n2 Gabe  Chemistry\n3 Alex  Biology  \n\n\nThe second table contains slightly different information: it holds which year a given student is currently into their studies.\n\nstage <- tibble(name = c(\"Sacha\", \"Alex\", \"Jamie\"),\n                year = c(3, 1, 2))\nprint(stage)\n\n# A tibble: 3 × 2\n  name   year\n  <chr> <dbl>\n1 Sacha     3\n2 Alex      1\n3 Jamie     2\n\n\nNotice that, while Sacha and Alex appear in both tables, Gabe is only included in studies and Jamie only in stage. While in such tiny datasets this might seem like an avoidable oversight, such non-perfect alignment of data can be the norm when working with data spanning hundreds, thousands, or more rows. Here, for the purposes of illustration, we use small tables, but the principles we learn here apply in a broader context as well.\nThere are four commonly used ways of joining these tables into one single dataset. All of them follow the same general pattern: the arguments are two tibbles (or data frames) to be joined, plus a by = argument which lists the name(s) of the column(s) based on which the tables should be joined. The output is always a single tibble (data frame), containing some type of joining of the data. Let us now look at each joining method in detail.\n\n8.1.1 left_join\nThe left_join function keeps only those rows that appear in the first of the two tables to be joined:\n\nleft_join(studies, stage, by = \"name\")\n\n# A tibble: 3 × 3\n  name  subject    year\n  <chr> <chr>     <dbl>\n1 Sacha Physics       3\n2 Gabe  Chemistry    NA\n3 Alex  Biology       1\n\n\nThere are two things to notice. First, Jamie is missing from the name column above, even though s/he did appear in the stage tibble. This is exactly the point of left_join: if a row entry in the joining column (specified in by =) does not appear in the first table listed in the arguments (here, the name column of studies), then it is omitted. Second, the year entry for Gabe is NA. This is because Gabe is absent from the stage table, and therefore has no associated year of study. Rather than make up nonsense, R fills out such missing data with NA values.\n\n\n8.1.2 right_join\nThis function works just like left_join, except only those rows are retained which appear in the second of the two tables to be joined:\n\nright_join(studies, stage, by = \"name\")\n\n# A tibble: 3 × 3\n  name  subject  year\n  <chr> <chr>   <dbl>\n1 Sacha Physics     3\n2 Alex  Biology     1\n3 Jamie <NA>        2\n\n\nIn other words, this is exactly the same as calling left_join with its first two arguments reversed:\n\nleft_join(stage, studies, by = \"name\")\n\n# A tibble: 3 × 3\n  name   year subject\n  <chr> <dbl> <chr>  \n1 Sacha     3 Physics\n2 Alex      1 Biology\n3 Jamie     2 <NA>   \n\n\nThe only difference is in the ordering of the columns, but the data contained in the tables are identical.\nIn this case, the columns subject is NA for Jamie. The reason is the same as it was before: since the studies table has no name entry for Jamie, the corresponding subject area is filled in with a missing value NA.\n\n\n8.1.3 inner_join\nThis function retains only those rows which appear in both tables to be joined. For our example, since Gabe only appears in studies and Jamie only in stage, they will be dropped by inner_join and only Sacha and Alex are retained (since they appear in both tables):\n\ninner_join(studies, stage, by = \"name\")\n\n# A tibble: 2 × 3\n  name  subject  year\n  <chr> <chr>   <dbl>\n1 Sacha Physics     3\n2 Alex  Biology     1\n\n\n\n\n8.1.4 full_join\nThe complement to inner_join, this function retains all rows in all tables, filling in missing values with NAs everywhere:\n\nfull_join(studies, stage, by = \"name\")\n\n# A tibble: 4 × 3\n  name  subject    year\n  <chr> <chr>     <dbl>\n1 Sacha Physics       3\n2 Gabe  Chemistry    NA\n3 Alex  Biology       1\n4 Jamie <NA>          2\n\n\nA useful table summarizing these options, taken from a more comprehensive (though slightly out-of-date) cheat sheet, is below:\n\n(As you see, apart from the four so-called mutating joins we have learned about, there are also two filtering joins included in this cheat sheet as well. We will not be covering those here, but feel free to check out their help pages.)\n\n\n8.1.5 Joining by multiple columns\nIt is also possible to use the above joining functions specifying multiple columns to join data by. To illustrate how to do this and what this means, imagine that we slightly modify the student data. The first table will contain the name, study area, and year of study for each student. The second table will contain the name and study area of each student, plus whether they have passed their most recent exam:\n\nprogram  <- tibble(name     = c(\"Sacha\", \"Gabe\", \"Alex\"),\n                   subject  = c(\"Physics\", \"Chemistry\", \"Biology\"),\n                   year     = c(1, 3, 2))\nprint(program)\n\n# A tibble: 3 × 3\n  name  subject    year\n  <chr> <chr>     <dbl>\n1 Sacha Physics       1\n2 Gabe  Chemistry     3\n3 Alex  Biology       2\n\n\n\nprogress <- tibble(name     = c(\"Sacha\", \"Gabe\", \"Jamie\"),\n                   subject  = c(\"Physics\", \"Chemistry\", \"Biology\"),\n                   examPass = c(TRUE, FALSE, TRUE))\nprint(progress)\n\n# A tibble: 3 × 3\n  name  subject   examPass\n  <chr> <chr>     <lgl>   \n1 Sacha Physics   TRUE    \n2 Gabe  Chemistry FALSE   \n3 Jamie Biology   TRUE    \n\n\nAnd now, since the tables share not just one but two columns, it makes sense to join them using both. This can be done by specifying each column inside a vector in the by = argument. For example, left-joining program and progress by both name and subject leads to a joint table in which all unique name-subject combinations found in program are retained, but those found only in progress are discarded:\n\nleft_join(program, progress, by = c(\"name\", \"subject\"))\n\n# A tibble: 3 × 4\n  name  subject    year examPass\n  <chr> <chr>     <dbl> <lgl>   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n3 Alex  Biology       2 NA      \n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs mentioned, the columns by which one joins the tables must be included in a vector. Forgetting to do this will either fail or produce faulty output:\n\nleft_join(program, progress, by = \"name\", \"subject\")\n\n# A tibble: 3 × 5\n  name  subject.x  year subject.y examPass\n  <chr> <chr>     <dbl> <chr>     <lgl>   \n1 Sacha Physics       1 Physics   TRUE    \n2 Gabe  Chemistry     3 Chemistry FALSE   \n3 Alex  Biology       2 <NA>      NA      \n\n\nThe problem is that the comma after name is interpreted as the start of the next argument to left_join, instead of being a part of the by = argument. Again, the solution is simple: enclose multiple column names in a vector.\n\n\nThe other joining functions also work as expected:\n\nright_join(program, progress, by = c(\"name\", \"subject\"))\n\n# A tibble: 3 × 4\n  name  subject    year examPass\n  <chr> <chr>     <dbl> <lgl>   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n3 Jamie Biology      NA TRUE    \n\n\n\ninner_join(program, progress, by = c(\"name\", \"subject\"))\n\n# A tibble: 2 × 4\n  name  subject    year examPass\n  <chr> <chr>     <dbl> <lgl>   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n\n\n\nfull_join(program, progress, by = c(\"name\", \"subject\"))\n\n# A tibble: 4 × 4\n  name  subject    year examPass\n  <chr> <chr>     <dbl> <lgl>   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n3 Alex  Biology       2 NA      \n4 Jamie Biology      NA TRUE"
  },
  {
    "objectID": "Joining_data.html#binding-rows-and-columns-to-a-table",
    "href": "Joining_data.html#binding-rows-and-columns-to-a-table",
    "title": "8  Joining data",
    "section": "8.2 Binding rows and columns to a table",
    "text": "8.2 Binding rows and columns to a table\nOccasionally, a simpler problem presents itself: there is a single dataset, but its rows are contained across separate tables. For example, a table containing student names and subject areas might be spread across two tables, like this:\n\nstudies1 <- tibble(name    = c(\"Sacha\", \"Gabe\", \"Alex\"),\n                   subject = c(\"Physics\", \"Chemistry\", \"Biology\"))\nprint(studies1)\n\n# A tibble: 3 × 2\n  name  subject  \n  <chr> <chr>    \n1 Sacha Physics  \n2 Gabe  Chemistry\n3 Alex  Biology  \n\n\n\nstudies2 <- tibble(name    = c(\"Jamie\", \"Ashley\", \"Dallas\", \"Jordan\"),\n                   subject = c(\"Geology\", \"Mathematics\", \"Philosophy\", \"Physics\"))\nprint(studies2)\n\n# A tibble: 4 × 2\n  name   subject    \n  <chr>  <chr>      \n1 Jamie  Geology    \n2 Ashley Mathematics\n3 Dallas Philosophy \n4 Jordan Physics    \n\n\nThe tables have the exact same structure, in that the column names and types are identical. It’s just that the rows are, for some reason, disparate. To combine them together, we could recourse to full-joining the tables by both their columns:\n\nfull_join(studies1, studies2, by = c(\"name\", \"subject\"))\n\n# A tibble: 7 × 2\n  name   subject    \n  <chr>  <chr>      \n1 Sacha  Physics    \n2 Gabe   Chemistry  \n3 Alex   Biology    \n4 Jamie  Geology    \n5 Ashley Mathematics\n6 Dallas Philosophy \n7 Jordan Physics    \n\n\nThis, however, is not necessary. Whenever all we need to do is take two tables and stick their rows together, there is the simpler bind_rows:\n\nbind_rows(studies1, studies2)\n\n# A tibble: 7 × 2\n  name   subject    \n  <chr>  <chr>      \n1 Sacha  Physics    \n2 Gabe   Chemistry  \n3 Alex   Biology    \n4 Jamie  Geology    \n5 Ashley Mathematics\n6 Dallas Philosophy \n7 Jordan Physics    \n\n\nSimilarly, in case two tables have the same number of rows but different columns, one can stick their columns together using bind_cols. For example, suppose we have\n\nstudies <- tibble(name    = c(\"Sacha\", \"Gabe\", \"Alex\"),\n                  subject = c(\"Physics\", \"Chemistry\", \"Biology\"))\nprint(studies)\n\n# A tibble: 3 × 2\n  name  subject  \n  <chr> <chr>    \n1 Sacha Physics  \n2 Gabe  Chemistry\n3 Alex  Biology  \n\n\nas well as a table with year of study and result of last exam only:\n\nyearExam <- tibble(year     = c(3, 1, 2),\n                   examPass = c(FALSE, TRUE, TRUE))\nprint(yearExam)\n\n# A tibble: 3 × 2\n   year examPass\n  <dbl> <lgl>   \n1     3 FALSE   \n2     1 TRUE    \n3     2 TRUE    \n\n\nWe can now join these using bind_cols:\n\nbind_cols(studies, yearExam)\n\n# A tibble: 3 × 4\n  name  subject    year examPass\n  <chr> <chr>     <dbl> <lgl>   \n1 Sacha Physics       3 FALSE   \n2 Gabe  Chemistry     1 TRUE    \n3 Alex  Biology       2 TRUE"
  },
  {
    "objectID": "Joining_data.html#exercises",
    "href": "Joining_data.html#exercises",
    "title": "8  Joining data",
    "section": "8.3 Exercises",
    "text": "8.3 Exercises\nWe have used the data of Fauchald et al. (2017) before in other exercises. As a reminder, they tracked the population size of various herds of caribou in North America over time, and correlated population cycling with the amount of vegetation and sea-ice cover. Two files from their data are on Lisam: pop_size.tsv (herd population sizes), and sea_ice.tsv (sea ice cover per year and month).\n\nLoad these two datasets into two variables. They could be called pop and ice, for instance. Look at the data to familiarize yourself with them. How many rows and columns are in each?\nBefore doing anything else: how many rows will there be in the table that is the left join of pop and ice, based on the two columns Herd and Year? Perform the left join to see if you were correct. Where do you see NAs in the table, and why?\nNow do the same with right-joining, inner-joining, and full-joining pop and ice.\n\n\n\n\n\nFauchald, Per, Taejin Park, Hans Tømmervik, Ranga Myneni, and Vera Helene Hausner. 2017. “Arctic greening from warming promotes declines in caribou populations.” Science Advances 3 (4): e1601365. https://doi.org/10.1126/sciadv.1601365."
  },
  {
    "objectID": "Intro_statistics.html#sec-example_wilcox",
    "href": "Intro_statistics.html#sec-example_wilcox",
    "title": "9  Introducing statistical inference",
    "section": "9.1 Introductory example and the Wilcoxon test",
    "text": "9.1 Introductory example and the Wilcoxon test\nIn this chapter we will take a first look at statistical tests of the simplest kind: comparing two groups of data. To illustrate why statistical inference is needed in such comparisons, let us take a look at a fictive dataset which contains an equally fictive set of weight measurements of different bird individuals from the same species. The birds are assumed to come from two islands, a larger and a smaller one. The question is: do the data provide evidence of insular dwarfism—that is, the phenomenon that the body sizes of species tend to decline on small islands?\nWe can load the data:\n\nlibrary(tidyverse)\n\nbird <- read_csv(\"fictive_bird_example.csv\")\nprint(bird, n = Inf)\n\n# A tibble: 40 × 2\n   island  weight\n   <chr>    <dbl>\n 1 smaller  21.8 \n 2 smaller  19.0 \n 3 smaller  19.0 \n 4 smaller  21.2 \n 5 smaller  15.8 \n 6 smaller  14.3 \n 7 smaller  19.9 \n 8 smaller   9.68\n 9 smaller  17.0 \n10 smaller  12.7 \n11 smaller  22.0 \n12 smaller  15.7 \n13 smaller  16.7 \n14 smaller  27.9 \n15 smaller  17.4 \n16 smaller  12.6 \n17 smaller  33.4 \n18 smaller  25.8 \n19 smaller  20.1 \n20 smaller  21.3 \n21 larger   20.4 \n22 larger   28.3 \n23 larger   19.2 \n24 larger   24.1 \n25 larger   27.7 \n26 larger   16.1 \n27 larger   17.3 \n28 larger   21.2 \n29 larger   28.8 \n30 larger   21.9 \n31 larger   17.4 \n32 larger   13.0 \n33 larger   26.6 \n34 larger   27.0 \n35 larger   19.0 \n36 larger   25.2 \n37 larger   16.5 \n38 larger   25.2 \n39 larger   24.4 \n40 larger   24.7 \n\n\nA quick visualization below looks promising, with individuals on the smaller island indeed appearing to be smaller:\n\nggplot(bird) +\n  aes(x = island, y = weight) +\n  geom_boxplot(colour = \"steelblue\", fill = \"steelblue\",\n               alpha = 0.2, outlier.shape = NA) +\n  geom_jitter(alpha = 0.4, width = 0.05, colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the above plot, geom_jitter was used to display the actual data points that are summarized by the boxplots. The function geom_jitter is just like geom_point, except it adds a random sideways displacement to the data points, to reduce visual overlap between them. The width = 0.05 option restricts this convulsion of the points to a relatively narrow band. Since all data points are now displayed, it makes no sense to rely on the feature of box plots which explicitly draws points that are classified as outliers—their plotting is turned off by the outlier.shape = NA argument to geom_boxplot. It is a useful exercise to play around with these settings, to see what the effects of changing them are.\n\n\nFurthermore, the computed difference between the means and medians of the two samples are also clearly different:\n\nbird %>%\n  group_by(island) %>%\n  summarise(mean = mean(weight), median = median(weight))\n\n# A tibble: 2 × 3\n  island   mean median\n  <chr>   <dbl>  <dbl>\n1 larger   22.2   23.0\n2 smaller  19.2   19.0\n\n\nCan we conclude that the two samples are indeed different, and birds on the smaller island tend to be smaller, supporting the insular dwarfism hypothesis? As mentioned above, the data are fictive—they are not based on actual measurements. In fact, these “observations” were created by sampling each data point from the same distribution, regardless of island: a normal distribution with mean 20 and standard deviation 5. This means that any supposedly observed difference between the samples must be accidental. Would various statistical methods detect a difference? One thing we can do is what we have already done in Section 6.1: compute the 95% confidence intervals of the means, and see whether and how much they overlap.\n\nbird %>%\n  group_by(island) %>%\n  summarise(mean = mean(weight), # Means of the two groups\n            sd = sd(weight), # Standard deviations\n            N = n(), # Sample sizes\n            SEM = sd / sqrt(N), # Standard errors of the means\n            CI = qt(0.975, N - 1)) %>% # Confidence intervals\n  ungroup() %>% # Ungroup the data (not necessary here, but a good habit)\n  ggplot(aes(x = island, y = mean, ymin = mean - CI, ymax = mean + CI)) +\n  geom_point(colour = \"steelblue\") +\n  geom_errorbar(colour = \"steelblue\", width = 0.2) +\n  ylab(\"mean and 95% confidence interval\") +\n  theme_bw()\n\n\n\n\nAs seen, there is overlap between the 95% confidence intervals. While that by itself is not conclusive, it is an indication that the difference between the two samples may not be as relevant as it might have initially looked.\nLet us wait no longer, and perform a statistical test. One widely used test to check if two samples differ from one another is the Wilcoxon test (also known as the Mann-Whitney test). Its implementation is very simple:\n\nwilcox.test(weight ~ island, data = bird)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 271, p-value = 0.05589\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe function takes two arguments: a formula, and the data to be analyzed, in the form of a data frame or tibble. The formula in the first argument establishes a relationship between two (or more) columns of the data. We will discuss formulas and their syntax in more detail later. For now: the way to write them is to first type the variable we wish to predict or explain, then a tilde (~), and then the explanatory variable (predictor) by which the data are subdivided into the two groups. In our case, we are trying to explain the difference in weight between the islands, so weight comes before the tilde and the predictor island comes after.\n\n\n\n\n\n\nNote\n\n\n\nWe have used the tilde (~) before, when faceting plots with facet_grid; see Section 7.4. However, the tilde used in facet_grid has nothing to do with the one we use in formulas: the two symbols happen to be identical, but that is just an accident.\n\n\nLet us now look at the output produced by wilcox.test above. Most of it is not particularly relevant for us: we are first informed that a Wilcoxon rank sum exact test is being performed; then we see that we are explaining weight differences by island; then we see the test statistic W itself (we need not concern ourselves with its precise meaning); then the p-value; and finally, a reminder of what the alternative hypothesis is (the null hypothesis is that the shift in location is in fact zero).\nOne piece of datum we are interested in is the p-value. This tells us the probability of observing a result at least as extreme as we see in our data, given that the null hypothesis is true. In our case, we are measuring how different the two samples are from one another, and the null hypothesis is that the only difference we might observe is due purely to chance. We find that the probability of this being the case, given our data, is 0.056. What this number means is that we have a roughly one-in-eighteen chance that the observed difference is nothing but a fluke. Since science leans towards erring on the side of caution (i.e., we would rather miss out on making a discovery than falsely claim having seen an effect), this value is in general a bit too high for comfort. And indeed: since in this case we know that the data were generated by sampling from the same distribution, any distinctiveness between them is incidental.\n\n\n\n\n\n\nNote\n\n\n\nIn many subfields of science, it is standard practice to consider p-values falling below 0.05 as “significant” and those falling above as “non-significant”. Besides the fact that such a one-size-fits-all approach ought to be suspect even under the best of circumstances, a significance threshold of 0.05 is awfully permissive to errors. In fact, we should expect about one out of twenty of all papers ever published which have adopted this cutoff to be wrong! Digging deeper into this issue reveals that the figure is possibly much worse—see, e.g., Colquhoun (2014). One way to ameliorate the problem is to adopt a less exclusive and parochial view of p-values. Instead of having rigid significance thresholds, p-values can simply be reported and interpreted for what they are: the probability that the outcome is at least as extreme as observed, assuming that the null model holds. Treating this information as just one piece of the data puzzle is a first step towards avoiding the erroneous classification of random patterns as results.\n\n\nApart from the p-value (is the observed effect likely to be due to chance alone?), another important piece of information is some measure of the effect size: how different are the two samples? We have already computed the means and medians; their differences across the islands provide one way of measuring this effect size. It is possible to add a calculation of the effect size, as well as the confidence intervals, to the Wilcoxon test. All one needs to do is pass conf.int = TRUE as an argument:\n\nwilcox.test(weight ~ island, data = bird, conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 271, p-value = 0.05589\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -0.018386  6.820811\nsample estimates:\ndifference in location \n              3.436204 \n\n\nAs additional output, we now receive the 95% confidence interval, as well as the explicit difference between the “locations” of the two samples. (A small word of caution: this “difference in location” is neither the difference of the means nor the difference of the medians, but the median of the difference between samples from the two groups of data—feel free to check the help pages by typing ?wilcox.test for more details.) The confidence interval, as seen, includes zero, which can be interpreted as being difficult to rule out the possibility that the observed difference in location is just due to chance.\nThe default confidence level of 95% can be changed via the conf.level argument. For example, to use a 99% confidence interval instead:\n\nwilcox.test(weight ~ island, data = bird, conf.int = TRUE, conf.level = 0.99)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 271, p-value = 0.05589\nalternative hypothesis: true location shift is not equal to 0\n99 percent confidence interval:\n -1.16832  7.70439\nsample estimates:\ndifference in location \n              3.436204 \n\n\nIn summary, the Wilcoxon test failed to yield serious evidence in favor of rejecting the null hypothesis. Therefore, we cannot claim with any confidence that our fictive birds have different sizes across the two islands. While failure to reject the null is not the same as confirming that the null is true (absence of evidence is not evidence of absence!), the notion that the two samples are different could not be supported. In this particular case, since we ourselves have created the original data using the null hypothesis, we have the privilege of knowing that this is the truth. When working with real data, such knowledge is generally not available."
  },
  {
    "objectID": "Intro_statistics.html#some-general-conclusions",
    "href": "Intro_statistics.html#some-general-conclusions",
    "title": "9  Introducing statistical inference",
    "section": "9.2 Some general conclusions",
    "text": "9.2 Some general conclusions\nThe example of Section 9.1 illustrates two important general points.\nFirst, instead of jumping into statistical tests, we started the analysis with qualitative and descriptive data exploration: we plotted the data, computed its means and medians, etc. This is almost always the correct way to go. (The exception is when one analyzes data from a pre-registered experimental design. In that case, one must follow whatever statistical techniques were agreed upon before data collection even started.) To perform tests “blindly”, without visually exploring the data first, is rarely a good idea.\nSecond, we only performed the statistical test after we have made an effort to understand the data, and after setting clear expectations about what we might find. We knew, going into the test, that finding a difference between the two island samples was questionable. And indeed, the test revealed that such a distinction cannot be made in good conscience. Following a similar strategy for all statistical inference can prevent a lot of frustration. To make the point more sharply: do not perform a statistical test without knowing what its result will be! A more nuanced way of saying the same thing is that if you think you see a relationship in your data, then you should also make sure that your observation is not just a mirage, by using a statistical test. But if a relationship is not visually evident, you should first ask the question whether it makes much sense to try to explore it statistically."
  },
  {
    "objectID": "Intro_statistics.html#parametric-versus-non-parametric-tests",
    "href": "Intro_statistics.html#parametric-versus-non-parametric-tests",
    "title": "9  Introducing statistical inference",
    "section": "9.3 Parametric versus non-parametric tests",
    "text": "9.3 Parametric versus non-parametric tests\nThe Wilcoxon test we employed is non-parametric: it does not assume either the data or the residuals to come from any particular distribution. This is both the test’s advantage and disadvantage. It is an advantage because fewer assumptions must be satisfied for the test to be applicable. Non-parametric techniques also tend to fare better when only limited data are available. Since in many areas of biology (such as ecology) data are hard to come by, datasets are correspondingly small, making non-parametric techniques a natural candidate for interrogating the data with. On the downside, non-parametric tests tend to be less powerful than parametric ones: the additional assumptions of parametric tests (when they are actually met by real data) may allow for sharper inference.\nOne of the most common parametric alternatives to the Wilcoxon test is Welch’s t-test. Like the Wilcoxon test, the t-test assumes the independence of the two samples. In addition, it also assumes that the samples are normally distributed. Let us see what the t-test says about our fictive bird data:\n\nt.test(weight ~ island, data = bird)\n\n\n    Welch Two Sample t-test\n\ndata:  weight by island\nt = 1.8704, df = 36.92, p-value = 0.06937\nalternative hypothesis: true difference in means between group larger and group smaller is not equal to 0\n95 percent confidence interval:\n -0.2520327  6.2997175\nsample estimates:\n mean in group larger mean in group smaller \n             22.18479              19.16095 \n\n\nThe format of the output is similar to the one with the Wilcoxon test; the main differences are that the confidence intervals are included by default, and that the observed means of the two samples are also reported. Like with wilcox.test, the confidence level can be adjusted through the argument conf.level:\n\nt.test(weight ~ island, data = bird, conf.level = 0.99)\n\n\n    Welch Two Sample t-test\n\ndata:  weight by island\nt = 1.8704, df = 36.92, p-value = 0.06937\nalternative hypothesis: true difference in means between group larger and group smaller is not equal to 0\n99 percent confidence interval:\n -1.366524  7.414209\nsample estimates:\n mean in group larger mean in group smaller \n             22.18479              19.16095 \n\n\nImportantly, the message conveyed by the p-value of 0.069 is in line with what we saw from the Wilcoxon test: there is no strong evidence to claim that the two samples are different. In this particular case, it does not make any qualitative difference whether one uses the parametric t-test or the non-parametric Wilcoxon test.\nIn other situations, the choice of test may matter more. The following data have also been artificially created as an example. They contain two groups of “measurements”. In the first group (group x), the data have been sampled from a lognormal distribution with mean 0 and standard deviation 1. In the second group (group y), the data have been sampled from a lognormal distribution with mean 1 and standard deviation 1. Thus, we know a priori that the two distributions from which the samples were created are shifted compared to one another, and a statistical test may be able to reveal this shift. However, since the data are heavily non-normal, a t-test will struggle to do so. Let us load and visualize the data first:\n\nexample <- read_csv(\"t_wilcox_example.csv\")\n\nggplot(example, aes(x = measurement)) +\n  geom_histogram(bins = 25, alpha = 0.3,\n                 colour = \"steelblue\", fill = \"steelblue\") +\n  facet_wrap(~ group, labeller = label_both) +\n  theme_bw()\n\n\n\n\nApplying a t-test first:\n\nt.test(measurement ~ group, data = example)\n\n\n    Welch Two Sample t-test\n\ndata:  measurement by group\nt = -1.4796, df = 19.622, p-value = 0.1549\nalternative hypothesis: true difference in means between group x and group y is not equal to 0\n95 percent confidence interval:\n -19.853561   3.388384\nsample estimates:\nmean in group x mean in group y \n        2.33571        10.56830 \n\n\nThe parametric t-test cannot detect a difference between the samples—though, since its assumption of normality is violated, such a test should not have been attempted in the first place. By contrast, the non-parametric Wilcoxon test (correctly) suggests that there might be a difference:\n\nwilcox.test(measurement ~ group, data = example, conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  measurement by group\nW = 106, p-value = 0.01031\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -3.039534 -0.378063\nsample estimates:\ndifference in location \n             -1.798516 \n\n\nThe p-value of 0.01 means that there is a one-in-a-hundred probability that the observed difference is due only to chance."
  },
  {
    "objectID": "Intro_statistics.html#statistical-tests-in-a-data-analysis-pipeline-the-dot-notation",
    "href": "Intro_statistics.html#statistical-tests-in-a-data-analysis-pipeline-the-dot-notation",
    "title": "9  Introducing statistical inference",
    "section": "9.4 Statistical tests in a data analysis pipeline: The dot notation",
    "text": "9.4 Statistical tests in a data analysis pipeline: The dot notation\nOne natural question is whether statistical tests can be included in a data analysis pipeline. The answer is yes, but not without having to consider some complications that are due to historical accidents. Attempting the following results in an error:\n\nbird %>%\n  wilcox.test(weight ~ island, conf.int = TRUE)\n\nError in wilcox.test.default(., weight ~ island, conf.int = TRUE): 'x' must be numeric\n\n\nThe reason for the error is in how the pipe operator %>% works: it substitutes the expression on the left of the pipe into the first argument of the expression on the right. This works splendidly with functions of the tidyverse, since these are designed to always take the data as their first argument. However, a quick glance at the documentation of both wilcox.test and t.test (neither of which are tidyverse functions) reveals that they take the data as their second argument, with the formula coming first. A naive application of the pipe operator, like above, will therefore fail.\nFortunately, there is a way to work around this. One may use the dot (.) in an expression to stand for whatever was being piped into it. The following will therefore work:\n\nbird %>%\n  wilcox.test(weight ~ island, data = ., conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 271, p-value = 0.05589\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -0.018386  6.820811\nsample estimates:\ndifference in location \n              3.436204 \n\n\nThe dot stands for the object being piped in—which, in our case, is simply the tibble bird. The data = . argument therefore evaluates to data = bird.\nThis ought to conclude all there is to using the dot notation, except that there are a few subtleties involved. As we know, the pipe substitutes the object on the left as the first argument of the object on the right. In the above expression, this behavior was overridden: by using the dot, the data were substituted as the second argument instead. This will be the case as long as the dot appears alone, without being nested into some other function call. However, if the dot is part of another function, then the expression to the left of the pipe will additionally be substituted, silently, as the first argument on the right. For example, suppose that we want to remove rows with missing data from the table first, before doing a statistical test. We can do this with the drop_na function. (Since the bird table does not have any rows with NA values, this does not actually do anything in this context, but it is easy to imagine data where some rows do have missing entries, in which case removing them first could be useful.) But then, the following piece of code will fail:\n\nbird %>%\n  wilcox.test(weight ~ island, data = drop_na(.), conf.int = TRUE)\n\nError in wilcox.test.default(., weight ~ island, data = drop_na(.), conf.int = TRUE): 'x' must be numeric\n\n\nHere the dot appears as part of the drop_na function. Thus, bird is not just substituted where the dot is, but also silently as the first function argument to wilcox.test. Which then makes the function think that bird is the formula, and weight ~ island is the data—consequently, we get an error.\nIf necessary, it is possible to override this behavior, by putting curly braces around the expression(s) into which we are piping. The curly braces always prevent the silent substitution of the piped expression, which will therefore only be substituted wherever the dot explicitly appears. The above, faulty code can therefore be fixed by using curly braces:\n\nbird %>%\n  { wilcox.test(weight ~ island, data = drop_na(.), conf.int = TRUE) }\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 271, p-value = 0.05589\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -0.018386  6.820811\nsample estimates:\ndifference in location \n              3.436204 \n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile this behavior of the dot notation may appear arbitrary, there is method to the madness. In most situations, including the dot within another function indeed means that we would like the dot to also be passed as the first argument. For example, we could add row numbers to the bird tibble, by creating a new column whose entries range from one to the number of columns in the data. The number of columns can be obtained with the nrow function. Therefore, the following is possible:\n\nbird %>%\n  mutate(rowID = 1:nrow(.)) %>% # Create new column, numbering the rows\n  select(rowID, island, weight) # Reorder the columns so rowID is first\n\n# A tibble: 40 × 3\n   rowID island  weight\n   <int> <chr>    <dbl>\n 1     1 smaller  21.8 \n 2     2 smaller  19.0 \n 3     3 smaller  19.0 \n 4     4 smaller  21.2 \n 5     5 smaller  15.8 \n 6     6 smaller  14.3 \n 7     7 smaller  19.9 \n 8     8 smaller   9.68\n 9     9 smaller  17.0 \n10    10 smaller  12.7 \n# … with 30 more rows\n\n\nIn this case, the first argument to mutate, as always, is the piped-in tibble. Due to the convention that the dot gets silently substituted as the first argument as well (because the dot appeared within a function call), we did not need to write mutate(., rowID = 1:nrow(.)) above in the second line. Thus, the seemingly arbitrary rules do lead to less awkward code, at least some of the time."
  },
  {
    "objectID": "Intro_statistics.html#exercises",
    "href": "Intro_statistics.html#exercises",
    "title": "9  Introducing statistical inference",
    "section": "9.5 Exercises",
    "text": "9.5 Exercises\n\nAtmospheric ozone levels are important for urban gardens, because ozone concentrations above 80 parts per hundred million can damage lettuce plants. The file ozone.csv contains measured ozone concentrations in gardens distributed around a city. The gardens were either east or west of the city center. Is there a difference in average ozone concentration between eastern vs. western gardens? Create a plot of the concentrations first and try to guess the answer, before performing any statistical analyses. Then do the statistics, using both nonparametric and parametric methods. What result do you find? Do the outcomes of the different methods agree with one another?\nIn the built-in iris dataset, the sepal and petal characteristics of the various species differ. Let us focus on just petal length here. Is there a detectable difference between the petal lengths of Iris versicolor and I. virginica? (Since we are not working with I. setosa in this exercise, make sure to remove them from the data first—e.g., by using filter.) Like before, start with a graph which you can base a hypothesis on. Then perform both nonparametric and parametric tests to see if your hypothesis can be supported.\nRepeat the above exercise for any and all of the possible trait-species pair combinations in the iris dataset. E.g., one can compare the sepal widths of I. setosa and I. virginica, or the petal widths of I. versicolor and I. setosa, and so on.\n\n\n\n\n\nColquhoun, David. 2014. “An investigation of the false discovery rate and the misinterpretation of p-values.” Royal Society Open Science 1 (3): 140216. https://doi.org/10.1098/rsos.140216."
  },
  {
    "objectID": "Anova.html#the-kruskalwallis-and-dunn-tests",
    "href": "Anova.html#the-kruskalwallis-and-dunn-tests",
    "title": "10  The Kruskal–Wallis test and one-way ANOVA",
    "section": "10.1 The Kruskal–Wallis and Dunn tests",
    "text": "10.1 The Kruskal–Wallis and Dunn tests\nThe tools of Chapter 9 allow us to compare two groups of data. But what do we do when we have more than two groups? For example, we might wish to know how different treatments influence plant growth, as measured by dry weight. If there are two treatments, then there will be three (instead of two) groups, because the treatments will be compared with a control group which does not receive treatment.\nSuch a dataset is, in fact, built into R, and is called PlantGrowth. Here it is:\n\nlibrary(tidyverse)\n\nPlantGrowth %>% as_tibble() %>% print(n = Inf)\n\n# A tibble: 30 × 2\n   weight group\n    <dbl> <fct>\n 1   4.17 ctrl \n 2   5.58 ctrl \n 3   5.18 ctrl \n 4   6.11 ctrl \n 5   4.5  ctrl \n 6   4.61 ctrl \n 7   5.17 ctrl \n 8   4.53 ctrl \n 9   5.33 ctrl \n10   5.14 ctrl \n11   4.81 trt1 \n12   4.17 trt1 \n13   4.41 trt1 \n14   3.59 trt1 \n15   5.87 trt1 \n16   3.83 trt1 \n17   6.03 trt1 \n18   4.89 trt1 \n19   4.32 trt1 \n20   4.69 trt1 \n21   6.31 trt2 \n22   5.12 trt2 \n23   5.54 trt2 \n24   5.5  trt2 \n25   5.37 trt2 \n26   5.29 trt2 \n27   4.92 trt2 \n28   6.15 trt2 \n29   5.8  trt2 \n30   5.26 trt2 \n\n\n(The PlantGrowth data are in the form of a data frame instead of a tibble; hence we convert it above by using as_tibble.) We see that each group consists of ten observations, and we have three groups: the control (ctrl) treatment 1 (trt1), and treatment 2 (trt2). As usual, we visualize the data first, before doing any tests:\n\nggplot(PlantGrowth) +\n  aes(x = group, y = weight) +\n  geom_boxplot(colour = \"steelblue\", fill = \"steelblue\",\n               alpha = 0.2, outlier.shape = NA) +\n  geom_jitter(alpha = 0.4, width = 0.05, colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\nBased on the figure, it is reasonable to expect there to be a real difference between treatment 1 and treatment 2. But whether there is good reason to think that the control truly differs from any of the treatments is unclear.\nOne way of finding out could be to perform pairwise Wilcoxon tests between each pair of groups. This, in fact, would be quite feasible here, because there are only three comparisons to be made (control vs. treatment 1; control vs. treatment 2, and treatment 1 vs. treatment 2). Here are the results of doing this:\n\nas_tibble(PlantGrowth) %>% # The PlantGrowth dataset, converted to a tibble\n  filter(group != \"trt2\") %>% # Keep only the control and treatment 1\n  wilcox.test(weight ~ group, data = ., conf.int = TRUE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  weight by group\nW = 67.5, p-value = 0.1986\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -0.2899731  1.0100554\nsample estimates:\ndifference in location \n             0.4213948 \n\nas_tibble(PlantGrowth) %>%\n  filter(group != \"trt1\") %>% # Keep only the control and treatment 2\n  wilcox.test(weight ~ group, data = ., conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 25, p-value = 0.06301\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -1.00  0.04\nsample estimates:\ndifference in location \n                 -0.49 \n\nas_tibble(PlantGrowth) %>%\n  filter(group != \"ctrl\") %>% # Keep only treatments 1 and 2\n  wilcox.test(weight ~ group, data = ., conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -1.50 -0.31\nsample estimates:\ndifference in location \n                -0.945 \n\n\nAs expected, the difference between the control and the two treatments is not particularly credible (especially in the case of treatment 1), but there is good indication that the difference between the two treatments is not just due to chance.\nWhile this method of using repeated Wilcoxon tests worked fine above, there are some problems with this approach. One is that it can quickly get out of hand, because having n groups means there will be n · (n - 1) / 2 pairs to consider. For instance, if the number of groups is 12 (not a particularly large number), then there are 66 unique pairs already. It would not be pleasant to perform this many tests, even if any single test is quite simple to run.\nThe other problem has to do with multiple testing and its influence on the interpretation of p-values. Very simply put, the problem is that if sufficiently many groups are compared, then we might find at least one pair with a low p-value—not because the null hypothesis is false, but because across a large number of observations some p-values might turn out lower than others just by chance. Remember: p-values measure, in effect, the probability that the observed effect is too stark to be due to simple coincidence. But if we create sufficiently many opportunities for such a coincidence to arise, then of course one eventually will. One of the best explanations of this point is in the following cartoon by xkcd:\n\nFortunately, there is no need to rely on repeated Wilcoxon tests when there are more than two groups, as other methods are available. One such method is the Kruskal–Wallis test, which can be used with any number of groups as long as those groups vary within a single factor. (In Chapter 11 we will see examples where multiple independent factors are varied, and each possible combination results in a separate group. For example, if the effects of three different dosages of vitamin C are examined on the tooth growth of Guinea pigs, and the vitamin is also supplied in two distinct forms of either orange juice or raw ascorbic acid, then there will be 3 × 2 = 6 groups, defined by the two factors of dosage and form of supplement.) The Kruskal–Wallis test is non-parametric, and therefore does not rely on assumptions such as the normality of the residuals. Its application is precisely analogous to how the Wilcoxon test is implemented. Therefore, to perform the test on the PlantGrowth data:\n\nkruskal.test(weight ~ group, data = PlantGrowth)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  weight by group\nKruskal-Wallis chi-squared = 7.9882, df = 2, p-value = 0.01842\n\n\nThe null hypothesis of the Kruskal–Wallis test is that the observations from all groups were sampled from the same underlying distribution—that is, that there are no differences between the groups other than those attributed to random noise. Consequently, when the p-value is low (like above), this means that it is unlikely that the data in all groups come from the same distribution, and thus that at least one group differs from the others.\nWhile that is something, we want more: an actual comparison of the groups (like we have done with our repeated Wilcoxon tests). The way forward is to perform a post hoc (Latin “after this”) test. In this case, the Dunn test is one such post hoc test. This test is implemented in R, but not in any of the basic packages. To use it, one must first install the FSA package:\n\ninstall.packages(\"FSA\")\n\nOnce it is installed, the package should be loaded:\n\nlibrary(FSA)\n\nAnd then, the Dunn test (dunnTest) follows the same syntax as wilcox.test or kruskal.test:\n\ndunnTest(weight ~ group, data = PlantGrowth)\n\nDunn (1964) Kruskal-Wallis multiple comparison\n\n\n  p-values adjusted with the Holm method.\n\n\n   Comparison         Z    P.unadj      P.adj\n1 ctrl - trt1  1.117725 0.26368427 0.26368427\n2 ctrl - trt2 -1.689290 0.09116394 0.18232789\n3 trt1 - trt2 -2.807015 0.00500029 0.01500087\n\n\nThe table above is the output of the test, and has four columns. The first column shows which two groups are being compared. The next column, called Z, is the value of the test statistic, which we need not concern ourselves with here. Next, we have the unadjusted p-values; and finally, the adjusted p-values (P.adj), which have been corrected to account for the multiple testing problem mentioned above. Therefore, the adjusted p-values will always be as large or larger than the unadjusted ones.\nWhat has the Dunn test revealed? Precisely what we have been suspecting: that the only difference worth noting is the one between the two treatments (last row of the table)."
  },
  {
    "objectID": "Anova.html#one-way-anova",
    "href": "Anova.html#one-way-anova",
    "title": "10  The Kruskal–Wallis test and one-way ANOVA",
    "section": "10.2 One-way ANOVA",
    "text": "10.2 One-way ANOVA\nThe Kruskal–Wallis test is the non-parametric analogue of the one-way ANOVA (ANalysis Of VAriance). The “one-way” in the name refers to the property that there is a single factor indexing the groups, as discussed earlier. ANOVA relies on assumptions such as normality of the residuals and having the same number of observations in each group (balanced design). Otherwise, it is much like the Kruskal–Wallis test. The function in R that performs an ANOVA is lm (short for “linear model”):\n\nlm(weight ~ group, data = PlantGrowth)\n\n\nCall:\nlm(formula = weight ~ group, data = PlantGrowth)\n\nCoefficients:\n(Intercept)    grouptrt1    grouptrt2  \n      5.032       -0.371        0.494  \n\n\nThe output is not particularly informative: it simply lists the regression coefficients without the requisite information to see whether those coefficients actually mean anything. To get more, one can pass the result of the lm function to one of two other functions: either anova or summary. The former generates an ANOVA or sum-of-squares table (thus, despite its name, it does not actually perform the anova, just formats its output in a specific way). In this table, each factor indexing the groups, as well as their interactions (if present), get one row:\n\nlm(weight ~ group, data = PlantGrowth) %>% anova()\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value  Pr(>F)  \ngroup      2  3.7663  1.8832  4.8461 0.01591 *\nResiduals 27 10.4921  0.3886                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value above (in the row belonging to the factor group, under the column Pr(>F)) is the analogue of the p-value calculated by the Kruskal–Wallis test (which was 0.01842). We can see that the two tests agree qualitatively.\nThe latter, summary function focuses on the regression coefficients instead, listing all of them in a table, alongside other information:\n\nlm(weight ~ group, data = PlantGrowth) %>% summary()\n\n\nCall:\nlm(formula = weight ~ group, data = PlantGrowth)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0710 -0.4180 -0.0060  0.2627  1.3690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   5.0320     0.1971  25.527   <2e-16 ***\ngrouptrt1    -0.3710     0.2788  -1.331   0.1944    \ngrouptrt2     0.4940     0.2788   1.772   0.0877 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6234 on 27 degrees of freedom\nMultiple R-squared:  0.2641,    Adjusted R-squared:  0.2096 \nF-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\n\nThe output first prints the function call to lm we used. Then it gives a quick overview of the residuals: the minimum and maximum values, the point of the first and third quantiles, and the median—in other words, it contains the same information one would use to create a box plot with. This ought to give a quick and rough idea of whether the residuals are violently skewed, or have at least a chance of being normally distributed, which is a key assumption behind ANOVA. (We will discuss a better method for assessing the normality of the residuals in Section 10.3.) The next item in the output is the table of fitted regression coefficients. Here we can find the estimated values of the coefficients, their standard error, the associated t-statistic, and the p-values (the Pr(>|t|) column). Here the p-values refer to the likelihood that the observed difference from zero is due to chance.\nTo interpret the coefficients, note that R computes them relative to some reference group—which, unless specified otherwise, is the factor that comes first in the alphabet. From ctrl, trt1, and trt2, it is ctrl that comes first, and therefore the control serves as the baseline. The row called (Intercept) tells us the estimated value made by the regression, assuming that all non-baseline variables are set to zero. In this context, this means that both trt1 and trt2 are zeroed out, and therefore (Intercept) simply measures the average value within the group ctrl. The estimates in the rows grouptrt1 and grouptrt2 (these names were created by mushing together the name of the column in which the factor is found with the factors’ names themselves) are the deviations from the baseline provided by (Intercept)—that is, from the mean within the control. So we can surmise that the estimated mean of trt1 is 5.032 - 0.371 = 4.661, and that of trt2 is 5.032 + 0.494 = 5.526. Indeed, this is what we find relying on simple summaries computing the means in each group:\n\n\n\n\nPlantGrowth %>%\n  group_by(group) %>%\n  summarise(meanWeight = mean(weight)) %>%\n  ungroup() # Not necessary here, strictly speaking, but does not hurt\n\n# A tibble: 3 × 2\n  group meanWeight\n  <fct>      <dbl>\n1 ctrl       5.032\n2 trt1       4.661\n3 trt2       5.526\n\n\n\n\n\nAfter the table of regression coefficients, we get some information on the residual standard error, degrees of freedom, the coefficient of determination R2 (a value between 0 and 1; the closer it is to 1, the more tightly the data fit the model), the adjusted R2 (which is like the regular R2 but with a penalty for using too many fitting parameters), the F-statistic, and the overall p-value—this is the same as the one in the ANOVA table earlier, created with the anova function."
  },
  {
    "objectID": "Anova.html#sec-diagnostics",
    "href": "Anova.html#sec-diagnostics",
    "title": "10  The Kruskal–Wallis test and one-way ANOVA",
    "section": "10.3 Diagnostic plots",
    "text": "10.3 Diagnostic plots\nSince ANOVA is a parametric method relying on equal variances in each group and the normality of the residuals, it is important to check whether these assumptions in fact hold. There is a convenient way to do so, via diagnostic plots. Such plots can be created via the autoplot function of the ggfortify package. We can therefore install this package first, in case we do not have it already:\n\ninstall.packages(\"ggfortify\")\n\nAnd then load it:\n\nlibrary(ggfortify)\n\nAnd now, we can simply give the result of the lm function as input to the function autoplot:\n\nlm(weight ~ group, data = PlantGrowth) %>% autoplot()\n\n\n\n\nWe see four plots above. The top left of these shows the residuals against the fitted values (notice that the three values along the x-axis correspond to the means of the three groups ctrl, trt1, and trt2). If the points have no trend of increase or decrease, and the spread of the points is roughly constant, then they adhere to ANOVA assumptions. The blue line is a moving regression line which helps decide whether there is a trend; in this case, it would be hard to argue that any trend is due to something else than chance.\nThe bottom left plot is much the same as the top left one, except that it takes the absolute values of the residuals. This is done because, since residuals will by definition be more or less symmetrically distributed around zero, one can effectively double the precision of the diagnostic by focusing only on magnitudes. For statistical reasons that do not concern us here, the square roots of these absolute values tend to behave much better, which is why we see the square root being taken along the y-axis. The blue line is again a locally-weighted estimate, which ought to be as flat as possible. In this case, we see that it more or less is.\nThe top right graph offers a visualization of how well the residuals follow a normal distribution. The idea behind this quantile-quantile plot (Q–Q plot) is that if the residuals are indeed normally distributed, then we can line them up in increasing order along the x-axis, and for each of them, plot the theoretically expected value (based on normality) along the y-axis. If these observed vs. theoretically predicted values fall on the identity line, then there is a perfect match between theory and observation, and the residuals are normally distributed. The stronger the deviation from the dashed line indicating a perfect theoretical match,1 the more non-normal the residuals are.\nThe bottom right graph measures the “leverage” of each point, which is a measure of how sensitively the regression reacts to removing one data point. We will not be concerned with this plot.\nIn fact, we can remove this graph from the diagnostic plots. We may even remove the top left one, since the bottom left carries the same information in a better way. Further, we can also remove the blue lines, which often do more to confuse than to help. (For example, one can see a slight downward trend in the blue line in the bottom left graph. Looking at all points together reveals how little this trend means.) To select only the bottom-left and top-right diagnostic plots, one can pass the argument which = 3:2 to autoplot, which chooses just the third and second of the graphs (in that order). In turn, the smooth.colour = NA argument will remove the blue lines:\n\nlm(weight ~ group, data = PlantGrowth) %>%\n  autoplot(which = 3:2, smooth.colour = NA)\n\n\n\n\nOne may can go even further—since autoplot returns a standard ggplot object, we can add various theme options:\n\nlm(weight ~ group, data = PlantGrowth) %>%\n  autoplot(which=3:2, smooth.colour=NA, colour=\"steelblue\", alpha=0.7) +\n  theme_bw()"
  },
  {
    "objectID": "Anova.html#sec-tukey",
    "href": "Anova.html#sec-tukey",
    "title": "10  The Kruskal–Wallis test and one-way ANOVA",
    "section": "10.4 Tukey’s honest significant differences",
    "text": "10.4 Tukey’s honest significant differences\nJust as the Dunn test is a post-hoc test for the Kruskal–Wallis test, the Tukey test (or Tukey’s honest significant test) is a post-hoc test for an ANOVA. The corresponding function in R is called TukeyHSD, and it should be applied to the result of the lm function—with one caveat. The Tukey test requires the output of the ANOVA to be in a particular format. Therefore, before passing the result of lm to TukeyHSD, we first have to pass it to a helper function called aov (“analysis of variance”). Like this:\n\nlm(weight ~ group, data = PlantGrowth) %>% aov() %>% TukeyHSD()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = .)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\n\nThe table is similar to the one produced by the Dunn test, with a few differences. The first column tells us which groups of data are compared. The second column is the raw difference between the group means, the third and fourth are the lower and upper limits of a 95% confidence interval for the difference, and the last one are the p-values (adjusted for multiple comparisons). In this case, the results from the Dunn test and the Tukey test are in agreement: only the difference between the two control groups stands a reasonable chance of being real. The fact that multiple methods lead to the same conclusion increases the trust we can place in their results being correct.\nIn case using the aov function as an intermediary feels like a hassle, one can easily make life simpler, by writing a helper function which automatically calls it, together with the Tukey test:\n\ntukey.test <- function(lmFit) {\n  lmFit %>% aov() %>% TukeyHSD()\n}\n\nHere lmFit is a model fit object returned by the lm function. Using this, we can rewrite the above more simply:\n\nlm(weight ~ group, data = PlantGrowth) %>% tukey.test()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = .)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064"
  },
  {
    "objectID": "Anova.html#exercises",
    "href": "Anova.html#exercises",
    "title": "10  The Kruskal–Wallis test and one-way ANOVA",
    "section": "10.5 Exercises",
    "text": "10.5 Exercises\n\nThe file daphnia_growth.csv contains data on the growth rate of Daphnia populations that are infected with various parasites. There are four groups of observations: the control (no parasites), infection with Metschnikowia bicuspidata, infection with Pansporella perplexa, and finally, infection with Pasteuria ramosa. Each group has ten replicate observations. Are growth rates affected by parasite load?\n\nBefore doing any tests, visualize and explore the data, and make sure you have a solid expectation for the results of any statistical analysis.\nAnswer the question whether growth rates affected by parasite load by first applying a non-parametric test (and a subsequent non-parametric post-hoc test if needed).\nNext, apply a parametric test in the same way: by applying the test and running post-hoc tests if needed.\nDo not forget to create diagnostic plots, to see if the assumptions behind the parametric test are satisfied to an acceptable degree. Is that the case? And do the results from the parametric and non-parametric tests agree with one another?\n\nIn ponds.csv, measured acidity data (pH) is reported from four different ponds. Do the ponds differ in acidity, and if so, which ones from which others? Answer using both non-parametric and parametric tests, with appropriate post-hoc analyses. Check whether these different methods of analysis agree, and make sure that the assumptions behind the parametric test are satisfied using diagnostic plots. (Note: in this dataset, some values are missing.)"
  },
  {
    "objectID": "Anova_two_way.html#two-way-anova",
    "href": "Anova_two_way.html#two-way-anova",
    "title": "11  Two-way ANOVA and the Scheirer–Ray–Hare test",
    "section": "11.1 Two-way ANOVA",
    "text": "11.1 Two-way ANOVA\nChapter 10 discussed techniques for analyzing data which fall into multiple categories, but those categories are levels of a single factor. Here we go further and work with data classified by two independent factors.\nA good example is provided by the built-in dataset ToothGrowth, which contains data on the tooth growth of Guinea pigs in response to receiving vitamin C.\n\nlibrary(tidyverse)\n\nas_tibble(ToothGrowth)\n\n# A tibble: 60 × 3\n     len supp   dose\n   <dbl> <fct> <dbl>\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# … with 50 more rows\n\n\nExploring the data more closely, it turns out that there are three dosage levels (0.5, 1, and 2) and two types of supplement (VC for raw vitamin C or ascorbic acid, and OJ for orange juice). As usual, we first visualize the data. In doing so, it is useful to convert dose to a factor (Section 7.3): the three dosage levels play the role of a categorical variable (“low”, “medium” ,and “high” levels of vitamin C dosage), and we are not so interested in the actual magnitudes of those dosages.\n\nas_tibble(ToothGrowth) %>%\n  mutate(dose = as_factor(dose)) %>%\n  ggplot() +\n  aes(x = supp, y = len) +\n  geom_boxplot(alpha = 0.2, outlier.shape = NA,\n               colour = \"steelblue\", fill = \"steelblue\") +\n  geom_jitter(alpha = 0.4, width = 0.05, colour = \"steelblue\") +\n  labs(x = \"vitamin C dosage [mg / day]\", y = \"tooth length [mm]\") +\n  facet_grid(. ~ dose, labeller = label_both) +\n  theme_bw()\n\n\n\n\nIntuitively, we would expect there to be an effect of dosage, because the higher the dosage the longer the teeth become. We would also expect an effect of supplement type, because orange juice seems to perform better (at least no worse) than raw ascorbic acid in facilitating tooth growth. Continuing with the linear models from Chapter 10, it is easy to include two factors:\n\nToothGrowth %>%\n  mutate(dose = as_factor(dose)) %>%\n  lm(len ~ dose + supp, data = .) %>%\n  anova()\n\nAnalysis of Variance Table\n\nResponse: len\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \ndose       2 2426.43 1213.22  82.811 < 2.2e-16 ***\nsupp       1  205.35  205.35  14.017 0.0004293 ***\nResiduals 56  820.43   14.65                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe new feature above is the inclusion of dose + supp as the predictor, instead of just a single one. As seen from the ANOVA table, both the effects of dosage and supplement type appear to be real.\nHowever, this model ignores something that might be potentially relevant: the interaction between the two factors. This means that the nature of the relationship between tooth length and one of the predictors depends on the value of the other predictor. For the Guinea pig data, a case can be made that the effect of the supplement type depends on dosage: when the dosage level is either 0.5 or 1 mg/day, orange juice leads to longer teeth than ascorbic acid—but this benefit disappears at the highest dosage level of 2 mg/day.\nAccounting for interaction terms in a statistical model is easy. All one needs to do is add one more term to the formula, denoted dose:supp:\n\nToothGrowth %>%\n  mutate(dose = as_factor(dose)) %>%\n  lm(len ~ dose + supp + dose:supp, data = .) %>%\n  anova()\n\nAnalysis of Variance Table\n\nResponse: len\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \ndose       2 2426.43 1213.22  92.000 < 2.2e-16 ***\nsupp       1  205.35  205.35  15.572 0.0002312 ***\ndose:supp  2  108.32   54.16   4.107 0.0218603 *  \nResiduals 54  712.11   13.19                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe model result confirms that our intuition was likely correct: there does appear to be a real interaction effect between the two factors.\nThe inclusion of two factors with their interaction is so common in linear models that there is a shorthand notation to make it easier. Writing dose * supp is exactly the same as the above dose + supp + dose:supp. Let us see this in action:\n\nToothGrowth %>%\n  mutate(dose = as_factor(dose)) %>%\n  lm(len ~ dose * supp, data = .) %>%\n  anova()\n\nAnalysis of Variance Table\n\nResponse: len\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \ndose       2 2426.43 1213.22  92.000 < 2.2e-16 ***\nsupp       1  205.35  205.35  15.572 0.0002312 ***\ndose:supp  2  108.32   54.16   4.107 0.0218603 *  \nResiduals 54  712.11   13.19                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe two results are identical.\nAs in the case of one-way ANOVA, diagnostic plots and post-hoc testing (Tukey test) are useful tools. The diagnostic plots look excellent, so we can be confident about interpreting the p-values and other statistics of the linear model correctly:\n\nlibrary(ggfortify)\n\nToothGrowth %>%\n  mutate(dose = as_factor(dose)) %>%\n  lm(len ~ dose * supp, data = .) %>%\n  autoplot(which=3:2, smooth.colour=NA, colour=\"steelblue\", alpha=0.7) +\n  theme_bw()\n\n\n\n\nThe Tukey test can be used to compare each factor in isolation, as well as their combinations:\n\nToothGrowth %>%\n  mutate(dose = as_factor(dose)) %>%\n  lm(len ~ dose * supp, data = .) %>%\n  aov() %>%\n  TukeyHSD()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = .)\n\n$dose\n        diff       lwr       upr   p adj\n1-0.5  9.130  6.362488 11.897512 0.0e+00\n2-0.5 15.495 12.727488 18.262512 0.0e+00\n2-1    6.365  3.597488  9.132512 2.7e-06\n\n$supp\n      diff       lwr       upr     p adj\nVC-OJ -3.7 -5.579828 -1.820172 0.0002312\n\n$`dose:supp`\n                diff        lwr         upr     p adj\n1:OJ-0.5:OJ     9.47   4.671876  14.2681238 0.0000046\n2:OJ-0.5:OJ    12.83   8.031876  17.6281238 0.0000000\n0.5:VC-0.5:OJ  -5.25 -10.048124  -0.4518762 0.0242521\n1:VC-0.5:OJ     3.54  -1.258124   8.3381238 0.2640208\n2:VC-0.5:OJ    12.91   8.111876  17.7081238 0.0000000\n2:OJ-1:OJ       3.36  -1.438124   8.1581238 0.3187361\n0.5:VC-1:OJ   -14.72 -19.518124  -9.9218762 0.0000000\n1:VC-1:OJ      -5.93 -10.728124  -1.1318762 0.0073930\n2:VC-1:OJ       3.44  -1.358124   8.2381238 0.2936430\n0.5:VC-2:OJ   -18.08 -22.878124 -13.2818762 0.0000000\n1:VC-2:OJ      -9.29 -14.088124  -4.4918762 0.0000069\n2:VC-2:OJ       0.08  -4.718124   4.8781238 1.0000000\n1:VC-0.5:VC     8.79   3.991876  13.5881238 0.0000210\n2:VC-0.5:VC    18.16  13.361876  22.9581238 0.0000000\n2:VC-1:VC       9.37   4.571876  14.1681238 0.0000058\n\n\n(Again, due to how TukeyHSD is designed, the aov function must be called before one can use it on a linear model fit.) Here we first have a comparison between the dosage levels, averaging over supplement type. Even after this averaging there is a clear difference between the effects of each dosage level, as can be suspected based on a plot which ignores the supp factor:\n\nas_tibble(ToothGrowth) %>%\n  mutate(dose = as_factor(dose)) %>%\n  ggplot() +\n  aes(x = dose, y = len) +\n  geom_boxplot(alpha = 0.2, outlier.shape = NA,\n               colour = \"steelblue\", fill = \"steelblue\") +\n  geom_jitter(alpha = 0.4, width = 0.05, colour = \"steelblue\") +\n  labs(x = \"vitamin C dosage [mg / day]\", y = \"tooth length [mm]\") +\n  theme_bw()\n\n\n\n\nSimilarly, the difference between the two supplement types appears to be real (the Tukey test gave p adj = 0.0002312), even when not distinguishing by dosage—although this is somewhat less visible on a graph:\n\nas_tibble(ToothGrowth) %>%\n  ggplot() +\n  aes(x = supp, y = len) +\n  geom_boxplot(alpha = 0.2, outlier.shape = NA,\n               colour = \"steelblue\", fill = \"steelblue\") +\n  geom_jitter(alpha = 0.4, width = 0.05, colour = \"steelblue\") +\n  labs(x = \"vitamin C dosage [mg / day]\", y = \"tooth length [mm]\") +\n  theme_bw()\n\n\n\n\nFinally, in the $`dose:supp` part of the table, one can compare every particular experimental group (indexed by both dose and supp) with every other.\nIt is possible to use the summary function instead of anova when running the linear model. However, this table is likely not what we are looking for, because instead of having one row per factor and their interaction, it prints one row per fitted parameter. In case one is curious, here is what it looks like:\n\nToothGrowth %>%\n  mutate(dose = as_factor(dose)) %>%\n  lm(len ~ dose * supp, data = .) %>%\n  summary()\n\n\nCall:\nlm(formula = len ~ dose * supp, data = .)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8.20  -2.72  -0.27   2.65   8.27 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    13.230      1.148  11.521 3.60e-16 ***\ndose1           9.470      1.624   5.831 3.18e-07 ***\ndose2          12.830      1.624   7.900 1.43e-10 ***\nsuppVC         -5.250      1.624  -3.233  0.00209 ** \ndose1:suppVC   -0.680      2.297  -0.296  0.76831    \ndose2:suppVC    5.330      2.297   2.321  0.02411 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.631 on 54 degrees of freedom\nMultiple R-squared:  0.7937,    Adjusted R-squared:  0.7746 \nF-statistic: 41.56 on 5 and 54 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "Anova_two_way.html#the-scheirerrayhare-test",
    "href": "Anova_two_way.html#the-scheirerrayhare-test",
    "title": "11  Two-way ANOVA and the Scheirer–Ray–Hare test",
    "section": "11.2 The Scheirer–Ray–Hare test",
    "text": "11.2 The Scheirer–Ray–Hare test\nFor the sake of completeness, we mention that much like in the case of one-way ANOVA, there is a non-parametric version of the two-way ANOVA as well. This is the Scheirer–Ray–Hare test, which is therefore the two-way analogue of the Kruskal–Wallis test. To use this test, one must install and load the package rcompanion:\n\n\n\n\ninstall.packages(\"rcompanion\")\n\nlibrary(rcompanion)\n\nAnd now, we can use the function scheirerRayHare much like kruskal.test or lm:\n\nToothGrowth %>%\n  mutate(dose = as_factor(dose)) %>%\n  scheirerRayHare(len ~ dose * supp, data = .)\n\n\nDV:  len \nObservations:  60 \nD:  0.999222 \nMS total:  305 \n\n\n          Df  Sum Sq      H p.value\ndose       2 12394.4 40.669 0.00000\nsupp       1  1050.0  3.445 0.06343\ndose:supp  2   515.5  1.692 0.42923\nResiduals 54  4021.1               \n\n\nNote that this test is skeptical about the role of the supplement type, and definitely thinks that the interaction between it and dosage is not different from what one might get by pure chance. This illustrates one problem with the test: it is not very powerful in detecting patterns, even when they are there. To make matters worse, there is no appropriate post-hoc test available in conjunction with the Scheirer–Ray–Hare test. For these reasons, its use is more restricted than of other non-parametric tests, like the Wilcoxon and Kruskal–Wallis tests. It is good to know about it as an option, but often one must rely on other methods, such as the parametric two-way ANOVA."
  },
  {
    "objectID": "Anova_two_way.html#sec-exercises-anova-two-way",
    "href": "Anova_two_way.html#sec-exercises-anova-two-way",
    "title": "11  Two-way ANOVA and the Scheirer–Ray–Hare test",
    "section": "11.3 Exercises",
    "text": "11.3 Exercises\n\nThe file cow_growth.csv has data on the growth of individual cows which have received different grains (wheat, oats, or barley) and, independently, one of four different dietary supplements (one of which is no supplement, for control). Each of these diet combinations (twelve diets: three grains, times four supplements) had four cows observed. Is there any effect of these treatments on cow growth? Is there any interaction between the grain and the supplement given to the cows—some secret super-combination which makes the cows grow especially well (or poorly)?\n\nAs usual, before doing any tests, visualize and explore the data, and make sure you have a solid expectation for the results of any statistical analysis.\nAnswer the question by applying a parametric test. Run post-hoc tests as well if needed. Do not forget to create diagnostic plots, to see if the assumptions behind the parametric test are satisfied to an acceptable degree.\n\nThe built-in CO2 data frame contains measurements from an experiment on the cold tolerance of the grass species Echinochloa crus-galli. The dataset has five columns:\n\nPlant: unique identifier for each plant individual.\nType: either Quebec or Mississippi, depending on the origin of the plant.\nTreatment: whether the plant individual was chilled or nonchilled for the experiment.\nconc: carbon dioxide concentration in the surrounding environment.\nuptake: carbon dioxide uptake rate.\n\nHow do uptake rates depend on Type, Treatment, and their interaction? (For this exercise, you can ignore Plant and conc.) Start by forming a hypothesis based on visualizing the data. Then perform a parametric test and a corresponding post-hoc test. Make sure to use diagnostic plots to gauge the quality of its assumptions."
  },
  {
    "objectID": "Linear_regression.html#interpreting-the-results-of-a-linear-regression",
    "href": "Linear_regression.html#interpreting-the-results-of-a-linear-regression",
    "title": "12  Simple linear regression",
    "section": "12.1 Interpreting the results of a linear regression",
    "text": "12.1 Interpreting the results of a linear regression\nThe intercept and slope which minimize the sum of the squared deviations from the fit can be obtained for any data. But the question is: does such a linear fit actually mean anything? The situation is similar to what we discussed in Section 9.1: one can always compute the difference of the means between two groups of data, but whether the observed difference is meaningfully different from zero is another question. In the case of the two groups, we introduced techniques such as the Wilcoxon test and the t-test to answer that question.\nFor linear regression, a similar thing is possible. As seen, the output of lm above contains p-values and other statistics. The p-values are both practically zero, indicating that one can be absolutely certain that the observed nonzero intercept and slope aren’t simply due to chance. But for these conclusions to be meaningful, the usual assumptions must hold: the residuals should be independent from one another, should be normally distributed, and should be homoscedastic (have equal variance for all values of the predictor).\nOne can check whether these assumptions hold with diagnostic plots. In this particular example, we have a nearly perfect match-up between theory and the data, so the statistical results fro the linear regression should be reliable:\n\nlibrary(ggfortify)\n\nlm(Son ~ Father, data = heights) %>%\n  autoplot(which=3:2, smooth.colour=NA, colour=\"steelblue\", alpha=0.4) +\n  theme_bw()\n\n\n\n\nTo further illustrate possible difficulties of interpretation, let us take a look at a famous dataset that was designed for precisely this purpose (Anscombe 1973). The data are built into R (with the name anscombe), but are not in the most convenient format:\n\nprint(anscombe)\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\n\nThese are actually four datasets merged into one: x1 and y1 are x and y coordinates of the points from the first set, x1 and y2 from the second set, and so on. We can use pivot_longer to normalize these data:\n\nans_long <- anscombe %>%\n  pivot_longer(cols = everything(), names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\")\nprint(ans_long)\n\n# A tibble: 44 × 3\n   set       x     y\n   <chr> <dbl> <dbl>\n 1 1        10  8.04\n 2 2        10  9.14\n 3 3        10  7.46\n 4 4         8  6.58\n 5 1         8  6.95\n 6 2         8  8.14\n 7 3         8  6.77\n 8 4         8  5.76\n 9 1        13  7.58\n10 2        13  8.74\n# … with 34 more rows\n\n\nWe can now visualize each set, along with linear fits:\n\nans_long %>%\n  ggplot() +\n  aes(x = x, y = y, colour = set) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  facet_wrap(~ set, nrow = 2, labeller = label_both) +\n  theme_bw()\n\n\n\n\nThe data have been carefully crafted so that the least-squares regression line has an intercept of 3 and a slope of 0.5 for each of the four sets. Furthermore, the p-values are also identical to many decimal places. But this visual representation reveals what would have been much harder to intuit otherwise: that only the first set has a real chance of conforming to the assumptions of linear regression. Performing the regression on just this set and creating diagnostic plots:\n\nlm(y ~ x, data = filter(ans_long, set == \"1\")) %>% summary()\n\n\nCall:\nlm(formula = y ~ x, data = filter(ans_long, set == \"1\"))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx             0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm(y ~ x, data = filter(ans_long, set == \"1\")) %>%\n  autoplot(which = 3:2, smooth.colour = NA, colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\nThere is nothing to suggest on the diagnostic plots that there should be anything wrong with the regression—and, in fact, there isn’t anything wrong with it. The computed p-values for the intercept and slope are therefore reliable.\nThe situation changes for the other three sets. Let us look at set 2:\n\nlm(y ~ x, data = filter(ans_long, set == \"2\")) %>% summary()\n\n\nCall:\nlm(formula = y ~ x, data = filter(ans_long, set == \"2\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx              0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm(y ~ x, data = filter(ans_long, set == \"2\")) %>%\n  autoplot(which = 3:2, smooth.colour = NA, colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\nBlindly reading off the p-values without considering the diagnostic plots might lead one to take them seriously. This would be wrong however, as the assumptions of the linear regression are clearly not fulfilled based on the diagnostic plots. Especially the left one shows that the residuals are not independent, and certainly not identically and normally distributed.\nIn set 3, the trends are driven too much by a single outlier:\n\nlm(y ~ x, data = filter(ans_long, set == \"3\")) %>% summary()\n\n\nCall:\nlm(formula = y ~ x, data = filter(ans_long, set == \"3\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx             0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm(y ~ x, data = filter(ans_long, set == \"3\")) %>%\n  autoplot(which = 3:2, smooth.colour = NA, colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\nAs before, the diagnostic plots show that the independence of the residuals is violated. Finally, in set 4, the whole regression is based on a single point whose predictor is different from the rest:\n\nlm(y ~ x, data = filter(ans_long, set == \"4\")) %>% summary()\n\n\nCall:\nlm(formula = y ~ x, data = filter(ans_long, set == \"4\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx             0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nlm(y ~ x, data = filter(ans_long, set == \"4\")) %>%\n  autoplot(which = 3:2, smooth.colour = NA, colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\nClearly, homoscedasticity (equality of residual variances across all values of the predictor) is heavily violated.\nThese examples are there to urge caution when interpreting regression statistics. This problem becomes much more acute when relying on multiple regression, where there are more than one predictor variables. Since high-dimensional data cannot be visualized as easily as the datasets above, often the diagnostic plots are the only way to tell whether the assumptions of regression hold or not."
  },
  {
    "objectID": "Linear_regression.html#a-non-parametric-method-theilsen-regression",
    "href": "Linear_regression.html#a-non-parametric-method-theilsen-regression",
    "title": "12  Simple linear regression",
    "section": "12.2 A non-parametric method: Theil–Sen regression",
    "text": "12.2 A non-parametric method: Theil–Sen regression\nA nonparametric alternative to least-squares regression is the Theil–Sen regression. It is generally much more robust against outliers than the least-squares method. It also does not require that the residuals are normally distributed, or that they are homoscedastic. There are also two disadvantages, the main one being that it can only be used for simple regression (one single predictor). It can also be slower to compute, but with modern computers, this is rarely an issue.\nThe way Theil–Sen regression works is simple:\n\nA line is fit between all possible pairs of points, and their slopes are recorded.\nThe overall regression slope m is the median of all these pairwise slopes.\nThe intercept b is the median of all yi – m xi values, where xi is the ith predictor and yi the ith measurement at that predictor.\n\nTo use the Theil–Sen regression, one has to install the package mblm (“median-based linear models”):\n\n\n\n\ninstall.packages(\"mblm\")\n\nlibrary(mblm)\n\nThe function performing the regression is itself called mblm. A note of caution: its data argument, for some reason, is not called data but dataframe. Let us apply it to set 3 in the Anscombe dataset (the one with the single strong outlier):\n\nmblm(y ~ x, dataframe = filter(ans_long, set == \"3\")) %>% summary()\n\n\nCall:\nmblm(formula = y ~ x, dataframe = filter(ans_long, set == \"3\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.0045 -0.0022  0.0000  0.0025  4.2435 \n\nCoefficients:\n             Estimate       MAD V value Pr(>|V|)   \n(Intercept) 4.0050000 0.0074130      65  0.00501 **\nx           0.3455000 0.0007413      66  0.00380 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.415 on 9 degrees of freedom\n\n\nAs seen, the predicted intercept and slope are no longer 3 and 0.5, but 4 and 0.35 instead. Visualizing this, side by side with the ordinary least-squares regression:\n\nleastSquaresFit <-lm(y ~ x, data = filter(ans_long, set == \"3\"))\nTheilSenFit <-mblm(y ~ x, dataframe = filter(ans_long, set == \"3\"))\n\nans_long %>%\n  filter(set == \"3\") %>%\n  mutate(`least squares` = predict(leastSquaresFit),\n         `Theil-Sen` = predict(TheilSenFit)) %>%\n  pivot_longer(cols = c(\"least squares\", \"Theil-Sen\"),\n               names_to = \"type\", values_to = \"prediction\") %>%\n  ggplot() +\n  geom_point(aes(x = x, y = y), colour = \"steelblue\") +\n  geom_line(aes(x = x, y = prediction), colour = \"goldenrod\") +\n  facet_grid(. ~ type) +\n  theme_bw()\n\n\n\n\nThe Theil–Sen regression correctly recognizes the outlier for what it is, and remains unaffected by it."
  },
  {
    "objectID": "Linear_regression.html#exercises",
    "href": "Linear_regression.html#exercises",
    "title": "12  Simple linear regression",
    "section": "12.3 Exercises",
    "text": "12.3 Exercises\n\nThe file plant.growth.rate.csv contains individual plant growth data (mm/week), as a function of soil moisture content. Do plants grow better in more moist soils? Visualize the relationship, then perform and interpret a linear regression using both parametric and non-parametric methods. Use diagnostic plots to check whether the assumptions of the parametric test are satisfied.\nIt is difficult to measure the height of a tree. By contrast, the diameter at breast height (DBH) is easy to measure. Can one infer the height of a tree by measuring its DBH? The built-in dataset trees contains DBH data (somewhat misleadingly labeled Girth), as well as measured height and timber volume of 31 felled black cherry trees. You can ignore timber volume, and focus instead on how well DBH predicts tree height. Plot the relationship, perform both parametric and non-parametric regression, and create diagnostic plots. Interpret the results, and summarize how reliable it is to use DBH to infer tree height.\n\n\n\n\n\nAnscombe, Francis J. 1973. “Graphs in Statistical Analysis.” American Statistician 27 (1): 17–21. https://doi.org/10.1080/00031305.1973.10478966.\n\n\nGalton, Francis. 1886. “Regression Towards Mediocrity in Hereditary Stature.” Journal of the Anthropological Institute of Great Britain and Ireland 15: 246–63."
  },
  {
    "objectID": "Nonlinear_regression.html#combining-categorical-and-continuous-variables-in-linear-models",
    "href": "Nonlinear_regression.html#combining-categorical-and-continuous-variables-in-linear-models",
    "title": "13  More general linear models; nonlinear regression",
    "section": "13.1 Combining categorical and continuous variables in linear models",
    "text": "13.1 Combining categorical and continuous variables in linear models\nSo far we have used at most two predictors when dealing with linear models (lm). This was in Chapter 11, where we looked the effects of two categorical variables, as well as their interaction. Chapter 12 introduced the idea of using a continuous, instead of a categorical, predictor. But we have not been combining these.\nIn fact, one can build arbitrarily complicated linear models from an arbitrary combination of continuous and categorical variables, and their interactions. Let us consider the built-in CO2 dataset as an example, which was already used before in an exercise (Section 11.3). Briefly, the data contain measurements from an experiment on the cold tolerance of the grass species Echinochloa crus-galli. The dataset has five columns: Plant (a unique identifier for each plant individual), Type (either Quebec or Mississippi depending on the origin of the plant), Treatment (whether the plant individual was chilled or nonchilled for the experiment), conc (ambient carbon dioxide concentration), and uptake (carbon dioxide uptake rate by the plant).\n\nlibrary(tidyverse)\nas_tibble(CO2)\n\n# A tibble: 84 × 5\n   Plant Type   Treatment   conc uptake\n   <ord> <fct>  <fct>      <dbl>  <dbl>\n 1 Qn1   Quebec nonchilled    95   16  \n 2 Qn1   Quebec nonchilled   175   30.4\n 3 Qn1   Quebec nonchilled   250   34.8\n 4 Qn1   Quebec nonchilled   350   37.2\n 5 Qn1   Quebec nonchilled   500   35.3\n 6 Qn1   Quebec nonchilled   675   39.2\n 7 Qn1   Quebec nonchilled  1000   39.7\n 8 Qn2   Quebec nonchilled    95   13.6\n 9 Qn2   Quebec nonchilled   175   27.3\n10 Qn2   Quebec nonchilled   250   37.1\n# … with 74 more rows\n\n\nWe can plot the observed distributions of CO2 uptake rates for each type and treatment:\n\nas_tibble(CO2) %>%\n  ggplot(aes(x = 0, y = uptake)) +\n  geom_boxplot(colour = \"steelblue\", fill = \"steelblue\",\n               alpha = 0.2, outlier.shape = NA) +\n  geom_jitter(colour = \"steelblue\", alpha = 0.5, width = 0.05) +\n  facet_grid(Type ~ Treatment) +\n  ylab(\"uptake rate\") +\n  theme_bw() +\n  theme(axis.title.x = element_blank(), # The x-axis is meaningless here,\n        axis.ticks.x = element_blank(), # so remove title, tick marks,\n        axis.text.x = element_blank())  # and labels from it\n\n\n\n\nThis, however, is only part of the story, as becomes obvious if we also plot the ambient CO2 concentrations (conc) along the x-axis:\n\nas_tibble(CO2) %>%\n  ggplot(aes(x = conc, y = uptake)) +\n  geom_point(colour = \"steelblue\", alpha = 0.8) +\n  facet_grid(Type ~ Treatment) +\n  labs(x = \"concentration\", y = \"uptake rate\") +\n  theme_bw()\n\n\n\n\nWe see that there is also a clear, saturating relationship between CO2 concentration and uptake rates. It is definitely not a linear relationship, but a saturating one. This does not mean that a linear model is useless for analyzing these data: the trend of whether the data increase or decrease can still be captured (although it is not recommended to use the model for numerical prediction purposes). One model that may come to mind is as follows:\n\nlm(uptake ~ conc + Type * Treatment, data = CO2) %>% anova()\n\nAnalysis of Variance Table\n\nResponse: uptake\n               Df Sum Sq Mean Sq F value    Pr(>F)    \nconc            1 2285.0  2285.0 63.5032 1.002e-11 ***\nType            1 3365.5  3365.5 93.5330 4.787e-15 ***\nTreatment       1  988.1   988.1 27.4611 1.300e-06 ***\nType:Treatment  1  225.7   225.7  6.2733   0.01432 *  \nResiduals      79 2842.6    36.0                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn other words, the uptake rates are modeled via a combination of the effect of concentration (a continuous variable) plus the interaction of type and treatment (two categorical variables). Recall that Type * Treatment is shorthand for Type + Treatment + Type:Treatment, the sum of the main effects and the interaction between them. The rationale for choosing this model is that the boxplot above reveals a potential interaction between the two factors Type and Treatment (the effect of changing Treatment from chilled to nonchilled depends on whether the Type was Quebec or Mississippi), and on top of this, we also want to capture the positive dependence on CO2 concentration. The ANOVA table above concurs: each of these categories come out with low p-values, indicating that what we see is unlikely to be due to just chance. To make sure that the assumptions on which this interpretation rests are held, we look at the diagnostic plots:\n\nlibrary(ggfortify)\n\nlm(uptake ~ conc + Type * Treatment, data = CO2) %>%\n  autoplot(which=3:2, smooth.colour=NA, colour=\"steelblue\", alpha=0.7) +\n  theme_bw()\n\n\n\n\nWhile the Q–Q plot could be better, there is not much reason not to trust the model—other than the fact, or course, that the data in fact depend on concentrations in a manifestly nonlinear way. We will come back to this point in Section 13.2.\nIt is also informative to apply the function summary on the model fit in addition to anova, to obtain the regression slopes and intercept:\n\nlm(uptake ~ conc + Type * Treatment, data = CO2) %>% summary()\n\n\nCall:\nlm(formula = uptake ~ conc + Type * Treatment, data = CO2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.4240  -2.3674   0.7641   3.8749   9.6278 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      27.620528   1.627945  16.966  < 2e-16 ***\nconc                              0.017731   0.002225   7.969 1.00e-11 ***\nTypeMississippi                  -9.380952   1.851185  -5.068 2.59e-06 ***\nTreatmentchilled                 -3.580952   1.851185  -1.934   0.0566 .  \nTypeMississippi:Treatmentchilled -6.557143   2.617972  -2.505   0.0143 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.999 on 79 degrees of freedom\nMultiple R-squared:  0.7072,    Adjusted R-squared:  0.6923 \nF-statistic: 47.69 on 4 and 79 DF,  p-value: < 2.2e-16\n\n\nAs good as this model looks, one can argue based on the plot of the data that there could also be an interaction between conc and the other two factors. After all, the saturation levels of the uptake rate are always higher in Quebec than in Mississippi, and the effect of chilling also depends on Type. A model which accounts for all these effects and their interactions is uptake ~ conc * Type * Treatment:\n\nlm(uptake ~ conc * Type * Treatment, data = CO2) %>% anova()\n\nAnalysis of Variance Table\n\nResponse: uptake\n                    Df Sum Sq Mean Sq  F value    Pr(>F)    \nconc                 1 2285.0  2285.0  68.1766 3.545e-12 ***\nType                 1 3365.5  3365.5 100.4164 1.516e-15 ***\nTreatment            1  988.1   988.1  29.4821 6.512e-07 ***\nconc:Type            1  208.0   208.0   6.2060   0.01491 *  \nconc:Treatment       1   31.9    31.9   0.9509   0.33258    \nType:Treatment       1  225.7   225.7   6.7350   0.01134 *  \nconc:Type:Treatment  1   55.5    55.5   1.6570   0.20192    \nResiduals           76 2547.2    33.5                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlm(uptake ~ conc * Type * Treatment, data = CO2) %>% summary()\n\n\nCall:\nlm(formula = uptake ~ conc * Type * Treatment, data = CO2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.3773  -2.7602   0.9517   3.7368  10.7414 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                           25.585034   2.255256  11.345  < 2e-16 ***\nconc                                   0.022410   0.004295   5.218 1.52e-06 ***\nTypeMississippi                       -7.131741   3.189414  -2.236   0.0283 *  \nTreatmentchilled                      -4.163993   3.189414  -1.306   0.1956    \nconc:TypeMississippi                  -0.005171   0.006074  -0.851   0.3973    \nconc:Treatmentchilled                  0.001340   0.006074   0.221   0.8259    \nTypeMississippi:Treatmentchilled      -1.747509   4.510513  -0.387   0.6995    \nconc:TypeMississippi:Treatmentchilled -0.011057   0.008589  -1.287   0.2019    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.789 on 76 degrees of freedom\nMultiple R-squared:  0.7376,    Adjusted R-squared:  0.7134 \nF-statistic: 30.52 on 7 and 76 DF,  p-value: < 2.2e-16\n\nlm(uptake ~ conc * Type * Treatment, data = CO2) %>%\n  autoplot(which=3:2, smooth.colour=NA, colour=\"steelblue\", alpha=0.7) +\n  theme_bw()\n\n\n\n\nThis confirms what we saw on the plot of the data: that the basic shape of the relationship between concentration and uptake is unaffected by either Type or Treatment (i.e., the term conc:Type:Treatment in the ANOVA table has a high associated p-value). It also illustrates the general point that there are very often multiple candidate models, and choosing between them is a question of judgment, trial-and-error, and successively improving the model structure based on results from earlier modeling attempts."
  },
  {
    "objectID": "Nonlinear_regression.html#sec-nonlin-regression",
    "href": "Nonlinear_regression.html#sec-nonlin-regression",
    "title": "13  More general linear models; nonlinear regression",
    "section": "13.2 Nonlinear regression",
    "text": "13.2 Nonlinear regression\nThe relationship between CO2 concentration and uptake rates are definitely not linear, regardless of treatment or type. So the question arises: can one fit a nonlinear function to these data? As an example, let us focus on just Quebec and the nonchilled treatment, to better illustrate the ideas behind nonlinear regression. Here are the data:\n\nas_tibble(CO2) %>%\n  filter(Type == \"Quebec\", Treatment == \"nonchilled\") %>%\n  ggplot(aes(x = conc, y = uptake)) +\n  geom_point(colour = \"steelblue\", alpha = 0.8) +\n  labs(x = \"concentration\", y = \"uptake rate\") +\n  theme_bw()\n\n\n\n\nIf the function we wish to fit is not linear, we have to specify its shape. One commonly used shape for describing the above saturating pattern is the Michaelis–Menten curve. This is given by the following equation: \\[ \\rho = \\frac{V c}{K + c} \\] Here \\(\\rho\\) is the uptake rate, \\(c\\) is the concentration, and \\(V\\) and \\(K\\) are two parameters which can modify the shape of the function. The figure below illustrates what curves one can get by varying these parameters:\n\nexpand_grid(V = c(10, 20, 30), # This function creates a tibble with all\n            K = c(1, 5, 10),   # possible combinations of the inputs\n            concentration = seq(0, 60, l = 201)) %>%\n  group_by(V, K) %>%\n  mutate(uptake = V * concentration / (K + concentration)) %>%\n  ungroup() %>%\n  ggplot() +\n  aes(x = concentration, y = uptake) +\n  geom_line(colour = \"steelblue\") +\n  facet_grid(V ~ K, labeller = label_both) +\n  theme_bw()\n\n\n\n\nThe task is to find the values of \\(V\\) and \\(K\\) that provide the best fit to the data. Like in the case of linear regression, this can be done via the least-squares criterion: the best fit is provided by the curve which minimizes the sum of the squared deviations of the observed points from it. Unlike with linear regression however, this curve can be very difficult to find. In fact, there is no known general procedure that would be able to minimize the sum of squares under all circumstances. What algorithms can do is to find the best fit, given some initial guesses for the parameters that are at least not violently off of the true values. Just how close the guess needs to be is context-dependent, and highlights an important problem: nonlinear regression can be as much an art as it is a science. For the types of curves we will be fitting though, the more subtle problems will never come up, and a “good enough” initial guess can vary within a relatively wide range.\nSo, how can one guess the values of \\(V\\) and \\(K\\)? To do this, one has to have an understanding of how the parameters influence the curves. For \\(V\\), this interpretation is not difficult to infer. Notice that if concentrations are very, very large, then in the denominator of the formula \\(\\rho = V c / (K + c)\\), we might as well say that \\(K + c\\) is approximately equal to \\(c\\) (if \\(c\\) is a million and \\(K\\) is one, then one is justified in treating the sum as being about one million still). This means that for large \\(c\\), the formula reduces to \\(\\rho \\approx V c / c = V\\). In other words \\(V\\) is the saturation uptake rate: the maximum value of the uptake. This, incidentally, is clearly visible in the plots above: when \\(V\\) is 10 (top row), the curves always tend towards 10 for large concentrations; when \\(V\\) is 20, they tend towards 20 (middle row), and when \\(V\\) is 30, they tend towards 30.\nThe interpretation of \\(K\\) is slightly less straightforward, but still simple. To see what it means, let us ask what the uptake rate would be, were the concentration’s value equal to \\(K\\). In that case, we get \\(\\rho = V K / (K + K)\\) (we simply substituted \\(c = K\\) into the formula), or \\(\\rho = VK / (2K) = V/2\\). That is, \\(K\\) is the concentration at which the uptake rate reaches half its maximum.\nLooking at the data again, both these parameters can be roughly estimated:\n\nas_tibble(CO2) %>%\n  filter(Type == \"Quebec\", Treatment == \"nonchilled\") %>%\n  ggplot(aes(x = conc, y = uptake)) +\n  geom_point(colour = \"steelblue\", alpha = 0.8) +\n  geom_hline(yintercept = 43, linetype = \"dashed\", colour = \"steelblue\") +\n  annotate(geom = \"segment\", x = 0, y = 43/2, xend = 125, yend = 43/2,\n           linetype = \"dashed\", colour = \"steelblue\") +\n  annotate(geom = \"segment\", x = 125, y = 43/2, xend = 125, yend = 0,\n           linetype = \"dashed\", colour = \"steelblue\") +\n  scale_x_continuous(name = \"concentration\",\n                     limits = c(0, NA), expand = c(0, 0)) +\n  scale_y_continuous(name = \"uptake rate\",\n                     limits = c(0, NA), expand = c(0, 0)) +\n  theme_bw()\n\n\n\n\nSo guessing that \\(V\\) is about 43 and \\(K\\) is about 125 seems to be close to the mark.\nTo actually perform the nonlinear regression, one can use the nls function (“Nonlinear Least Squares”). It begins much like lm, taking a formula and a data frame. However, the formula is no longer a shorthand notation for a linear model, and therefore has to be entered literally. Additionally, there is another argument to nls called start; this is where the starting values have to be specified. The start argument has to be in the form of a list. Lists are an important data structure, worth a little interlude to explain how they work.\n\n13.2.1 Interlude: lists\nLists are like vectors except they can hold arbitrary data in their entries. So unlike vectors which are composed of either all numbers or all character strings or all logical values, lists may have a combination of these. Furthermore, list entries are not restricted to elementary types: vectors, or even data frames may also be list entries. To define a list, all one needs to do is type list, and then give a sequence of named entries. For example, the following creates a list with three entries: a will be equal to 3, b to the string \"Hello!\", and myTable to a small tibble.\n\nlist(a = 3, b = \"Hello!\", myTable = tibble(x = c(1, 2), y = c(3, 4)))\n\n$a\n[1] 3\n\n$b\n[1] \"Hello!\"\n\n$myTable\n# A tibble: 2 × 2\n      x     y\n  <dbl> <dbl>\n1     1     3\n2     2     4\n\n\nOne can refer to the entries of the list either with the $ notation, or using double brackets. Assigning the above list to a variable called myList first:\n\nmyList <- list(\n  a = 3,\n  b = \"Hello!\",\n  myTable = tibble(x = c(1, 2), y = c(3, 4))\n)\n\nWe can now access the entries of myList either as\n\nmyList$myTable # Access the entry called myTable in the list\n\n# A tibble: 2 × 2\n      x     y\n  <dbl> <dbl>\n1     1     3\n2     2     4\n\n\nOr as\n\nmyList[[3]] # Access the third entry (myTable) in the list\n\n# A tibble: 2 × 2\n      x     y\n  <dbl> <dbl>\n1     1     3\n2     2     4\n\n\nThe $ notation is the same as when one accesses columns of data frames. This is not a coincidence: internally, data frames are represented as lists of columns, with each column being a vector. It follows that the double-bracket notation can also be used in conjunction with data frames: to access the first column of CO2, we can not only write CO2$Plant, but also CO2[[1]].\nA list seems to be just a more flexible version of a vector—so why would we want to use vectors instead of lists in the first place? The answer has to do with efficiency: the price to pay for the increased flexibility offered by lists is that they are slower to do operations on. While this would not be a problem for the applications found in this book, it can become important when dealing with large datasets or heavy numerical computations. As a corollary, R has many useful functions which work on vectors but do not work by default on lists. To mention just the simplest examples: mean, median, sd, and sum will throw an error when applied to lists. That is,\n\nsum(c(1, 2, 3))\n\n[1] 6\n\n\nreturns the expected 6, because it was applied to a vector. But the same expression results in an error when the vector is replaced by a list:\n\nsum(list(1, 2, 3))\n\nError in sum(list(1, 2, 3)): invalid 'type' (list) of argument\n\n\n\n\n13.2.2 Back to nonlinear regression\nWith this brief introduction to lists, we now understand what it means that the start argument to nls must be a list, with the entries named after the parameters to be fitted. Using the previously-guessed values of \\(V\\) and \\(K\\) being around 43 and 125, respectively, means we can use start = list(V = 43, K = 125). We can now look at the nls function and use it to produce a fit of the Michaelis–Menten curve to our data:\n\nnonlinearFit <- as_tibble(CO2) %>%\n  filter(Type == \"Quebec\", Treatment == \"nonchilled\") %>%\n  nls(uptake ~ V*conc/(K + conc), data = ., start = list(V = 43, K = 125))\n\nprint(nonlinearFit)\n\nNonlinear regression model\n  model: uptake ~ V * conc/(K + conc)\n   data: .\n     V      K \n 51.36 136.32 \n residual sum-of-squares: 319.2\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 9.728e-06\n\n\nObserve that in the formula, we use the column name conc when we want to use the data inside that column, but use made-up names (in this case, V and K) for the unknown parameters we are trying to obtain. Their starting values were filled in from our earlier visual estimation. From these starting values, the best fitting solution is found. We see that their values are 51.36 for \\(V\\) and 136.32 for \\(K\\).\nThe result of nls can be used inside summary to get more information on the fitted parameters (note: the anova function is not applicable to nonlinear regression). Doing so results in the following regression table:\n\nsummary(nonlinearFit)\n\n\nFormula: uptake ~ V * conc/(K + conc)\n\nParameters:\n  Estimate Std. Error t value Pr(>|t|)    \nV   51.359      2.831  18.140 1.86e-13 ***\nK  136.319     27.027   5.044 7.21e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.099 on 19 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 9.728e-06\n\n\nAs in the case of linear regression, the statistical analysis will only be reliable if the assumptions of the independence, normality, and homoscedasticity of the residuals are maintained.\nTo conclude this section, it can be useful to plot the data together with the fitted nonlinear curve, to make sure visually that the fit is reasonable. There are several possible ways of doing this; here we discuss two of them. First, one can rely on the predict function (which works also for lm) that will tell us, for each value of the predictor, what the model-predicted values are. So\n\npredict(nonlinearFit)\n\n [1] 21.09265 28.87030 33.23631 36.96287 40.35655 42.72982 45.19795 21.09265\n [9] 28.87030 33.23631 36.96287 40.35655 42.72982 45.19795 21.09265 28.87030\n[17] 33.23631 36.96287 40.35655 42.72982 45.19795\n\n\nreturns a vector for each conc in the original data, documenting what the model thinks the corresponding uptake rate ought to be. They can then be compared with the data:\n\nas_tibble(CO2) %>%\n  filter(Type == \"Quebec\", Treatment == \"nonchilled\") %>%\n  mutate(pred = predict(nonlinearFit)) %>%\n  ggplot() +\n  geom_point(aes(x = conc, y = uptake), colour = \"steelblue\", alpha = 0.8) +\n  geom_line(aes(x = conc, y = pred), linetype = \"dashed\", alpha = 0.5) +\n  labs(x = \"concentration\", y = \"uptake rate\") +\n  theme_bw()\n\n\n\n\nIn the above plot, the data points and the predictions each had their own set of aeshetics. For this reason, the aesthetic mappings were not defined separately, but locally inside each geom_. This is perfectly legal, and can help whenever different geometries take different aesthetics from the data. Second, notice that the prediction was drawn with a dashed, semi-transparent line. This is intentional, to make it distinct from data. It signals that the curve does not correspond to data we are plotting, but to a model’s predictions.\nThe second method can be useful if the data are sufficiently scarce that the fitted line looks “rugged”, a bit too piecewise (this can be seen in the above example as well if one looks carefully). In that case, it is possible to draw the curve of any function using geom_function. We can use the fitted parameters in drawing it, and it will not suffer from being too piecewise:\n\nV_fitted <- coef(nonlinearFit)[\"V\"] # Get fitted values of V and K\nK_fitted <- coef(nonlinearFit)[\"K\"] # from the vector of coefficients\n\nas_tibble(CO2) %>%\n  filter(Type == \"Quebec\", Treatment == \"nonchilled\") %>%\n  ggplot() +\n  geom_point(aes(x = conc, y = uptake), colour = \"steelblue\", alpha = 0.8) +\n  geom_function(fun = function(x) V_fitted * x / (K_fitted + x),\n                linetype = \"dashed\", alpha = 0.5) +\n  labs(x = \"concentration\", y = \"uptake rate\") +\n  theme_bw()\n\n\n\n\nThe result is much the same as before, although carefully looking at the dashed line shows that the second curve is smoother than the first. In this case, this does not matter much, but it could be aesthetically more relevant in others."
  },
  {
    "objectID": "Nonlinear_regression.html#exercises",
    "href": "Nonlinear_regression.html#exercises",
    "title": "13  More general linear models; nonlinear regression",
    "section": "13.3 Exercises",
    "text": "13.3 Exercises\n\nExponential growth\nLet \\(N(t)\\) be the population abundance of some organism at time \\(t\\). An exponentially growing population increases according to \\[ N(t) = N_0 \\mathrm{e}^{rt} \\] where \\(N_0\\) is the initial population size at time \\(t=0\\), and \\(r\\) is the exponential rate of increase.\n\nDownload the data file pop_growth_1.csv from Lisam and load it in R.\nUse nls() to fit the above exponential growth model to this dataset. Do not treat \\(N_0\\) as a free parameter, but instead use the actual population size at time \\(t=0\\). This leaves \\(r\\) as the only parameter to be fitted. Do so, using an appropriate starting value.\nAssume that the data describes a population of water lilies, and that a single ‘individual’ weighs 1 gram. If the population would continue to grow unrestricted, what would be its biomass after nine months (about 280 days)? What object would have a comparable weight to this population, and what does that tell us about unrestricted population growth in general?\n\n\n\nNitrogen uptake\nCedergreen and Madsen (2002) reported data from an experiment on nitrogen uptake by the duckweed Lemna minor, where the predictor variable is the initial substrate concentration and the response variable is the uptake rate. In this type of experiment, it is anticipated that the uptake will increase as the concentration increases, approaching a horizontal asymptote. The data are available in uptake.csv.\n\nCreate a plot of the data, with the nitrogen concentrations along the x-axis and the corresponding uptake rates along the y-axis.\nFit a Michaelis-Menten model (describing saturating dynamics) to the data. This model is given by \\[ \\rho = \\frac{V c}{K + c} \\] where \\(V\\) and \\(K\\) are two constants, \\(c\\) is the concentration, and \\(\\rho\\) the uptake rate. Make initial guesses for the two parameters \\(V\\) and \\(K\\) based on the graph, and perform the nonlinear regression.\nGiven your nonlinear regression results, what is the maximum possible nitrogen uptake rate of L. minor?\n\n\n\nLogistic growth\nThe simplest model illustrating population regulation and regulated growth is the logistic model, defined by the differential equation \\[ \\frac{\\mathrm{d} N(t)}{\\mathrm{d} t} = rN(t) \\left( 1 - \\frac{N(t)}{K} \\right) \\] Here \\(N(t)\\) is the population abundance at time \\(t\\), \\(r\\) is the exponential growth rate of the population when rare, and \\(K\\) is the maximum abundance it can sustainably achieve (the “carrying capacity”). It should be obvious that when \\(N(t) = K\\), the derivative vanishes, signalling that the population size no longer changes.\nThe above differential equation is one of the few which can be solved explicitly. Its solution reads \\[ N(t) = N_0 \\frac{\\mathrm{e}^{rt}}{1-(1-\\mathrm{e}^{rt})(N_0/K)} \\] where \\(N_0\\) is the initial population size at time \\(t=0\\). Let us fit this model to some population growth data.\n\nDownload the data file pop_growth_2.csv, load it in R, and plot the population abundances against time.\nFit the above model to the data using the nls() function, with appropriate guesses for the starting values of \\(r\\) and \\(K\\).\nPlot the data and the model prediction together. What are the estimated values of \\(r\\) and \\(K\\)?\n\n\n\n\n\nCedergreen, Nina, and Tom Vindbæk Madsen. 2002. “Nitrogen uptake by the floating macrophyte Lemna minor.” New Phytologist 155 (2): 285–92. https://doi.org/10.1046/j.1469-8137.2002.00463.x."
  },
  {
    "objectID": "Intro_map.html#introduction",
    "href": "Intro_map.html#introduction",
    "title": "14  Higher-order functions and mapping",
    "section": "14.1 Introduction",
    "text": "14.1 Introduction\n\nlibrary(tidyverse) # Loading the tidyverse, before doing anything else\n\nIn Chapter 2 we learned how to create user-defined functions. An example was provided in Section 10.4, where we made our life easier by eliminating the need to always call aov before performing a Tukey test with TukeyHSD. Without the function, we must write:\n\nlm(weight ~ group, data = PlantGrowth) %>% aov() %>% TukeyHSD()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = .)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\n\nInstead, we can write a simple function that takes a linear model fit object as its input and produces the Tukey test as its output:\n\ntukeyTest <- function(modelFit) modelFit %>% aov() %>% TukeyHSD()\n\nUsing this function, we can now simply write:\n\nlm(weight ~ group, data = PlantGrowth) %>% tukeyTest()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = .)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\n\n\n\n\n\n\n\nNote\n\n\n\nTwo features in the definition of tukeyTest above may look unfamiliar. First, the body of the function was not enclosed between a pair of curly braces {...}. The rule in R is as follows: an arbitrary block of code between curly braces, potentially made up of several expressions, is treated as one single expression. Its value is by definition the final evaluated expression within the braces. This also means that the curly braces are superfluous whenever they contain only a single expression.\nSecond, in this function the return keyword was not used. Like curly braces, return is optional. If omitted, the final expression evaluated within the body of the function automatically becomes the return value.\nWhether to use braces and return even in cases when they are not needed is largely a question of personal coding style. From now on, they will often be omitted whenever their absence does not hamper program readability.\n\n\nAnother useful function we can write helps use statistical procedures within a pipeline. As we have seen, most statistical functions take a formula as their first argument and the data as their second (and then they may take further, method-specific arguments as well, like conf.int in the function wilcox.test). Since the pipe operator %>% is often used in conjunction with functions like select, mutate, pivot_longer, summarise, etc. which all return a data frame, it would be convenient to reverse the order of arguments in all statistical functions, with the data coming first and the formula coming second. In fact, such a function is easy to write. We could call it stats:\n\nstats <- function(data, formula, method) method(formula, data)\n\nHere method is the statistical function we wish to use. For example:\n\nPlantGrowth %>%\n  filter(group != \"ctrl\") %>%\n  stats(weight ~ group, wilcox.test)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n\n\nThis is now fully equivalent to:\n\nPlantGrowth %>%\n  filter(group != \"ctrl\") %>%\n  wilcox.test(weight ~ group, data = .)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n\n\nWe can add a further improvement to the stats function. As it is, it can only take the formula and the data as inputs, but not any other, function-specific arguments. In R, there is a way of passing arbitrary extra arguments, using the ellipsis (...). We can redefine stats this way:\n\nstats <- function(data, formula, method, ...) {\n  method(formula, data, ...) # The ... means \"possibly more arguments\"\n}\n\nAnd now, we can pass extra arguments that we would not have been able to do before. For instance, we can request confidence intervals from wilcox.test:\n\nPlantGrowth %>%\n  filter(group != \"ctrl\") %>%\n  stats(weight ~ group, wilcox.test, conf.int = TRUE, conf.level = 0.99)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n99 percent confidence interval:\n -1.70 -0.03\nsample estimates:\ndifference in location \n                -0.945 \n\n\nFrom now on, we can use stats for performing various statistical procedures:\n\nlibrary(FSA) # For the Dunn test\nPlantGrowth %>% stats(weight ~ group, lm) %>% anova()\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value  Pr(>F)  \ngroup      2  3.7663  1.8832  4.8461 0.01591 *\nResiduals 27 10.4921  0.3886                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPlantGrowth %>% stats(weight ~ group, lm) %>% tukeyTest()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = .)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\nPlantGrowth %>% stats(weight ~ group, kruskal.test)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  weight by group\nKruskal-Wallis chi-squared = 7.9882, df = 2, p-value = 0.01842\n\nPlantGrowth %>% stats(weight ~ group, dunnTest)\n\n   Comparison         Z    P.unadj      P.adj\n1 ctrl - trt1  1.117725 0.26368427 0.26368427\n2 ctrl - trt2 -1.689290 0.09116394 0.18232789\n3 trt1 - trt2 -2.807015 0.00500029 0.01500087"
  },
  {
    "objectID": "Intro_map.html#higher-order-functions",
    "href": "Intro_map.html#higher-order-functions",
    "title": "14  Higher-order functions and mapping",
    "section": "14.2 Higher-order functions",
    "text": "14.2 Higher-order functions\nIf we think about the stats function above, something strange has happened. We are used to the idea of functions taking numbers, character strings, logical values, or even data frames as their arguments. What we have not paid much attention to before is what happens if the input to a function is itself another function. Yet this is precisely what stats does: its method argument is a function object.\nIn R, this is perfectly legal, and its use and interpretation is every bit as natural as it was in stats. Functions which take other functions as arguments are often called higher-order functions.1 To emphasize again: there really is nothing special about such functions, and they can be used in much the same way as “ordinary” functions.\nOne very natural example for a higher-order function is integration. An integral (at least in simple cases) takes three inputs: a function to integrate, a lower limit of integration, and an upper limit of integration. The output is the (sign-weighted) area under the function’s curve in between the lower and upper limits. This is stressed even in the usual mathematical notation for integrals: when we write \\[ \\int_0^1 x^2 \\,\\text{d} x = \\frac{1}{3}\\] (a true statement), we show the lower and upper limits of 0 and 1 at the bottom and top of the integral sign, and the function to be integrated (in this case, \\(f(x) = x^2\\)) in between the integral sign and \\(\\text{d} x\\).\nIf you do not know how integrals do their magic, there is no need to worry, because R has a built in function called integrate to do the calculations for you. integrate takes the three arguments described above: the function to integrate, and the lower and upper limits of integration. To perform the above integral, we can write:\n\n# The squaring function: sqr(2) returns 4, sqr(4) returns 16, etc.\nsqr <- function(x) x^2\n# Perform the integral between 0 and 1:\nintegrate(sqr, 0, 1)\n\n0.3333333 with absolute error < 3.7e-15\n\n\nThe answer is indeed one-third.2 But there was no obligation to use the square function above. We could have used any other one. For instance, to compute the integral of the cosine function between 0 and 2π, we can type:\n\nintegrate(cos, 0, 2*pi)\n\n4.359836e-16 with absolute error < 4.5e-14\n\n\nWe get the expected result of zero, within numerical error.\nOne thing to know about function objects like sqr is that they do not need a name to be used. In the definition sqr <- function(x) x^2, we assigned the function object function(x) x^2 to the symbol sqr, so we wouldn’t have to write it out all the time. But since sqr is just a name that stands for function(x) x^2, calling (function(x) x^2)(4) is the same as calling sqr(4), both returning 16. If a function is used only once within another (higher-order) function, then we might not wish to bother with naming the function separately. Thus, the following is exactly equivalent to integrating the sqr function:\n\nintegrate(function(x) x^2, 0, 1)\n\n0.3333333 with absolute error < 3.7e-15\n\n\nFunctions without names are often called anonymous functions. They are commonly used within other, higher-order functions. Their use is not mandatory: it is always possible to first define the function with a name, and then use that name instead (e.g., using sqr instead of function(x) x^2, after defining sqr <- function(x) x^2). However, they can be convenient, and it is also important to recognize them in R code written by others."
  },
  {
    "objectID": "Intro_map.html#sec-mapfamily",
    "href": "Intro_map.html#sec-mapfamily",
    "title": "14  Higher-order functions and mapping",
    "section": "14.3 The map family of functions",
    "text": "14.3 The map family of functions\nThe purrr package is a standard, automatically-loaded part of the tidyverse. It contains a large family of mapping functions. These allow one to perform repetitive tasks by applying the same function to all elements of a vector, list, or column in a data frame.\nTo illustrate their use, how would we obtain the squares of all integers from 1 to 10? Using our earlier sqr function, we could painstakingly type out sqr(1), then sqr(2), and so on, up until sqr(10) (we ought to be grateful that the task was to obtain the squares of the first ten integers, instead of the first ten thousand). But there is no need to do this, as this is exactly what map can do. map takes two arguments: some data (e.g., a vector of values), and a function. It then applies that function to all data entries. So a much quicker way of obtaining the squares of all integers from 1 to 10 is this:\n\nmap(1:10, sqr)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n[[6]]\n[1] 36\n\n[[7]]\n[1] 49\n\n[[8]]\n[1] 64\n\n[[9]]\n[1] 81\n\n[[10]]\n[1] 100\n\n\nOr, in case we prefer anonymous functions and do not want to bother with defining our own sqr routine:\n\nmap(1:10, function(x) x^2)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n[[6]]\n[1] 36\n\n[[7]]\n[1] 49\n\n[[8]]\n[1] 64\n\n[[9]]\n[1] 81\n\n[[10]]\n[1] 100\n\n\nThe results are all there, although they are prefaced by double-bracketed indices [[1]], [[2]], and so on. You may recall from Section 13.2.1 that this is the notation used to reference the entries of lists. That is correct: map returns a list of values, not a vector. We will see momentarily that this can be very useful behavior, but here it can feel overkill. Fortunately, it is easy to get back a vector instead of a list. Since the entries of vectors must have a well-defined, uniform type (numeric, character string, logical, etc.), we have to tell R what kind of result we want. In our case, we want numeric results. The function to do this is called map_dbl (“map into double-precision numerical values”). It can be used just like map; the only difference between the two is that the output type changes from list to numeric vector:\n\nmap_dbl(1:10, function(x) x^2)\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\nSimilarly, there are functions map_chr, map_lgl, and some others, which create vectors of the appropriate type. For example, to append the agglutination “-ing” to various verbs, we can do:\n\nc(\"attend\", \"visit\", \"support\", \"help\", \"savour\") %>%\n  map_chr(function(text) paste0(text, \"ing\"))\n\n[1] \"attending\"  \"visiting\"   \"supporting\" \"helping\"    \"savouring\" \n\n\nInterestingly, we could also try\n\nmap_chr(1:10, function(x) x^2)\n\n [1] \"1.000000\"   \"4.000000\"   \"9.000000\"   \"16.000000\"  \"25.000000\" \n [6] \"36.000000\"  \"49.000000\"  \"64.000000\"  \"81.000000\"  \"100.000000\"\n\n\nand see that, although the computations were performed correctly, the output was converted from numbers to character strings encoding those numbers."
  },
  {
    "objectID": "Intro_map.html#making-use-of-map-when-vectorization-fails",
    "href": "Intro_map.html#making-use-of-map-when-vectorization-fails",
    "title": "14  Higher-order functions and mapping",
    "section": "14.4 Making use of map when vectorization fails",
    "text": "14.4 Making use of map when vectorization fails\nOne perhaps obvious criticism of map as we have used it is that its use was not really needed. Early on, we learned that when simple functions are applied to a vector of values, they get applied element-wise. This is called vectorization, and it is a very useful property of R. So (1:10)^2, in our case, achieves the same thing as map_dbl(1:10, function(x) x^2). Similarly, if we simply write cos(1:100), we get the cosine of all integers between 1 and 100 without having to type out map_dbl(1:100, cos). So why bother with map then?\nThe answer is that map can handle cases where vectorization is not available. Most simple functions in R are vectorized, but there are plenty of non-vectorizable operations. To give an example, let us start from a simple dataset: the PlantGrowth table we looked at before, but without the control ctrl group. This leaves just the two treatment groups trt1 and trt2:\n\nplantTrt <- filter(PlantGrowth, group != \"ctrl\")\nprint(plantTrt)\n\n   weight group\n1    4.81  trt1\n2    4.17  trt1\n3    4.41  trt1\n4    3.59  trt1\n5    5.87  trt1\n6    3.83  trt1\n7    6.03  trt1\n8    4.89  trt1\n9    4.32  trt1\n10   4.69  trt1\n11   6.31  trt2\n12   5.12  trt2\n13   5.54  trt2\n14   5.50  trt2\n15   5.37  trt2\n16   5.29  trt2\n17   4.92  trt2\n18   6.15  trt2\n19   5.80  trt2\n20   5.26  trt2\n\n\nWe might want to perform a Wilcoxon test on these data, but with a number of different confidence levels. A naive approach would be to supply the required confidence levels as a vector:\n\nwilcox.test(weight ~ group, data = plantTrt,\n            conf.int = TRUE, conf.level = c(0.8, 0.9, 0.95, 0.99))\n\nError in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): 'conf.level' must be a single number between 0 and 1\n\n\nThis generates an error: because of the way wilcox.test is internally implemented in R, it does not allow or recognize a vector input in place of conf.level. It must be a single number instead. This, however, can be overcome if we just use map:\n\nc(0.8, 0.9, 0.95, 0.99) %>% # The vector of confidence levels\n  map(function(confLevel) wilcox.test(weight ~ group, data = plantTrt,\n                            conf.int = TRUE, conf.level = confLevel))\n\n[[1]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n80 percent confidence interval:\n -1.33 -0.56\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[2]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n90 percent confidence interval:\n -1.43 -0.44\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[3]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -1.50 -0.31\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[4]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n99 percent confidence interval:\n -1.70 -0.03\nsample estimates:\ndifference in location \n                -0.945 \n\n\nNotice that we used map and not map_dbl or map_chr, because wilcox.test returns a complex model object which cannot be coerced into a vector. This is precisely when map, which generates a list whose entries can be arbitrary, is especially useful. (Try the above with map_dbl; it will throw an error.) As a final comment, it would of course have been possible to define a function separately, instead of using the anonymous function above:\n\nwilcoxConf <- function(confLevel) {\n  wilcox.test(weight ~ group, data = plantTrt,\n              conf.int = TRUE, conf.level = confLevel)\n}\n\nc(0.8, 0.9, 0.95, 0.99) %>% map(wilcoxConf)\n\n[[1]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n80 percent confidence interval:\n -1.33 -0.56\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[2]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n90 percent confidence interval:\n -1.43 -0.44\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[3]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -1.50 -0.31\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[4]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n99 percent confidence interval:\n -1.70 -0.03\nsample estimates:\ndifference in location \n                -0.945"
  },
  {
    "objectID": "Intro_map.html#exercises",
    "href": "Intro_map.html#exercises",
    "title": "14  Higher-order functions and mapping",
    "section": "14.5 Exercises",
    "text": "14.5 Exercises\n\nStart from the following sequence of values: values <- seq(-5*pi, 5*pi, by = 0.01) (a vector with values going from -5π to 5π, in steps of 0.01). Now do the following steps:\n\nDefine a new vector, x, which contains the cosine (cos) of each value in values. Use map_dbl.\nNow define another vector, y, and again using map_dbl, compute the function sin(t) + cos(14 * t / 5) / 5 for every value t in values. You can either define this function separately to use inside map_dbl, or create it anonymously.\nFinally, create a tibble whose two columns are x and y, and plot them against each other using geom_path. See what you get!\n\nHow does the integral of cos(x) change if the lower limit of integration is fixed at 0, but the upper limit gradually increases from 0 to 2π? Define a sequence of upper limits upper <- seq(0, 2*pi, by = 0.1). Then, using map_dbl, create a vector integrals whose entries are the integral of cos(x) from 0 to each upper limit. Finally, plot integrals against upper, using geom_point or geom_line. What is the function you see? (Note: the integrate function returns a complicated list object instead of just a single number. To access just the value of the integral, you can use integrate(...)$value, where ... means all the arguments to the function you are supposed to write when solving the problem.)"
  },
  {
    "objectID": "Multiple_analysis.html#motivating-example",
    "href": "Multiple_analysis.html#motivating-example",
    "title": "15  Nested data and multiple analysis",
    "section": "15.1 Motivating example",
    "text": "15.1 Motivating example\nThis chapter is on performing statistical (or other) analyses en masse. To motivate the problem, let us start from a dataset that has been adapted from Bolstad et al. (2015):\n\nlibrary(tidyverse)\n\nfly <- read_csv(\"fruit_fly_wings.csv\")\nprint(fly)\n\n# A tibble: 10,327 × 6\n   Species   ID          Date      Sex   WingSize L2Length\n   <chr>     <chr>       <chr>     <chr>    <dbl>    <dbl>\n 1 D_acutila ACU1006.TIF 24_Jul_01 F        0.131    0.497\n 2 D_acutila ACU1009.TIF 24_Jul_01 F        0.136    0.488\n 3 D_acutila ACU1010.TIF 24_Jul_01 F        0.195    0.540\n 4 D_acutila ACU1013.TIF 24_Jul_01 F        0.277    0.646\n 5 D_acutila ACU1018.TIF 24_Jul_01 F        0.152    0.498\n 6 D_acutila ACU1021.TIF 24_Jul_01 F        0.175    0.492\n 7 D_acutila ACU1048.TIF 24_Jul_01 F        0.230    0.600\n 8 D_acutila ACU1049.TIF 24_Jul_01 F        0.174    0.546\n 9 D_acutila ACU1054.TIF 05_Sep_01 F        0.108    0.429\n10 D_acutila ACU1059.TIF 05_Sep_01 F        0.205    0.580\n# … with 10,317 more rows\n\n\nThe data contain measurements on individual fruit flies, belonging to various species and either sex as indicated by the Species and Sex columns. Each individual is uniquely identified (ID), and the date of the measurement has also been recorded (Date). Most importantly, the length of the wing (WingSize) and the length of the L2 vein that runs across the wing (L2Length) have been recorded.\nWhat is the distribution of wing sizes across species and sexes? To begin answering this question, we can start with a plot:\n\nggplot(fly) +\n  aes(x = WingSize, y = Species, colour = Sex, fill = Sex) +\n  geom_boxplot(alpha = 0.2) +\n  scale_colour_manual(values = c(\"steelblue\", \"goldenrod\")) +\n  scale_fill_manual(values = c(\"steelblue\", \"goldenrod\")) +\n  xlab(\"Wing size\") +\n  theme_bw()\n\n\n\n\nLooking at this figure, it does appear as if females often had larger wings than males within the same species. The main question in this chapter is how we can test this—for instance, how would it be possible to perform a Wilcoxon test for all 55 species in the data?"
  },
  {
    "objectID": "Multiple_analysis.html#nested-data-frames",
    "href": "Multiple_analysis.html#nested-data-frames",
    "title": "15  Nested data and multiple analysis",
    "section": "15.2 Nested data frames",
    "text": "15.2 Nested data frames\nWe are by now used to the idea that the columns of tibbles can hold numbers, character strings, logical values, or even factors. But here is an interesting question: can a column of a tibble hold other tibbles?\nThe short answer is yes. The somewhat longer answer is that this is possible, but not directly so. Instead, one has to make the column into a list (Section 13.2.1).1 While this could of course be done by hand using the list function, there are other options in the tidyverse which facilitate creating tibbles which have other tibbles in their columns. One of these is called nest. This function receives a name first, which will become the name of the newly-created column holding the sub-tibbles. Then, after an equality sign, one lists the columns, in a vector, which one would like to package into those sub-tibbles. For example, to keep Species as a separate column and wrap everything else into sub-tibbles, we can do:\n\nfly %>% nest(data = c(ID, Date, Sex, WingSize, L2Length))\n\n# A tibble: 55 × 2\n   Species      data              \n   <chr>        <list>            \n 1 D_acutila    <tibble [205 × 5]>\n 2 D_algonqu    <tibble [237 × 5]>\n 3 D_texana     <tibble [215 × 5]>\n 4 Z_Sg.Anaprio <tibble [200 × 5]>\n 5 D_athabasca  <tibble [79 × 5]> \n 6 D_bifasci    <tibble [217 × 5]>\n 7 D_busckii    <tibble [105 × 5]>\n 8 I_crucige    <tibble [211 × 5]>\n 9 Det_nigro    <tibble [15 × 5]> \n10 H_duncani    <tibble [219 × 5]>\n# … with 45 more rows\n\n\nWhat do we see? We ended up with a tibble that has two columns. One is Species and has type character string. The other is data and has the type of list, as indicated by the <list> tag. The entries in this column are tibbles, with varying numbers of rows (as many as the number of individuals for the given species), and five columns; namely, those that we specified we wanted to nest. We can check and see what is inside these tibbles. For example, the contents of the first row (species: D. acutila) are:\n\nfly %>%\n  nest(data = c(ID, Date, Sex, WingSize, L2Length)) %>%\n  pull(data) %>% # Get contents of just the \"data\" column\n  pluck(1) # Take the first of all those tibbles\n\n# A tibble: 205 × 5\n   ID          Date      Sex   WingSize L2Length\n   <chr>       <chr>     <chr>    <dbl>    <dbl>\n 1 ACU1006.TIF 24_Jul_01 F        0.131    0.497\n 2 ACU1009.TIF 24_Jul_01 F        0.136    0.488\n 3 ACU1010.TIF 24_Jul_01 F        0.195    0.540\n 4 ACU1013.TIF 24_Jul_01 F        0.277    0.646\n 5 ACU1018.TIF 24_Jul_01 F        0.152    0.498\n 6 ACU1021.TIF 24_Jul_01 F        0.175    0.492\n 7 ACU1048.TIF 24_Jul_01 F        0.230    0.600\n 8 ACU1049.TIF 24_Jul_01 F        0.174    0.546\n 9 ACU1054.TIF 05_Sep_01 F        0.108    0.429\n10 ACU1059.TIF 05_Sep_01 F        0.205    0.580\n# … with 195 more rows\n\n\nSo this sub-table contains the information that pertains to just D. acutila individuals.\nWhen choosing which columns to wrap into sub-tibbles with nest, all the conventions and functionalities apply that one can use with the select function. So the above nesting could be equivalently and more simply be performed with:\n\nfly %>% nest(data = !Species)\n\n# A tibble: 55 × 2\n   Species      data              \n   <chr>        <list>            \n 1 D_acutila    <tibble [205 × 5]>\n 2 D_algonqu    <tibble [237 × 5]>\n 3 D_texana     <tibble [215 × 5]>\n 4 Z_Sg.Anaprio <tibble [200 × 5]>\n 5 D_athabasca  <tibble [79 × 5]> \n 6 D_bifasci    <tibble [217 × 5]>\n 7 D_busckii    <tibble [105 × 5]>\n 8 I_crucige    <tibble [211 × 5]>\n 9 Det_nigro    <tibble [15 × 5]> \n10 H_duncani    <tibble [219 × 5]>\n# … with 45 more rows\n\n\nThat is: apply the nesting to all columns that are not called Species. This can be used in more complicated cases as well. For example, if we wish to nest data pertaining to particular species-sex combinations, we can do the following:\n\nfly %>% nest(data = !Species & !Sex)\n\n# A tibble: 110 × 3\n   Species      Sex   data              \n   <chr>        <chr> <list>            \n 1 D_acutila    F     <tibble [104 × 4]>\n 2 D_acutila    M     <tibble [101 × 4]>\n 3 D_algonqu    F     <tibble [144 × 4]>\n 4 D_algonqu    M     <tibble [93 × 4]> \n 5 D_texana     F     <tibble [108 × 4]>\n 6 D_texana     M     <tibble [107 × 4]>\n 7 Z_Sg.Anaprio F     <tibble [95 × 4]> \n 8 Z_Sg.Anaprio M     <tibble [105 × 4]>\n 9 D_athabasca  F     <tibble [20 × 4]> \n10 D_athabasca  M     <tibble [59 × 4]> \n# … with 100 more rows\n\n\nwhere nest(data = !Species & !Sex) (nest all columns that are not Species and not Sex) is equivalent to the longer nest(data = c(ID, Date, WingSize, L2Length)).\nFinally, columns holding nested data can be unnested, meaning that their contents are expanded back into the original data frame. The function to do this with is called unnest:\n\nfly %>% nest(data = !Species & !Sex) %>% unnest(data)\n\n# A tibble: 10,327 × 6\n   Species   Sex   ID          Date      WingSize L2Length\n   <chr>     <chr> <chr>       <chr>        <dbl>    <dbl>\n 1 D_acutila F     ACU1006.TIF 24_Jul_01    0.131    0.497\n 2 D_acutila F     ACU1009.TIF 24_Jul_01    0.136    0.488\n 3 D_acutila F     ACU1010.TIF 24_Jul_01    0.195    0.540\n 4 D_acutila F     ACU1013.TIF 24_Jul_01    0.277    0.646\n 5 D_acutila F     ACU1018.TIF 24_Jul_01    0.152    0.498\n 6 D_acutila F     ACU1021.TIF 24_Jul_01    0.175    0.492\n 7 D_acutila F     ACU1048.TIF 24_Jul_01    0.230    0.600\n 8 D_acutila F     ACU1049.TIF 24_Jul_01    0.174    0.546\n 9 D_acutila F     ACU1054.TIF 05_Sep_01    0.108    0.429\n10 D_acutila F     ACU1059.TIF 05_Sep_01    0.205    0.580\n# … with 10,317 more rows"
  },
  {
    "objectID": "Multiple_analysis.html#performing-statistical-tests-using-nested-data",
    "href": "Multiple_analysis.html#performing-statistical-tests-using-nested-data",
    "title": "15  Nested data and multiple analysis",
    "section": "15.3 Performing statistical tests using nested data",
    "text": "15.3 Performing statistical tests using nested data\nHow can we test for all 55 fly species in this dataset whether there is a significant difference between average male and female wing lengths? The answer is to first nest the data using fly %>% nest(data = !Species), so that we end up with a table which has one row per each species. We then need to run a Wilcoxon test on each of them. But this we know how to do from Chapter 14: we can rely on the map function.\n\nfly %>%\n  nest(data = !Species) %>%\n  mutate(test = map(data, function(x) wilcox.test(L2Length ~ Sex, data = x)))\n\n# A tibble: 55 × 3\n   Species      data               test   \n   <chr>        <list>             <list> \n 1 D_acutila    <tibble [205 × 5]> <htest>\n 2 D_algonqu    <tibble [237 × 5]> <htest>\n 3 D_texana     <tibble [215 × 5]> <htest>\n 4 Z_Sg.Anaprio <tibble [200 × 5]> <htest>\n 5 D_athabasca  <tibble [79 × 5]>  <htest>\n 6 D_bifasci    <tibble [217 × 5]> <htest>\n 7 D_busckii    <tibble [105 × 5]> <htest>\n 8 I_crucige    <tibble [211 × 5]> <htest>\n 9 Det_nigro    <tibble [15 × 5]>  <htest>\n10 H_duncani    <tibble [219 × 5]> <htest>\n# … with 45 more rows\n\n\nAnd voilà: we now have the Wilcoxon test results in the column test, for each species! All we need to do is retrieve this information.\nDoing so is not completely straightforward, because wilcox.test does not return a data frame or tibble. Instead, it returns a complicated model fit object which cannot be unnested into the outer table. If we try, we get an error:\n\nfly %>%\n  nest(data = !Species) %>%\n  mutate(test = map(data, function(x) wilcox.test(L2Length~Sex, data=x))) %>%\n  unnest(test)\n\nError in `list_sizes()`:\n! `x[[1]]` must be a vector, not a <htest> object.\n\n\nFortunately, there is an easy way to convert the output of the Wilcoxon test into a tibble. The broom package is designed to do exactly this. It is part of the tidyverse, though it does not get automatically loaded with it. We load this package first:\n\nlibrary(broom)\n\nThe function in this package that creates a tibble out of the results of statistical models (almost any model in fact, not just the Wilcoxon test) is called tidy. Let us see how it works. If we do a Wilcoxon test between females and males for the whole data (without distinguishing between species), we get:\n\nwilcox.test(WingSize ~ Sex, data = fly, conf.int = TRUE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  WingSize by Sex\nW = 16695507, p-value < 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n 0.07910817 0.09418589\nsample estimates:\ndifference in location \n            0.08664608 \n\n\nBy applying the tidy function to this result, it gets converted into a tibble:\n\nwilcox.test(WingSize ~ Sex, data = fly, conf.int = TRUE) %>% tidy()\n\n# A tibble: 1 × 7\n  estimate statistic   p.value conf.low conf.high method                 alter…¹\n     <dbl>     <dbl>     <dbl>    <dbl>     <dbl> <chr>                  <chr>  \n1   0.0866  16695507 2.54e-109   0.0791    0.0942 Wilcoxon rank sum tes… two.si…\n# … with abbreviated variable name ¹​alternative\n\n\nSo we can insert a step into our analysis pipeline which converts the test column into a list of data frames:\n\nfly %>%\n  nest(data = !Species) %>%\n  mutate(test = map(data, function(x) wilcox.test(L2Length~Sex, data=x))) %>%\n  mutate(test = map(test, tidy)) %>%\n  unnest(test)\n\n# A tibble: 55 × 6\n   Species      data               statistic  p.value method             alter…¹\n   <chr>        <list>                 <dbl>    <dbl> <chr>              <chr>  \n 1 D_acutila    <tibble [205 × 5]>      9640 5.03e-25 Wilcoxon rank sum… two.si…\n 2 D_algonqu    <tibble [237 × 5]>     12494 2.34e-29 Wilcoxon rank sum… two.si…\n 3 D_texana     <tibble [215 × 5]>      9189 7.55e-14 Wilcoxon rank sum… two.si…\n 4 Z_Sg.Anaprio <tibble [200 × 5]>      6330 1.03e- 3 Wilcoxon rank sum… two.si…\n 5 D_athabasca  <tibble [79 × 5]>       1116 3.13e- 9 Wilcoxon rank sum… two.si…\n 6 D_bifasci    <tibble [217 × 5]>     11447 2.56e-33 Wilcoxon rank sum… two.si…\n 7 D_busckii    <tibble [105 × 5]>      1654 7.63e- 2 Wilcoxon rank sum… two.si…\n 8 I_crucige    <tibble [211 × 5]>      5238 4.63e- 1 Wilcoxon rank sum… two.si…\n 9 Det_nigro    <tibble [15 × 5]>         20 8.51e- 1 Wilcoxon rank sum… two.si…\n10 H_duncani    <tibble [219 × 5]>     10195 2.25e-19 Wilcoxon rank sum… two.si…\n# … with 45 more rows, and abbreviated variable name ¹​alternative\n\n\nAnd now we have the results. As an example, we can check the distribution of p-values: how often is there a statistically significant sex difference? Let us visualize this, by ordering the species based on p-values:\n\nfly %>%\n  nest(data = !Species) %>%\n  mutate(test = map(data, function(x) wilcox.test(L2Length~Sex, data=x))) %>%\n  mutate(test = map(test, tidy)) %>%\n  unnest(test) %>%\n  arrange(p.value) %>%\n  mutate(Species = as_factor(Species)) %>%\n  ggplot(aes(x = p.value, y = Species)) +\n  geom_col(colour = \"steelblue\", fill = \"steelblue\", alpha = 0.3) +\n  scale_x_continuous(name = \"p-value\", limits = c(0, 1)) +\n  theme_bw()\n\n\n\n\nThis graph shows that most species have very small p-values, but that there are four clear outliers. We can extract the names of these outlier species:\n\noutlierSpecies <- fly %>%\n  nest(data = !Species) %>%\n  mutate(test = map(data, function(x) wilcox.test(L2Length~Sex, data=x))) %>%\n  mutate(test = map(test, tidy)) %>%\n  unnest(test) %>%\n  arrange(desc(p.value)) %>%\n  slice(1:4) %>% # Choose first 4 rows from the sorted table\n  pull(Species) # Get the 4 species names\n\nprint(outlierSpecies)\n\n[1] \"Det_nigro\" \"N_sordida\" \"I_crucige\" \"D_busckii\"\n\n\nAnd then we can plot the wing length data for just these ones:\n\nfly %>%\n  filter(Species %in% outlierSpecies) %>%\n  ggplot() +\n  aes(x = Sex, y = WingSize) +\n  geom_boxplot(colour = \"steelblue\", fill = \"steelblue\",\n               alpha = 0.2, outlier.shape = NA) +\n  geom_jitter(colour = \"steelblue\", alpha = 0.4) +\n  facet_grid(. ~ Species) +\n  theme_bw()\n\n\n\n\nFor D_busckii, I_crucige, and N_sordida, it is difficult not to conclude that the low p-values indicate the lack of a meaningful sex difference in wing length. For Det_nigro on the other hand, there are very few sampled individuals. Therefore one would most likely need more data to say.\nIn summary, a very powerful way of analyzing data is to perform many analyses at once. To do so, one first has to nest the data. Then the analysis can be performed for every row with the help of the map function. Finally, one unnests the data and interprets the results. Often, an overview of the data will lead to insights that would have been difficult to gain otherwise. In our case, we saw that all but a handful of species exhibit a significant sex difference in wing length. For the few outliers, we saw that one of them lacks sufficient data, and therefore conclusions about this species should be postponed until more data are acquired."
  },
  {
    "objectID": "Multiple_analysis.html#exercises",
    "href": "Multiple_analysis.html#exercises",
    "title": "15  Nested data and multiple analysis",
    "section": "15.4 Exercises",
    "text": "15.4 Exercises\n\nOur analysis on the sex differences in the wing length of fruit flies revealed whether there was a significant difference within each species. However, it did not say anything about which sex tends to have a longer wing. Perform this analysis here.\n\nCreate a graph with the difference between mean female and mean male wing lengths along the x-axis, and the species along the y-axis. You can represent the difference of means in each species by a point (geom_point).\nHow many species are there where females have longer wings on average? How many where males do?\nWhich species shows the largest degree of sexual dimorphism?\nList the species where males have, on average, longer wings than females.\n\nThe goal of the original study by Bolstad et al. (2015 PNAS) was to see the allometric relationship between wing length and the length of the L2 vein that runs across the wing, in different species of fruit flies.\n\nObtain the slope from a linear regression between wing size and L2 vein length (which has been logged in the data) for each species and sex.\nCreate a histogram of the regression slopes. What is the (approximate) distribution of the slopes?\nWhat is the mean and the standard deviation of the distribution of slopes? What is their range? Are the slopes all positive, all negative, or vary between the two? What does this tell you about the relationship between wing size and L2 vein length in general?\nPlot your results, with the regression slopes along the x-axis and the species-sex combination along the y-axis, sorted in the order of the regression slopes. Which species-sex combination has the largest slope? Which one has the smallest?\n\nThe gapminder package contains information on the population size, average life expectancy, and per capita GDP for 142 countries, from 1952 to 2007 (in steps of 5 years). Download and install this package via install.packages(\"gapminder\"), then load it with library(gapminder). If you now type gapminder in the console, you should see a table with six columns. Here we will be focusing on the columns country, continent, year, and pop (the population size of the country in the given year). Now do the following exercises.\n\nLet us see if and when population growth has been exponential in these countries. If the growth of the population size is exponential, then the growth of its logarithm is linear. Therefore, as a first step, take the logarithms of all population sizes in the pop column.\nNest the data by country and continent, and obtain a linear fit of log population size against year for each.\nExtract from this, not the slope or p-value, but a different measure of the model’s quality: the proportion of variance explained (R2). Hint: you can do this with the glance function, which is part of the broom package. It works much in the same way as tidy, but extracts information about model quality instead of model parameters. The R2 value is contained in the column called r.squared.\nMake a plot with R2 along the x-axis and country along the y-axis, showing the R2 values by points. Colour the points based on the continent of the country.\nMake the same plot but first reorder the countries by r.squared.\nWhich handful of countries stand out as having a particularly poor fit with the linear model? Make a plot of just the seven countries with the lowest R2 values. Let year be along the x-axis, the log population size along the y-axis, and the different countries be in separate facets. Bonus exercise: alongside these population curves, display also the predictions from the linear regressions.\nWhich are the countries with the worst model fit? Do they tend to come from a particular continent or region? Given your knowledge of recent history, can you speculate on what the reasons could be for their deviations from exponential growth?\n\nIn this exercise, we explore the goodness-of-fit of linear models between sepal and petal lengths in the iris dataset.\n\nFirst, visualize the data. Create a plot of the iris dataset, using points whose x-coordinate is sepal length and y-coordinate is petal length. Let them be colored by species. Finally, show linear regressions on the points belonging to each species, using geom_smooth.\nObtain the slope of the fit for each species, and the associated p-value. Do this by first nesting the data by species, then fitting a linear model to each of them (with map), and extracting slopes and p-values by applying the tidy function in the broom package. Finally, unnest the data. What are the slopes? And are they significantly different from zero?\n\n\n\n\n\n\nBolstad, Geir H., Jason A. Cassara, Eladio Márquez, Thomas F. Hansen, Kim van der Linde, David Houle, and Christophe Pélabon. 2015. “Complex constraints on allometry revealed by artificial selection on the wing of Drosophila melanogaster.” Proceedings of the National Academy of Sciences 112 (43): 13284–89. https://doi.org/10.1073/pnas.1505357112."
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "16  References",
    "section": "",
    "text": "Anscombe, Francis J. 1973. “Graphs in\nStatistical Analysis.” American Statistician 27\n(1): 17–21. https://doi.org/10.1080/00031305.1973.10478966.\n\n\nBolstad, Geir H., Jason A. Cassara, Eladio Márquez, Thomas F. Hansen,\nKim van der Linde, David Houle, and Christophe Pélabon. 2015.\n“Complex constraints on allometry revealed by\nartificial selection on the wing of Drosophila\nmelanogaster.” Proceedings of the National Academy of\nSciences 112 (43): 13284–89. https://doi.org/10.1073/pnas.1505357112.\n\n\nCedergreen, Nina, and Tom Vindbæk Madsen. 2002. “Nitrogen uptake by the floating macrophyte Lemna\nminor.” New Phytologist 155 (2): 285–92. https://doi.org/10.1046/j.1469-8137.2002.00463.x.\n\n\nColquhoun, David. 2014. “An investigation of\nthe false discovery rate and the misinterpretation of\np-values.” Royal Society Open Science 1 (3):\n140216. https://doi.org/10.1098/rsos.140216.\n\n\nFauchald, Per, Taejin Park, Hans Tømmervik, Ranga Myneni, and Vera\nHelene Hausner. 2017. “Arctic greening from\nwarming promotes declines in caribou populations.”\nScience Advances 3 (4): e1601365. https://doi.org/10.1126/sciadv.1601365.\n\n\nGalton, Francis. 1886. “Regression Towards\nMediocrity in Hereditary Stature.” Journal of the\nAnthropological Institute of Great Britain and Ireland 15: 246–63.\n\n\nGillespie, John H. 2004. Population Genetics: A Concise Guide.\nBaltimore, MD, USA: Johns Hopkins University Press.\n\n\nGoldberg, Emma E., Joshua R. Kohn, Russell Lande, Kelly A. Robertson,\nStephen A. Smith, and Boris Igić. 2010. “Species Selection\nMaintains Self-Incompatibility.” Science 330\n(6003): 493–95. https://doi.org/10.1126/science.1194513.\n\n\nSmith, Felisa A., S. Kathleen Lyons, S. K. Morgan Ernest, Kate E. Jones,\nDawn M. Kaufman, Tamar Dayan, Pablo A. Marquet, James H. Brown, and John\nP. Haskell. 2003. “Body Mass of Late\nQuaternary Mammals.” Ecology 84 (12): 3403. https://doi.org/10.1890/02-9003.\n\n\nWilkinson, Leland. 2006. The Grammar of\nGraphics. Secaucus, NJ, USA: Springer Science & Business\nMedia."
  }
]